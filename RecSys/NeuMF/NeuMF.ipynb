{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from box import Box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(df: pd.DataFrame) -> Tuple[pd.DataFrame, LabelEncoder, LabelEncoder]:\n",
    "    userId_label_encoder = LabelEncoder()\n",
    "    movieId_label_encoder = LabelEncoder()\n",
    "\n",
    "    df['userId'] = userId_label_encoder.fit_transform(df['userId'].values)\n",
    "    df['movieId'] = movieId_label_encoder.fit_transform(df['movieId'].values)\n",
    "\n",
    "    # encoder.inverse_transform() 으로 decode\n",
    "    return df, userId_label_encoder, movieId_label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainTestSplit(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Splits our original data into one test and one\n",
    "    training set. \n",
    "    The test set is made up of one item for each user. This is\n",
    "    our holdout item used to compute Top@K later.\n",
    "    The training set is the same as our original data but\n",
    "    without any of the holdout items.\n",
    "    Args:\n",
    "        df (dataframe): Our original data\n",
    "    Returns:\n",
    "        df_train (dataframe): All of our data except holdout items\n",
    "        df_test (dataframe): Only our holdout items.\n",
    "    \"\"\"\n",
    "\n",
    "    # Group by userId and select only the first item for\n",
    "    # each user (our holdout).\n",
    "    df_test = df.groupby(['userId']).first()\n",
    "    df_test['userId'] = df_test.index\n",
    "    df_test = df_test[['userId', 'movieId', 'rating', 'timestamp']]\n",
    "    df_test.index.name = None\n",
    "\n",
    "    # Remove the same items as we for our test set in our training set.\n",
    "    mask = df.groupby(['userId'])['userId'].transform(maskFirst).astype(bool)\n",
    "    df_train = df.loc[mask]\n",
    "\n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "def maskFirst(x):\n",
    "    \"\"\"\n",
    "    Return a list of 0 for the first item and 1 for all others\n",
    "    \"\"\"\n",
    "    result = np.ones_like(x)\n",
    "    result[0] = 0\n",
    "\n",
    "    return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNegatives(df_train: pd.DataFrame, df_test: pd.DataFrame, set_all_movies: set, num_negatives: int) -> pd.DataFrame:\n",
    "    list_negative = []\n",
    "\n",
    "    test_user = df_test['userId'].values.tolist()\n",
    "    test_movie = df_test['movieId'].values.tolist()\n",
    "\n",
    "    for user, movie in zip(test_user, test_movie):\n",
    "        list_train_user_movies = df_train[df_train['userId']==user]['movieId'].tolist()\n",
    "        set_pos_user_movies = set(list_train_user_movies + [movie])\n",
    "        list_user_neg_movies = list(set_all_movies - set_pos_user_movies)\n",
    "        \n",
    "        negatives = [(user, movie)] + np.random.choice(list_user_neg_movies, num_negatives, replace=False).tolist()\n",
    "        list_negative.append(negatives)\n",
    "\n",
    "    df_neg = pd.DataFrame(list_negative)\n",
    "\n",
    "    return df_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuMFDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, num_movies: int, num_negatives: int):\n",
    "        self.df = df\n",
    "        self.num_movies = num_movies\n",
    "        self.num_negatives = num_negatives\n",
    "        self.df_with_neg = self._getNegativeInstances()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user = self.df_with_neg.iloc[idx]['userId']\n",
    "        item = self.df_with_neg.iloc[idx]['movieId']\n",
    "        label = self.df_with_neg.iloc[idx]['label']\n",
    "\n",
    "        return user, item, label\n",
    "    \n",
    "    def _getNegativeInstances(self) -> pd.DataFrame:\n",
    "\n",
    "        df_users = self.df['userId'].values.tolist()\n",
    "\n",
    "        all_movies = set([i for i in range(self.num_movies)])\n",
    "        list_users, list_movies, list_labels = [], [], []\n",
    "\n",
    "        for user in tqdm(df_users):\n",
    "            user_pos_movies = set(self.df[self.df['userId']==user]['movieId'].tolist())\n",
    "            candi_user_neg_movies = list(all_movies - user_pos_movies)\n",
    "\n",
    "            user_neg_movies = np.random.choice(candi_user_neg_movies, self.num_negatives, replace=False).tolist()\n",
    "\n",
    "            self._appendListUserItemLabel(list_users, list_movies, list_labels, user, user_pos_movies, 1)\n",
    "            self._appendListUserItemLabel(list_users, list_movies, list_labels, user, user_neg_movies, 0)\n",
    "\n",
    "        print('make list done!')\n",
    "\n",
    "        df_with_neg = pd.DataFrame([x for x in zip(list_users, list_movies, list_labels)], columns=['userId', 'movieId', 'label'])\n",
    "\n",
    "        print('make pd.DataFrame doen!')\n",
    "\n",
    "        return df_with_neg\n",
    "\n",
    "    \n",
    "    def _appendListUserItemLabel(self,\n",
    "                                users: list, \n",
    "                                movies: list, \n",
    "                                labels: list, \n",
    "                                candi_user: int, \n",
    "                                candi_movies: list, \n",
    "                                candi_label: int,\n",
    "                                ) -> None:\n",
    "        for movie in candi_movies:\n",
    "            users.append(candi_user)\n",
    "            movies.append(movie)\n",
    "            labels.append(candi_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMF(nn.Module):\n",
    "    def __init__(self, num_users: int, num_items: int, latent_dim: int):\n",
    "        super(GMF, self).__init__()\n",
    "\n",
    "        self.embedding_user = nn.Embedding(num_users, latent_dim)\n",
    "        self.embedding_item = nn.Embedding(num_items, latent_dim)\n",
    "\n",
    "        self.prediction = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight.data, mean=0.0, std=0.01)\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight.data, 0, 0.01)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.fill_(0.0)\n",
    "\n",
    "    \n",
    "    def forward(self, user_input, item_input):\n",
    "        user_latent = self.embedding_user(user_input)\n",
    "        item_latent = self.embedding_item(item_input)\n",
    "\n",
    "        product = user_latent * item_latent\n",
    "\n",
    "        output = self.prediction(product)\n",
    "\n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_uesrs, num_items, latent_dim, dropout, layers=[20, 10]):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.embedding_user = nn.Embedding(num_uesrs, latent_dim)\n",
    "        self.embedding_item = nn.Embedding(num_items, latent_dim)\n",
    "\n",
    "        layers.insert(0, latent_dim * 2)\n",
    "        modules = []\n",
    "        for i in range(len(layers) - 1):\n",
    "            modules.append(nn.Dropout(p=dropout))\n",
    "            modules.append(nn.Linear(layers[i], layers[i+1]))\n",
    "            modules.append(nn.ReLU())\n",
    "        \n",
    "        self.dense_layers = nn.Sequential(*modules)\n",
    "\n",
    "        self.prediction = nn.Sequential(\n",
    "            nn.Linear(layers[-1], 1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    # initialize weights\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight.data, mean=0.0, std=0.01)\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight.data, 0, 0.01)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, user_input, item_input):\n",
    "        user_latent = self.embedding_user(user_input)\n",
    "        item_latent = self.embedding_item(item_input)\n",
    "\n",
    "        vector = torch.cat((user_latent, item_latent), dim=-1)\n",
    "\n",
    "        output = self.prediction(self.dense_layers(vector))\n",
    "\n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuMF(nn.Module):\n",
    "    def __init__(self, GMF, MLP, latent_dim, layers):\n",
    "        super(NeuMF, self).__init__()\n",
    "\n",
    "        self.GMF_embedding_user = GMF.embedding_user\n",
    "        self.GMF_embedding_item = GMF.embedding.item\n",
    "\n",
    "        self.MLP_embedding_user = MLP.embedding_user\n",
    "        self.MLP_embedding_item = MLP.embedding.item\n",
    "\n",
    "        self.MLP_dense_layers = MLP.dense_layers\n",
    "\n",
    "        self.prediction = nn.Sequential(OrderedDict([\n",
    "            ('prediction', nn.Linear(latent_dim + layers[-1], 1, bias=False)),\n",
    "            ('prediction', nn.Sigmoid())\n",
    "\n",
    "        ]))\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    # initialize weights\n",
    "    def _init_weights(self, module):\n",
    "        for name, layer in module.named_modules():\n",
    "            if isinstance(layer, nn.Linear) and name == 'prediction':\n",
    "                nn.init.normal_(layer.weight.data, mean=0.0, std=0.01)\n",
    "                if layer.bias is not None:\n",
    "                    layer.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, user_input, item_input):\n",
    "        GMF_user_latent = self.GMF_embedding_user(user_input)\n",
    "        GMF_item_latent = self.GMF_embedding_item(item_input)\n",
    "        GMF_output = GMF_user_latent * GMF_item_latent\n",
    "\n",
    "        MLP_user_latent = self.MLP_embedding_user(user_input)\n",
    "        MLP_item_latent = self.MLP_embedding_item(item_input)\n",
    "        vector = torch.cat((MLP_user_latent, MLP_item_latent), dim=-1)\n",
    "        MLP_output = self.MLP_dense_layers(vector)\n",
    "\n",
    "        concat_output = torch.cat((GMF_output, MLP_output), dim=-1)\n",
    "\n",
    "        output = self.prediction(concat_output)\n",
    "\n",
    "        return output.squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Metric: HR, NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, epochs, criterion, device):\n",
    "    model.train()\n",
    "\n",
    "    size = len(train_loader)\n",
    "\n",
    "    # 훈련 시간 측정\n",
    "    epoch_start = torch.cuda.Event(enable_timing=True)\n",
    "    epoch_end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "\n",
    "        # 시작 시간 기록\n",
    "        epoch_start.record()\n",
    "\n",
    "        for user, item, label in train_loader:\n",
    "            user = user.to(device)\n",
    "            item = item.to(device)\n",
    "            label = label.to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(user, item)\n",
    "            loss = criterion(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "\n",
    "        epoch_end.record()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        avg_loss = epoch_loss / size\n",
    "\n",
    "        print(\n",
    "            f'Epoch[{epoch+1}/{epochs}]\\ttrain_loss: {avg_loss:.4f}' +\n",
    "            f'\\t훈련시간: {epoch_start.elapsed_time(epoch_end)/1000:.2f} sec'\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use cuda\n"
     ]
    }
   ],
   "source": [
    "class DirFilePath:\n",
    "    dir_base = os.path.join(os.path.join('/opt','ml','paper','RecSys'))\n",
    "    dir_data = os.path.join(dir_base, 'Data', 'ml-latest-small')\n",
    "    path_rating = os.path.join(dir_data, 'ratings.csv')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Use {device}')\n",
    "\n",
    "# 해당 논문의 github에서 사용한 값 참고\n",
    "config = {\n",
    "    'seed': 42,\n",
    "    'batch_size': 256,\n",
    "    'epochs': 20,\n",
    "    'latent_dim': 8,\n",
    "    'num_neg': 4,\n",
    "    'lr': 0.001,\n",
    "    'optimizer': 'adam',\n",
    "    'criterion': 'BCE',\n",
    "    'device': device,\n",
    "    'layers': [64, 32, 16, 8],\n",
    "    'dropout': 0,\n",
    "    'topK': 10,\n",
    "    }\n",
    "\n",
    "config = Box(config)\n",
    "\n",
    "def setSeed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "setSeed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rating = pd.read_csv(DirFilePath.path_rating)\n",
    "df_rating, user_encoder, movie_encoder = encode(df_rating)\n",
    "\n",
    "df_train, df_test = trainTestSplit(df_rating)\n",
    "\n",
    "# # Create lists of all movies\n",
    "# set_all_movies = set(df_rating['movieId'].unique())\n",
    "\n",
    "# df_test_neg = getNegatives(df_train, df_test, set_all_movies, 99)\n",
    "\n",
    "num_movies = len(df_rating['movieId'].unique())\n",
    "num_users = len(df_rating['userId'].unique())\n",
    "\n",
    "dataset_train = NeuMFDataset(df_train, num_movies, 4)\n",
    "dataset_test = NeuMFDataset(df_test, num_movies, 99)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=config.batch_size, shuffle=True, drop_last=False)\n",
    "test_loader = DataLoader(dataset_test, batch_size=config.batch_size, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/20]\ttrain_loss: 0.5499\t훈련시간: 23.22 sec\n",
      "Epoch[2/20]\ttrain_loss: 0.0927\t훈련시간: 23.40 sec\n",
      "Epoch[3/20]\ttrain_loss: 0.0249\t훈련시간: 23.29 sec\n",
      "Epoch[4/20]\ttrain_loss: 0.0143\t훈련시간: 23.39 sec\n",
      "Epoch[5/20]\ttrain_loss: 0.0100\t훈련시간: 23.53 sec\n",
      "Epoch[6/20]\ttrain_loss: 0.0074\t훈련시간: 23.39 sec\n",
      "Epoch[7/20]\ttrain_loss: 0.0056\t훈련시간: 23.58 sec\n",
      "Epoch[8/20]\ttrain_loss: 0.0042\t훈련시간: 23.47 sec\n",
      "Epoch[9/20]\ttrain_loss: 0.0032\t훈련시간: 23.60 sec\n",
      "Epoch[10/20]\ttrain_loss: 0.0024\t훈련시간: 23.38 sec\n",
      "Epoch[11/20]\ttrain_loss: 0.0018\t훈련시간: 23.32 sec\n",
      "Epoch[12/20]\ttrain_loss: 0.0013\t훈련시간: 23.16 sec\n",
      "Epoch[13/20]\ttrain_loss: 0.0010\t훈련시간: 23.23 sec\n",
      "Epoch[14/20]\ttrain_loss: 0.0007\t훈련시간: 23.35 sec\n",
      "Epoch[15/20]\ttrain_loss: 0.0005\t훈련시간: 23.39 sec\n",
      "Epoch[16/20]\ttrain_loss: 0.0004\t훈련시간: 23.48 sec\n",
      "Epoch[17/20]\ttrain_loss: 0.0003\t훈련시간: 23.41 sec\n",
      "Epoch[18/20]\ttrain_loss: 0.0002\t훈련시간: 23.38 sec\n",
      "Epoch[19/20]\ttrain_loss: 0.0002\t훈련시간: 23.53 sec\n",
      "Epoch[20/20]\ttrain_loss: 0.0001\t훈련시간: 23.42 sec\n"
     ]
    }
   ],
   "source": [
    "GMF_model = GMF(num_users, num_movies, config.latent_dim)\n",
    "GMF_model.to(config.device)\n",
    "GMF_optimizer = torch.optim.Adam(GMF_model.parameters(), lr=config.lr)\n",
    "\n",
    "train(GMF_model, GMF_optimizer, train_loader, config.epochs, criterion, config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/20]\ttrain_loss: 0.1938\t훈련시간: 24.79 sec\n",
      "Epoch[2/20]\ttrain_loss: 0.0173\t훈련시간: 24.47 sec\n",
      "Epoch[3/20]\ttrain_loss: 0.0077\t훈련시간: 24.60 sec\n",
      "Epoch[4/20]\ttrain_loss: 0.0050\t훈련시간: 24.43 sec\n",
      "Epoch[5/20]\ttrain_loss: 0.0046\t훈련시간: 24.47 sec\n",
      "Epoch[6/20]\ttrain_loss: 0.0045\t훈련시간: 24.51 sec\n",
      "Epoch[7/20]\ttrain_loss: 0.0044\t훈련시간: 24.64 sec\n",
      "Epoch[8/20]\ttrain_loss: 0.0044\t훈련시간: 24.51 sec\n",
      "Epoch[9/20]\ttrain_loss: 0.0044\t훈련시간: 24.48 sec\n",
      "Epoch[10/20]\ttrain_loss: 0.0043\t훈련시간: 24.79 sec\n",
      "Epoch[11/20]\ttrain_loss: 0.0040\t훈련시간: 24.72 sec\n",
      "Epoch[12/20]\ttrain_loss: 0.0029\t훈련시간: 24.85 sec\n",
      "Epoch[13/20]\ttrain_loss: 0.0027\t훈련시간: 24.47 sec\n",
      "Epoch[14/20]\ttrain_loss: 0.0027\t훈련시간: 24.53 sec\n",
      "Epoch[15/20]\ttrain_loss: 0.0026\t훈련시간: 24.63 sec\n",
      "Epoch[16/20]\ttrain_loss: 0.0025\t훈련시간: 24.53 sec\n",
      "Epoch[17/20]\ttrain_loss: 0.0025\t훈련시간: 24.83 sec\n",
      "Epoch[18/20]\ttrain_loss: 0.0025\t훈련시간: 24.57 sec\n",
      "Epoch[19/20]\ttrain_loss: 0.0024\t훈련시간: 24.51 sec\n",
      "Epoch[20/20]\ttrain_loss: 0.0024\t훈련시간: 24.44 sec\n"
     ]
    }
   ],
   "source": [
    "MLP_model = MLP(num_users, num_movies, config.latent_dim, config.dropout, config.layers)\n",
    "MLP_model.to(config.device)\n",
    "MLP_optimizer = torch.optim.Adam(MLP_model.parameters(), lr=config.lr)\n",
    "\n",
    "train(MLP_model, MLP_optimizer, train_loader, config.epochs, criterion, config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
