{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from box import Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setSeed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset Preprocessing\n",
    "    \n",
    "* Implicit Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataInfo():\n",
    "    def __init__(self, file_path: str) -> None:\n",
    "        self.df = pd.read_csv(file_path)\n",
    "        self.user_encoder, self.movie_encoder = self._encode()\n",
    "\n",
    "        self.users = self.df['userId'].unique()\n",
    "        self.num_users = len(self.users)\n",
    "        self.min_feedback = 5 # for resolving cold-start problem\n",
    "        self.movies = self.df['movieId'].unique()\n",
    "        self.num_movies = len(self.movies)\n",
    "        self.train_movies = self._moreThanFeedback()\n",
    "        self.num_train_movies = len(self.train_movies)\n",
    "\n",
    "        self.df_pos_user_sequence, self.user_negative_samples = self._makeSequenceAndNeg()\n",
    "\n",
    "        self.df_pos_train, self.df_pos_test = self._trainTestSplit()\n",
    "    \n",
    "    def _trainTestSplit(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        # https://github.com/scikit-learn/scikit-learn/pull/16236\n",
    "        # GroupTimeSeriesSplit이 PR 중이라고 해서 직접 구현을 해야한다.\n",
    "        df_train = {\n",
    "            'user_id': list(),\n",
    "            'movie_id': list(),\n",
    "        }\n",
    "\n",
    "        df_test = {\n",
    "            'user_id': list(),\n",
    "            'movie_id': list(),\n",
    "        }\n",
    "\n",
    "        for user in self.users:\n",
    "            df_user = self.df_pos_user_sequence[self.df_pos_user_sequence['user_id']==user]\n",
    "            list_user_movies = df_user['movie_id'].tolist()\n",
    "\n",
    "            num_user = df_user.shape[0]\n",
    "            num_train_user = num_user - 1\n",
    "            num_test_user = 1\n",
    "\n",
    "            list_user_train_movies = list_user_movies[:num_train_user]\n",
    "            list_user_test_movies = list_user_movies[num_train_user:]\n",
    "\n",
    "            df_train['user_id'].extend([user]*num_train_user)\n",
    "            df_train['movie_id'].extend(list_user_train_movies)\n",
    "\n",
    "            df_test['user_id'].extend([user]*num_test_user)\n",
    "            df_test['movie_id'].extend(list_user_test_movies)\n",
    "            \n",
    "        df_train = pd.DataFrame(df_train)\n",
    "        df_train['label'] = 1\n",
    "\n",
    "        df_test = pd.DataFrame(df_test)\n",
    "        df_test['label'] = 1\n",
    "            \n",
    "        return df_train, df_test        \n",
    "\n",
    "\n",
    "    def _moreThanFeedback(self):\n",
    "        movie_ids_for_training = list()\n",
    "        for movie_id in self.df['movieId'].unique():\n",
    "            if self.df[self.df['movieId'] == movie_id].shape[0] >= self.min_feedback:\n",
    "                movie_ids_for_training.append(movie_id)\n",
    "        \n",
    "        return np.unique(movie_ids_for_training)\n",
    "\n",
    "\n",
    "    def _makeSequenceAndNeg(self):\n",
    "        pos_user_sequence = {\n",
    "            'user_id': list(),\n",
    "            'movie_id': list(),\n",
    "        }\n",
    "\n",
    "        user_negative_samples = dict()\n",
    "\n",
    "\n",
    "        for user in self.users:\n",
    "            user_sequence_movies = self.df[self.df['userId']==user].sort_values(by='timestamp', axis=0)['movieId'].tolist()\n",
    "            # 최조 조건을 만족하지 못한 것 포함, feedback있는 영화를 제외시킨다. \n",
    "            user_negative_movies = np.setdiff1d(self.movies, np.unique(user_sequence_movies))\n",
    "            user_negative_samples[user] = user_negative_movies\n",
    "\n",
    "            for movie in user_sequence_movies:\n",
    "                # 최소 feedback 조건을 만족시키지 못한 것은 추가하지 않는다.\n",
    "                if movie not in self.train_movies: continue\n",
    "                pos_user_sequence['user_id'].append(user)\n",
    "                pos_user_sequence['movie_id'].append(movie)\n",
    "\n",
    "\n",
    "        df_pos_user_sequence = pd.DataFrame(pos_user_sequence)\n",
    "        df_pos_user_sequence['label'] = 1\n",
    "\n",
    "        return df_pos_user_sequence, user_negative_samples\n",
    "\n",
    "\n",
    "    def _encode(self) -> Tuple[pd.DataFrame, LabelEncoder, LabelEncoder]:\n",
    "        userId_label_encoder = LabelEncoder()\n",
    "        movieId_label_encoder = LabelEncoder()\n",
    "\n",
    "        self.df['userId'] = userId_label_encoder.fit_transform(self.df['userId'].values)\n",
    "        self.df['movieId'] = movieId_label_encoder.fit_transform(self.df['movieId'].values)\n",
    "\n",
    "        # encoder.inverse_transform() 으로 decode\n",
    "        return userId_label_encoder, movieId_label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuMFDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user = self.df.iloc[idx]['user_id']\n",
    "        item = self.df.iloc[idx]['movie_id']\n",
    "        label = self.df.iloc[idx]['label']\n",
    "        \n",
    "        return torch.tensor(user).to(torch.int64), torch.tensor(item).to(torch.int64), torch.tensor(label).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMF(nn.Module):\n",
    "    def __init__(self, num_users: int, num_items: int, latent_dim: int):\n",
    "        super(GMF, self).__init__()\n",
    "\n",
    "        self.embedding_user = nn.Embedding(num_users, latent_dim)\n",
    "        self.embedding_item = nn.Embedding(num_items, latent_dim)\n",
    "\n",
    "        self.prediction = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for module in self.modules():\n",
    "            # print(module.__class__.__name__)\n",
    "            if isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight.data, mean=0.0, std=0.01)\n",
    "            elif isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.weight.data, 0, 0.01)\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data.fill_(0.0)\n",
    "\n",
    "    \n",
    "    def forward(self, user_input, item_input):\n",
    "        user_latent = self.embedding_user(user_input)\n",
    "        item_latent = self.embedding_item(item_input)\n",
    "\n",
    "        product = user_latent * item_latent\n",
    "\n",
    "        output = self.prediction(product)\n",
    "\n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_uesrs, num_items, latent_dim, dropout, layers=[20, 10]):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.embedding_user = nn.Embedding(num_uesrs, latent_dim)\n",
    "        self.embedding_item = nn.Embedding(num_items, latent_dim)\n",
    "\n",
    "        layers.insert(0, latent_dim * 2)\n",
    "        modules = []\n",
    "        for i in range(len(layers) - 1):\n",
    "            modules.append(nn.Dropout(p=dropout))\n",
    "            modules.append(nn.Linear(layers[i], layers[i+1]))\n",
    "            modules.append(nn.ReLU())\n",
    "        \n",
    "        self.dense_layers = nn.Sequential(*modules)\n",
    "\n",
    "        self.prediction = nn.Sequential(\n",
    "            nn.Linear(layers[-1], 1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    # initialize weights\n",
    "    def _init_weights(self, module):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight.data, mean=0.0, std=0.01)\n",
    "            elif isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.weight.data, 0, 0.01)\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, user_input, item_input):\n",
    "        user_latent = self.embedding_user(user_input)\n",
    "        item_latent = self.embedding_item(item_input)\n",
    "\n",
    "        vector = torch.cat((user_latent, item_latent), dim=-1)\n",
    "\n",
    "        output = self.prediction(self.dense_layers(vector))\n",
    "\n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuMF(nn.Module):\n",
    "    def __init__(self, GMF, MLP, latent_dim, layers):\n",
    "        super(NeuMF, self).__init__()\n",
    "\n",
    "        self.GMF_embedding_user = GMF.embedding_user\n",
    "        self.GMF_embedding_item = GMF.embedding_item\n",
    "\n",
    "        self.MLP_embedding_user = MLP.embedding_user\n",
    "        self.MLP_embedding_item = MLP.embedding_item\n",
    "\n",
    "        self.MLP_dense_layers = MLP.dense_layers\n",
    "\n",
    "        self.prediction = nn.Sequential(OrderedDict([\n",
    "            ('prediction', nn.Linear(latent_dim + layers[-1], 1, bias=False)),\n",
    "            ('prediction', nn.Sigmoid())\n",
    "\n",
    "        ]))\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    # initialize weights\n",
    "    def _init_weights(self, module):\n",
    "        for name, layer in module.named_modules():\n",
    "            if isinstance(layer, nn.Linear) and name == 'prediction':\n",
    "                nn.init.normal_(layer.weight.data, mean=0.0, std=0.01)\n",
    "                if layer.bias is not None:\n",
    "                    layer.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, user_input, item_input):\n",
    "        GMF_user_latent = self.GMF_embedding_user(user_input)\n",
    "        GMF_item_latent = self.GMF_embedding_item(item_input)\n",
    "        GMF_output = GMF_user_latent * GMF_item_latent\n",
    "\n",
    "        MLP_user_latent = self.MLP_embedding_user(user_input)\n",
    "        MLP_item_latent = self.MLP_embedding_item(item_input)\n",
    "        vector = torch.cat((MLP_user_latent, MLP_item_latent), dim=-1)\n",
    "        MLP_output = self.MLP_dense_layers(vector)\n",
    "\n",
    "        concat_output = torch.cat((GMF_output, MLP_output), dim=-1)\n",
    "\n",
    "        output = self.prediction(concat_output)\n",
    "\n",
    "        return output.squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train, Metric 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, model, optimizer, train_loader, test_loader, epochs, criterion, device, neg_smaples, num_neg, topK):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.epochs = epochs\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.neg_samples = neg_smaples\n",
    "        self.num_neg = num_neg\n",
    "        self.topK = topK\n",
    "\n",
    "        self.loss_list = []\n",
    "        self.hr_list = []\n",
    "        self.ndcg_list = []\n",
    "        \n",
    "        self._train_metric()\n",
    "    \n",
    "\n",
    "    def _train_metric(self):\n",
    "    # 훈련 시간 측정\n",
    "        epoch_start = torch.cuda.Event(enable_timing=True)\n",
    "        epoch_end = torch.cuda.Event(enable_timing=True)\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            # 시작 시간 기록\n",
    "            epoch_start.record()\n",
    "\n",
    "            avg_loss = self._train()\n",
    "            avg_hr, avg_ndcg = self._metric()\n",
    "        \n",
    "            epoch_end.record()\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "            self.loss_list.append(avg_loss)\n",
    "            self.hr_list.append(avg_hr)\n",
    "            self.ndcg_list.append(avg_ndcg)\n",
    "\n",
    "\n",
    "            print(\n",
    "                f'Epoch[{epoch+1}/{self.epochs}]\\ttrain_loss: {avg_loss:.4f}' +\n",
    "                f'\\tHR: {avg_hr:.4f}\\tNDCG: {avg_ndcg:.4f} '+\n",
    "                f'\\t훈련시간: {epoch_start.elapsed_time(epoch_end)/1000:.2f} sec'\n",
    "            )\n",
    "    \n",
    "    def _train(self):\n",
    "        self.model.train()\n",
    "\n",
    "        size = len(self.train_loader)\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for user, item, label in tqdm(self.train_loader):\n",
    "            user = user.to(self.device)\n",
    "            item = item.to(self.device)\n",
    "            label = label.to(self.device)\n",
    "\n",
    "            user_neg, item_neg, label_neg = self._negative_sampling(user, self.num_neg)\n",
    "            input_user, input_item, input_label = self._concat(user, user_neg, item, item_neg, label, label_neg)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            output = self.model(input_user, input_item)\n",
    "            loss = self.criterion(output, input_label)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "\n",
    "        avg_loss = epoch_loss / size\n",
    "\n",
    "        return avg_loss\n",
    "\n",
    "    def _metric(self):\n",
    "        self.model.eval()\n",
    "\n",
    "        size = len(self.test_loader)\n",
    "\n",
    "        epoch_hr = 0\n",
    "        epoch_ndcg = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for user, item, label in self.test_loader:\n",
    "                user = user.to(self.device)\n",
    "                item = item.to(self.device)\n",
    "                label = label.to(self.device).float()\n",
    "\n",
    "                user_neg, item_neg, label_neg = self._negative_sampling(user, 50)\n",
    "                input_user, input_item, input_label = self._concat(user, user_neg, item, item_neg, label, label_neg)\n",
    "\n",
    "                output = self.model(input_user, input_item)\n",
    "                _, indices = torch.topk(output, dim=0, k=self.topK)\n",
    "                rank_list = torch.take(input_item, indices).cpu().numpy().tolist()\n",
    "                target_item = item.cpu().numpy()[0]\n",
    "\n",
    "                epoch_hr += self._get_hit_ratio(rank_list, target_item)\n",
    "                epoch_ndcg += self._get_NDCG(rank_list, target_item)\n",
    "        \n",
    "        avg_hr = epoch_hr / size\n",
    "        avg_ndcg = epoch_ndcg / size\n",
    "\n",
    "        return avg_hr, avg_ndcg\n",
    "\n",
    "\n",
    "    \n",
    "    def _get_hit_ratio(self, rank_list, target_item):\n",
    "        for item in rank_list:\n",
    "            if item == target_item:\n",
    "                return 1\n",
    "        return 0\n",
    "\n",
    "\n",
    "    def _get_NDCG(self, rank_list, target_item):\n",
    "        for i in range(len(rank_list)):\n",
    "            item = rank_list[i]\n",
    "            if item == target_item:\n",
    "                return np.log(2) / np.log(i+2)\n",
    "        return 0\n",
    "            \n",
    "    def _negative_sampling(self, users, num_neg):\n",
    "        user_neg, item_neg, label_neg = [], [], []\n",
    "\n",
    "        users = users.detach().cpu().numpy()\n",
    "        for user in users:\n",
    "            items = np.random.choice(self.neg_samples[user], num_neg, replace=False)\n",
    "            for item in items:\n",
    "                user_neg.append(user)\n",
    "                item_neg.append(item)\n",
    "                label_neg.append(0)\n",
    "\n",
    "        \n",
    "        return (\n",
    "            torch.tensor(user_neg).to(torch.int64).to(self.device), \n",
    "            torch.tensor(item_neg).to(torch.int64).to(self.device), \n",
    "            torch.tensor(label_neg).to(self.device).float()\n",
    "        )\n",
    "\n",
    "\n",
    "    def _concat(self, user, user_neg, item, item_neg, label, label_neg):\n",
    "        input_user = torch.cat([user, user_neg], dim=0)\n",
    "        input_item = torch.cat([item, item_neg], dim=0)\n",
    "        input_label = torch.cat([label, label_neg], dim=0)\n",
    "\n",
    "        return  input_user, input_item, input_label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_base = os.path.join(os.path.join('/opt','ml','paper','RecSys'))\n",
    "dir_data = os.path.join(dir_base, 'Data', 'ml-latest-small')\n",
    "path_rating = os.path.join(dir_data, 'ratings.csv')\n",
    "dir_file_path = {\n",
    "    'dir_base': dir_base,\n",
    "    'dir_data': dir_data,\n",
    "    'rating': path_rating,\n",
    "}\n",
    "\n",
    "path = Box(dir_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Use {device}')\n",
    "\n",
    "# 해당 논문의 github에서 사용한 값 참고\n",
    "config = {\n",
    "    'seed': 42,\n",
    "    'device': device,\n",
    "    }\n",
    "\n",
    "config = Box(config)\n",
    "\n",
    "hyper = {\n",
    "    'batch_size': 256,\n",
    "    'epochs': 20,\n",
    "    'latent_dim': 8,\n",
    "    'num_neg': 3,\n",
    "    'lr': 0.001,\n",
    "    'optimizer': 'adam',\n",
    "    'criterion': 'BCE',\n",
    "    'layers': [64, 32, 16, 8],\n",
    "    'dropout': 0.2,\n",
    "    'topK': 10,\n",
    "    'num_neg': 4,\n",
    "}\n",
    "\n",
    "hyper = Box(hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataInfo(path.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataInfo(path.rating)\n",
    "\n",
    "torch.cuda.empty_cache() # if necessary\n",
    "\n",
    "train_dataset = NeuMFDataset(data.df_pos_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=hyper.batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "test_dataset = NeuMFDataset(data.df_pos_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMF_model = GMF(data.num_users, data.num_movies, hyper.latent_dim)\n",
    "GMF_model.to(config.device)\n",
    "GMF_optimizer = torch.optim.Adam(GMF_model.parameters(), lr=hyper.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_model = MLP(data.num_users, data.num_movies, hyper.latent_dim, hyper.dropout, hyper.layers)\n",
    "MLP_model.to(config.device)\n",
    "MLP_optimizer = torch.optim.Adam(MLP_model.parameters(), lr=hyper.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "NeuMF_model = NeuMF(GMF_model, MLP_model, hyper.latent_dim, hyper.layers)\n",
    "NeuMF_model.to(config.device)\n",
    "NeuMF_optimizer = torch.optim.Adam(NeuMF_model.parameters(), lr=hyper.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMF_trainer = Trainer(GMF_model, GMF_optimizer, train_loader, test_loader, hyper.epochs, criterion, config.device, data.user_negative_samples, hyper.num_neg, hyper.topK)\n",
    "MLP_trainer = Trainer(MLP_model, MLP_optimizer, train_loader, test_loader, hyper.epochs, criterion, config.device, data.user_negative_samples, hyper.num_neg, hyper.topK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/351 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([1280])) that is different to the input size (torch.Size([1280, 16])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m NeuMF_trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mNeuMF_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNeuMF_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_negative_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_neg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopK\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, optimizer, train_loader, test_loader, epochs, criterion, device, neg_smaples, num_neg, topK)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhr_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndcg_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mTrainer._train_metric\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# 시작 시간 기록\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     epoch_start\u001b[38;5;241m.\u001b[39mrecord()\n\u001b[0;32m---> 30\u001b[0m     avg_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     avg_hr, avg_ndcg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metric()\n\u001b[1;32m     33\u001b[0m     epoch_end\u001b[38;5;241m.\u001b[39mrecord()\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mTrainer._train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     63\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(input_user, input_item)\n\u001b[0;32m---> 64\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/loss.py:530\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:2518\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2516\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   2517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[0;32m-> 2518\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2519\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m   2521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2522\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([1280])) that is different to the input size (torch.Size([1280, 16])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "NeuMF_trainer = Trainer(NeuMF_model, NeuMF_optimizer, train_loader, test_loader, hyper.epochs, criterion, config.device, data.user_negative_samples, hyper.num_neg, hyper.topK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
