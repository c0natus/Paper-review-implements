{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Miniconda3\\envs\\rec\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcess():\n",
    "    def __init__(self, dir_path, file_name, *, sep=\"::\", str_cols=['user', 'item', 'rating', 'timestamp'],\n",
    "                ratio=(7, 1, 2), drop_num=9, drop_rating=3, use_cache=False):\n",
    "        \"\"\"\n",
    "        논문에 따라 pre-processing하는 코드.\n",
    "        Args:\n",
    "            dir_path    : Dataset directory path\n",
    "            file_name   : Interaction 파일 이름\n",
    "            sep         : Seperator\n",
    "            str_cols    : Interacton 파일에서 각 columns의 이름.\n",
    "            ratio       : Train, valid, test 비율\n",
    "            drop_num    : Unactive한 user 기준. 해당 수보다 interaction의 수가 큰 user만 사용.\n",
    "            drop_rating : User가 선호하는 item을 만족하는 기준. 해당 rating보다 큰 것을 선호한다고 판단.\n",
    "            user_cache  : Pre-processing을 된 파일 사용.\n",
    "        \"\"\"\n",
    "\n",
    "        self.str_user, self.str_item, self.str_rating, self.str_time = str_cols\n",
    "        clean_df_path = os.path.join(dir_path, 'clean_df.csv')\n",
    "        clean_train_path = os.path.join(dir_path, 'clean_train.csv')\n",
    "        clean_val_path = os.path.join(dir_path, 'clean_val.csv')\n",
    "        clean_test_path = os.path.join(dir_path, 'clean_test.csv')\n",
    "\n",
    "        if use_cache and \\\n",
    "            os.path.exists(clean_df_path) and \\\n",
    "            os.path.exists(clean_train_path) and \\\n",
    "            os.path.exists(clean_val_path) and \\\n",
    "            os.path.exists(clean_test_path):\n",
    "            # Pre-processing을 사용하고, 해당 파일들이 있으면 사용.\n",
    "            self.df_clean = pd.read_csv(clean_df_path)\n",
    "            self.df_train = pd.read_csv(clean_train_path)\n",
    "            self.df_val = pd.read_csv(clean_val_path)\n",
    "            self.df_test = pd.read_csv(clean_test_path)\n",
    "        else:\n",
    "            file_path = os.path.join(dir_path, file_name)\n",
    "            self.df = pd.read_csv(file_path, sep=sep, engine ='python', names=str_cols)\n",
    "\n",
    "            self.le_user = LabelEncoder() # user id encoder: 0, 3, 5, ... 처럼 연속되지 않을 수도 있기 때문에 연속적으로 만듦.\n",
    "            self.le_item = LabelEncoder() # item id encoder\n",
    "            self.df_clean = self.clean_and_sort(drop_num, drop_rating) # Unactive user를 drop하고 시간 순서대로 sorting\n",
    "            self.df_train, self.df_val, self.df_test = self.split_group_by_user(self.df_clean, ratio) # User마다 ratio만큼 train, valid, test data 만듦.\n",
    "\n",
    "            self.df_clean.to_csv(clean_df_path, index=False)\n",
    "            self.df_train.to_csv(clean_train_path, index=False)\n",
    "            self.df_val.to_csv(clean_val_path, index=False)\n",
    "            self.df_test.to_csv(clean_test_path, index=False)\n",
    "\n",
    "        self.num_users = self.df_clean[self.str_user].nunique() # User의 수: sparse matrix를 만들기 위해 필요\n",
    "        self.num_items = self.df_clean[self.str_item].nunique() # Item의 수: sparse matrix를 만들기 위해 필요\n",
    "\n",
    "        # Train, valid, test를 sparse matrix로 만듦.\n",
    "        self.sp_train = self.make_csr_matrix(self.df_train)\n",
    "        self.sp_val = self.make_csr_matrix(self.df_val)\n",
    "        self.sp_test = self.make_csr_matrix(self.df_test)\n",
    "\n",
    "    def clean_and_sort(self, drop_num, drop_rating):\n",
    "        df_clean = self.drop_unreliable(self.df, drop_rating)\n",
    "        df_clean = self.drop_unactive(df_clean, drop_num)\n",
    "        df_clean[self.str_rating] = 1.0 # rating 값을 사용하지 않고, implicit으로 사용.\n",
    "        df_sorted = df_clean.sort_values([self.str_user, self.str_time])\n",
    "        df_sorted[self.str_user] = self.le_user.fit_transform(df_sorted[self.str_user])\n",
    "        df_sorted[self.str_item] = self.le_user.fit_transform(df_sorted[self.str_item])\n",
    "        return df_sorted\n",
    "    \n",
    "    def drop_unreliable(self, df, drop_rating):\n",
    "        df_clean = df[df[self.str_rating] > drop_rating]\n",
    "        return df_clean\n",
    "\n",
    "    def drop_unactive(self, df, drop_num):\n",
    "        group_user_size = df.groupby(self.str_user).size()\n",
    "        clean_user = group_user_size[group_user_size > drop_num].index\n",
    "        df_clean = df[df[self.str_user].isin(clean_user)]\n",
    "        return df_clean\n",
    "    \n",
    "    def split_group_by_user(self, df, ratio):\n",
    "        group_users = df.groupby(self.str_user)\n",
    "        train = pd.DataFrame()\n",
    "        val = pd.DataFrame()\n",
    "        test = pd.DataFrame()\n",
    "\n",
    "        sum_ratio = sum(ratio)\n",
    "        test_ratio = round(ratio[2] / sum_ratio, 3)\n",
    "        sum_ratio -= ratio[2]\n",
    "        val_ratio = round(ratio[1] / sum_ratio, 3)\n",
    "\n",
    "        for _, df_user in group_users:\n",
    "            try:\n",
    "                sum_ratio = sum(ratio)\n",
    "                test_ratio = round(ratio[2] / sum_ratio, 3)\n",
    "                df_train_val, df_test = train_test_split(df_user, test_size=test_ratio, shuffle=False)\n",
    "                df_train, df_val = train_test_split(df_train_val, test_size=val_ratio, shuffle=False)\n",
    "                \n",
    "                train = pd.concat([train, df_train], ignore_index=True)\n",
    "                val = pd.concat([val, df_val], ignore_index=True)\n",
    "                test = pd.concat([test, df_test], ignore_index=True)\n",
    "            except:\n",
    "                print(df_user)\n",
    "        \n",
    "        return train, val, test\n",
    "    \n",
    "    def make_csr_matrix(self, df):\n",
    "        data = df[self.str_rating].values\n",
    "        row = df[self.str_user].values\n",
    "        col = df[self.str_item].values\n",
    "        return sp.csr_matrix((data, (row, col)), dtype='float32', shape=(self.num_users, self.num_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataDiffusion(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.data = dataset\n",
    "\n",
    "    def __getitem__(self, index): \n",
    "        item = self.data[index]\n",
    "        return item\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Diffusion and Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diffusion():\n",
    "    def __init__(self, steps=100, beta_start=1e-4, beta_end=0.02, device='cuda',\\\n",
    "            noise_scale=0.1, num_for_expectation=10):\n",
    "        \"\"\"\n",
    "        Forward diffusion 또는 주어진 model로 reverse diffusion한다.\n",
    "        Args:\n",
    "            steps               : reverse할 개수\n",
    "            beta_start          : Beta 시작 값, DDPM 논문에 적힌 1e-4 사용.\n",
    "            beta_end            : Beta 마지막 값, DDPM 논문에 적힌 0.02 사용. \n",
    "            noise_scale         : Beta를 생성할 때, noise의 정도를 조정하기 위해 사용.\n",
    "                                : 논문 3.4 personalized recommendation 1)의 마지막 문장 참고.\n",
    "            num_for_expectation : Importance sampling에 사용되는, expectation을 구하기 위해 필요한 loss의 수\n",
    "        \"\"\"\n",
    "        self.steps = steps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "        self.device = device\n",
    "        self.noise_scale = noise_scale\n",
    "\n",
    "        self.beta = torch.tensor(self.get_betas(), dtype=torch.float32).to(device)\n",
    "        self.alpha = 1.0 - self.beta\n",
    "        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n",
    "        self.alpha_bar_prev = torch.cat([torch.tensor([1.0]).to(device), self.alpha_bar[:-1]]).to(device) # 제일 처음 원소는 어차피 안 쓰임.\n",
    "        self.sqrt_alpha_bar = torch.sqrt(self.alpha_bar)\n",
    "        self.sqrt_one_minus_alpha_bar = torch.sqrt(1 - self.alpha_bar)\n",
    "\n",
    "        # 논문 수식 8\n",
    "        self.posterior_mean_coef1 = torch.sqrt(self.alpha) * (1 - self.alpha_bar_prev) / (1.0 - self.alpha_bar) # x_t 앞의 계수\n",
    "        self.posterior_mean_coef2 = torch.sqrt(self.alpha_bar_prev) * self.beta / (1.0 - self.alpha_bar) # x_0 앞의 계수\n",
    "        # 아래 부분은 posterior variance를 사용하는 값인 것 같다.\n",
    "        # 원래 DDPM에서는 beta를 사용했지만, 학습할 수도 있고, 아래처럼 다양한 것을 사용할 수 있다.\n",
    "        # 본 논문에서는 따로 언급은 없지만 log_var_clipped를 사용했다.\n",
    "        self.posterior_variance = self.beta * (1.0 - self.alpha_bar_prev) / (1.0 - self.alpha_bar)\n",
    "        self.posterior_log_variance_clipped = torch.log(\n",
    "            torch.cat([self.posterior_variance[1].unsqueeze(0), self.posterior_variance[1:]])\n",
    "        )\n",
    "\n",
    "        # Step을 sampling 하는 방법 중 importance sampling에 필요한 variable.\n",
    "        # Importance sampling은 DDPM을 향상 시키는 technique 중 하나.\n",
    "        # Importance sampling은 각 step마다 optimization의 어려움 정도가 다르다는 것을 가정한다.\n",
    "        # 그래서 Loss가 큰 step에 대한 학습을 강조하기 위해 importance sampling을 고려한다.\n",
    "        # 간단히 말하면 loss가 큰 step에 대해 sampling 확률을 높인다.\n",
    "        self.num_for_expectation = num_for_expectation # Monte Calro를 사용하기 위한 개수.\n",
    "        self.Lt_history = torch.zeros(steps, num_for_expectation, dtype=torch.float32).to(device)\n",
    "        self.Lt_count = torch.zeros(steps, dtype=int).to(device)\n",
    "\n",
    "    def get_betas(self, max_beta=0.999):\n",
    "        # DDPM에서는 beta가 linear하게 증가하도록 하고 있는데,\n",
    "        # 본 논문 eq 4는 alpha_bar가 linear하도록 beta를 설정하고 있다.\n",
    "        # 풀어서 써보면 alpha_bar = 1 - np.linspace(...)의 값을 가지게 된다.\n",
    "\n",
    "        start = self.noise_scale * self.beta_start\n",
    "        end = self.noise_scale * self.beta_end\n",
    "        alpha_bar = 1 - np.linspace(start, end, self.steps) # 1 - \\beta\n",
    "        betas = []\n",
    "        betas.append(1 - alpha_bar[0])\n",
    "        for i in range(1, self.steps):\n",
    "            betas.append(min(1 - alpha_bar[i] / alpha_bar[i - 1], max_beta))\n",
    "        return np.array(betas)\n",
    "\n",
    "    def sample_steps(self, batch_size, method='uniform', uniform_prob=0.001):\n",
    "        if method == 'importance':\n",
    "            if not (self.Lt_count == self.num_for_expectation).all():\n",
    "                # 모든 steps에 대한 loss가 num_for_expectation만큼 없으면 uniform 방식으로 sampling\n",
    "                return self.sample_steps(batch_size, method='uniform')\n",
    "\n",
    "            # 수식 14에 따라 sampling한다.\n",
    "            Lt_sqrt = torch.sqrt(torch.mean(self.Lt_history ** 2, axis=-1)) \n",
    "            # 10개 loss의 제곱에 대한 평균의 루트값.\n",
    "            # Lt_sqrt shape: (steps,)\n",
    "            \n",
    "            pt_prob = Lt_sqrt / torch.sum(Lt_sqrt)\n",
    "            pt_prob *= 1 - uniform_prob \n",
    "            pt_prob += uniform_prob / len(pt_prob)\n",
    "            # Loss의 크기에 따라 sampling 확률을 다르게 준다.\n",
    "            # 어느 정도 uniform_prob 만큼 sampling 되는 것을 보장.\n",
    "\n",
    "            step = torch.multinomial(input=pt_prob, num_samples=batch_size, replacement=True) \n",
    "            # 중복 sampling\n",
    "            pt = pt_prob.gather(dim=0, index=step) * len(pt_prob)\n",
    "            # 각 step의 확률 값을 가져온다.\n",
    "            # 수식 14에 따라 training에서 Lt / pt를 하기 위함.\n",
    "\n",
    "        elif method == 'uniform':\n",
    "            steps = torch.randint(low=1, high=self.steps, size=(batch_size,)).long()\n",
    "            pt = torch.ones_like(steps).float()\n",
    "            # loss의 평균 값을 구하기 때문에 len(pt)로 안 나눠줘도 된다.\n",
    "        \n",
    "        else: raise ValueError\n",
    "        \n",
    "        return steps.to(self.device), pt.to(self.device)\n",
    "\n",
    "    def get_noised_interaction(self, x_0, t):\n",
    "        \"\"\"\n",
    "        Noise를 추가한, 각 item에 대한 소비할 확률 값을 구하는 함수.\n",
    "        논문 수식 3 참고.\n",
    "        Args:\n",
    "            x_0 : Training dataset으로 만들어진 user interactions   (batch_size, num_items)\n",
    "            t   : noise step                                       (batch_size, )\n",
    "        \"\"\"\n",
    "        sqrt_alpha_bar = self.sqrt_alpha_bar[t][:, None]\n",
    "        mean_ = sqrt_alpha_bar * x_0\n",
    "        std_ = self.sqrt_one_minus_alpha_bar[t][:, None]\n",
    "        noise = torch.randn_like(x_0)\n",
    "        # 각 item마다 줄 noise를 sampling한다. -> reparameter trick에서 사용됨.\n",
    "\n",
    "        noised_interaction = mean_ + std_ * noise # reparmeter\n",
    "        return noised_interaction, noise\n",
    "\n",
    "\n",
    "    def sample_new_interaction(self, model, x_0, steps: int, sampling_noise=False):\n",
    "        \"\"\"\n",
    "        x_0부터 정해진 steps만큼 noise를 주고, \n",
    "        학습된 model을 가지고 reverse diffusion으로 추천을 생성한다.\n",
    "        Args:\n",
    "            model           : 학습된 model\n",
    "            x_0             : 초기 users의 interactions, shape: (batch_size, num_items), shape (batch_size, num_items)\n",
    "            steps           : Forward를 할 step, x_T까지 forward하지 않는다. 이유는 논문 3.3 참고.\n",
    "            sampling_noise  : 추천을 생성할 때, noise 추가 유무. False이면 variance 없이 mean 값만 사용.\n",
    "        \"\"\"\n",
    "\n",
    "        if steps == 0: \n",
    "            # noise를 전혀 추가하지 않고, reverse를 T번 진행\n",
    "            # 기존의 user의 interaction이 noise하다고 가정.\n",
    "            x_T = x_0\n",
    "        else:\n",
    "            T = torch.tensor([steps - 1] * x_0.shape[0]).to(x_0.device)\n",
    "            x_T = self.get_noised_interaction(x_0, T)\n",
    "\n",
    "        reverse_t = list(range(self.steps))[::-1]\n",
    "        \n",
    "        x_t = x_T\n",
    "        if self.noise_scale == 0:\n",
    "            # Denoise 과정이 없다.\n",
    "            # 즉, forward가 없다. 이러면 각 reverse는 x_t -> x_t를 복원하는 것이다.\n",
    "            # 결국 x_t -> x_t를 하는 AutoEndocer를 여러 개 쌓은 것과 같다.\n",
    "            for t_idx in reverse_t:\n",
    "                t = torch.tensor([t_idx] * x_t.shape[0]).to(x_0.device) # Shape: (batch_size, )\n",
    "                x_t = model(x_t, t)\n",
    "        else:\n",
    "            # Denosing 과정이 있다.\n",
    "            # x_t와 t가 주어졌을 때, p(x_{t-1}|x_t)의 평균과 분산 값을 구한다.\n",
    "            # 이를 통해 reparameterzation trick으로 x_{t-1}을 얻는다.\n",
    "            # 우리는 p(x_{t-1}|x_t)를 모른다. 여러 수식을 유도하면, p(x_{t-1}|x_t)의 likelihodd는 \n",
    "            # q(x_{t-1}|x_t, x_0)와 유사한 분포를 가지면 커진다.\n",
    "            # 이때, x_t는 알아도 x_0를 모른다. 그래서 학습된 model을 통해 x_0를 예측한 값을 사용한다.\n",
    "            for t_idx in reverse_t:\n",
    "                t = torch.tensor([t_idx] * x_t.shape[0]).to(x_0.device)\n",
    "                x_0_hat = model(x_t, t)\n",
    "                mean_hat = self.posterior_mean_coef1[t][:, None] * x_t + self.posterior_mean_coef2[t][:, None] * x_0_hat\n",
    "                if sampling_noise:\n",
    "                    # 추천을 생성할 때 uncertainty 추가. 즉, variance 사용\n",
    "                    variance = self.posterior_log_variance_clipped[t][:, None]\n",
    "                    if t_idx > 1: noise = torch.randn_like(x_t)\n",
    "                    else: noise = torch.zeros_like(x_t) # t == 0일 때 noise를 주지 않는다.\n",
    "                    x_t = mean_hat + torch.exp(0.5 * variance) * noise\n",
    "                else:\n",
    "                    # 추천을 생성할 때 variance를 사용하지 않음.\n",
    "                    # 따라서 평균값만 사용.\n",
    "                    x_t = mean_hat\n",
    "        return x_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEmb(nn.Module):\n",
    "    def __init__(self, pos_dim):\n",
    "        super(PosEmb, self).__init__()\n",
    "        \"\"\"\n",
    "        Sinusoidal timestep positional encoding.\n",
    "        Args\n",
    "            pos_dim : embedding dimension\n",
    "        \"\"\"\n",
    "        self.pos_dim = pos_dim\n",
    "        self.time_mlp = nn.Linear(pos_dim, pos_dim)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.normal_(m.bias.data, mean=0.0, std=0.001)\n",
    "\n",
    "    def forward(self, t, max_period=10000):\n",
    "        t = t.unsqueeze(-1).type(torch.float)\n",
    "        half_dim = self.pos_dim // 2\n",
    "        w_k = 1.0 / (\n",
    "            max_period\n",
    "            ** (torch.arange(0, half_dim, 1, device=t.device).float() / (half_dim-1))\n",
    "        )\n",
    "\n",
    "        half_emb = t.repeat(1, half_dim)\n",
    "        pos_sin = torch.sin(half_emb * w_k)\n",
    "        pos_cos = torch.cos(half_emb * w_k)\n",
    "        pos_enc = torch.cat([pos_sin, pos_cos], dim=-1)\n",
    "\n",
    "        emb = self.time_mlp(pos_enc)\n",
    "        return emb\n",
    "\n",
    "\n",
    "class ReverseDiffusion(nn.Module):\n",
    "    def __init__(self, in_dims, out_dims, step_dim=10, norm=False, droupout=0.5):\n",
    "        super(ReverseDiffusion, self).__init__()\n",
    "        \"\"\"\n",
    "        AutoEncoder 구조를 활용한다.\n",
    "        Args\n",
    "            in_dims     : [num_items, ...]\n",
    "            out_dims    : [..., num_itmes]\n",
    "            step_dim    : timestep positional embedding dim.\n",
    "        \"\"\"\n",
    "\n",
    "        self.norm = norm\n",
    "        self.pos_enc_layer = PosEmb(step_dim)\n",
    "        self.encoder = nn.ModuleList([])\n",
    "        self.decoder = nn.ModuleList([])\n",
    "\n",
    "        in_dims_w_step = [in_dims[0] + step_dim] + in_dims[1:]\n",
    "        for d_in, d_out in zip(in_dims_w_step[:-1], in_dims_w_step[1:]):\n",
    "            self.encoder.append(nn.Linear(d_in, d_out))\n",
    "            self.encoder.append(nn.Tanh())\n",
    "        for d_in, d_out in zip(out_dims[:-1], out_dims[1:]):\n",
    "            self.decoder.append(nn.Linear(d_in, d_out))\n",
    "\n",
    "        self.drop = nn.Dropout(droupout)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self,):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.normal_(m.bias.data, mean=0.0, std=0.001)\n",
    "\n",
    "    def forward(self, x, timesteps):\n",
    "        time_emb = self.pos_enc_layer(timesteps)\n",
    "        if self.norm: x = F.normalize(x)\n",
    "        x = self.drop(x)\n",
    "        h = torch.cat([x, time_emb], dim=-1)\n",
    "        for idx, layer in enumerate(self.encoder):\n",
    "            h = layer(h)\n",
    "        for idx, layer in enumerate(self.decoder):\n",
    "            h = layer(h)\n",
    "            if idx != len(self.decoder) - 1:\n",
    "                h = torch.tanh(h)\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SNR(diffusion, t):\n",
    "    \"\"\"\n",
    "    Compute the signal-to-noise ratio for a single timestep.\n",
    "    \"\"\"\n",
    "    diffusion.alpha_bar = diffusion.alpha_bar.to(t.device)\n",
    "    return diffusion.alpha_bar[t] / (1 - diffusion.alpha_bar[t])\n",
    "\n",
    "def caculate_loss(args, model, diffusion, x_0):\n",
    "    \"\"\"\n",
    "    Importance sampling을 사용하기 위해 nn.MSE로 loss를 계산하지 않는다.\n",
    "    Batch별 즉, user별 loss를 계산한 다음 importance sampling을 위해 Lt_history에 저장한다.\n",
    "    그 다음, batch별 평균 값을 loss로 활용한다.\n",
    "    \"\"\"\n",
    "\n",
    "    timesteps, pt = diffusion.sample_steps(x_0.shape[0])\n",
    "    if args.noise_scale != 0:\n",
    "        x_t, noise = diffusion.get_noised_interaction(x_0, timesteps)\n",
    "    else:\n",
    "        # Denoise 과정이 없다.\n",
    "        # 즉, forward가 없다. 이러면 각 reverse는 x_t -> x_t를 복원하는 것이다.\n",
    "        # 결국 x_t -> x_t를 하는 AutoEndocer를 여러 개 쌓은 것과 같다.\n",
    "        x_t = x_0\n",
    "\n",
    "\n",
    "    x_0_hat = model(x_t, timesteps)\n",
    "\n",
    "    loss_batch_item = (x_0_hat - x_0) ** 2\n",
    "    loss_batch = loss_batch_item.mean(dim=1)\n",
    "\n",
    "    if args.snr is True:\n",
    "        # timestep마다 loss weight를 다르게 둔다.\n",
    "        weight = SNR(diffusion, timesteps - 1) - SNR(diffusion, timesteps)\n",
    "        weight = torch.where((timesteps == 0), 1.0, weight)\n",
    "    else:\n",
    "        weight = torch.tensor([1.0] * x_0.shape[0]).to(args.device)\n",
    "    \n",
    "    weighted_loss_batch = weight * loss_batch\n",
    "\n",
    "    # Update Lt_history & Lt_count for importance sampling\n",
    "    for timestep, loss in zip(timesteps, weighted_loss_batch):\n",
    "        # loss는 timestep에 해당하는 loss 값이다.\n",
    "        if diffusion.Lt_count[timestep] == diffusion.num_for_expectation:\n",
    "            # 만약 history가 꽉 찼으면 old한 것을 버리고 새 것으로 채운다.\n",
    "            Lt_history_old = diffusion.Lt_history.clone()\n",
    "            diffusion.Lt_history[timestep, :-1] = Lt_history_old[timestep, 1:]\n",
    "            diffusion.Lt_history[timestep, -1] = loss.detach()\n",
    "        else:\n",
    "            try:\n",
    "                diffusion.Lt_history[timestep, diffusion.Lt_count[timestep]] = loss.detach()\n",
    "                diffusion.Lt_count[timestep] += 1\n",
    "            except:\n",
    "                print(timestep)\n",
    "                print(diffusion.Lt_count[timestep])\n",
    "                print(loss)\n",
    "                raise ValueError\n",
    "    \n",
    "    weighted_loss_batch /= pt # 논문 수식 14 참고.\n",
    "    weighted_loss = weighted_loss_batch.mean()\n",
    "    \n",
    "    return weighted_loss\n",
    "\n",
    "\n",
    "def train_one_epoch(args, model, diffusion, optimizer, dataloader):\n",
    "    batch_count = 0\n",
    "    total_loss = 0.0\n",
    "    for x_0 in dataloader:\n",
    "        x_0 = x_0.to(args.device)\n",
    "        batch_count += 1\n",
    "        loss = caculate_loss(args, model, diffusion, x_0)\n",
    "\n",
    "        total_loss += loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def compute_metric(target_items, predict_items, topK):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    ndcgs = []\n",
    "    mrrs = []\n",
    "    num_users = len(predict_items)\n",
    "\n",
    "    for k in topK:\n",
    "        sum_precision = sum_recall = sum_ndcg = sum_mrr = 0.0\n",
    "        for user_id in range(num_users):\n",
    "            if len(target_items[user_id]) == 0: continue\n",
    "            mrr_flag = True\n",
    "            num_hit = user_mrr = dcg = 0\n",
    "            \n",
    "            for rank_idx in range(k):\n",
    "                if predict_items[user_id][rank_idx] in target_items[user_id]:\n",
    "                    num_hit += 1 # precision, recall에 사용\n",
    "                    dcg += 1.0 / np.log2(rank_idx + 2)                    \n",
    "                    if mrr_flag:\n",
    "                        user_mrr = 1.0 / (rank_idx+1.0)\n",
    "                        mrr_flag = False\n",
    "            \n",
    "            idcg = 0.0\n",
    "            for rank_idx in range(len(target_items[user_id])):\n",
    "                idcg += 1.0/np.log2(rank_idx+2)\n",
    "            ndcg = (dcg/idcg)\n",
    "\n",
    "            sum_precision += num_hit / k\n",
    "            sum_recall += num_hit / len(target_items[user_id])\n",
    "            sum_ndcg += ndcg\n",
    "            sum_mrr += user_mrr\n",
    "\n",
    "        precision = round(sum_precision / num_users, 4)\n",
    "        recall = round(sum_recall / num_users, 4)\n",
    "        ndcg = round(sum_ndcg / num_users, 4)\n",
    "        mrr = round(sum_mrr / num_users, 4)\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        ndcgs.append(ndcg)\n",
    "        mrrs.append(mrr)\n",
    "\n",
    "    return precisions, recalls, ndcgs, mrrs\n",
    "\n",
    "\n",
    "def evaluate(args, model, diffusion, loader, label_items: sp.csr_matrix, \\\n",
    "            consumed_items: sp.csr_matrix, topK: list):\n",
    "    \"\"\"\n",
    "    Args\n",
    "        args                    : hyper-parameters\n",
    "        model                   : 학습된 model\n",
    "        diffsuion               : Diffusion\n",
    "        loader                  : Test data loader // no_shffule\n",
    "        label_items             : Ground Truth, shape: (num_users, num_items) 중에서 target item에만 1\n",
    "        consumed_items   : training data에서 사용된 이미 user가 선호도를 보인 items\n",
    "        topK                    : top K list ex) [10, 20, 50]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    num_user = label_items.shape[0]\n",
    "    user_idx_list = list(range(label_items.shape[0]))\n",
    "    # target_items.shape[0] 대신 consumed_items.shape[0]도 ㄱㅊ\n",
    "\n",
    "    predict_items = []\n",
    "    target_items = []\n",
    "\n",
    "    for user_id in range(num_user):\n",
    "        # user_id에 해당하는, sp.csr_matrix로 저장되어 있는 user의 label item id를 list로 저장.\n",
    "        # nonzero()하면 (row array, col array) 반환.\n",
    "        # col array: np.ndarray의 idx 값이 item id임.\n",
    "        target_items.append(label_items[user_id,:].nonzero()[1].tolist())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, x_0 in enumerate(loader):\n",
    "            start_batch_user_id = batch_idx*args.batch_size\n",
    "            end_batch_user_id = start_batch_user_id + len(x_0)\n",
    "            batch_consumed_items = consumed_items[user_idx_list[start_batch_user_id:end_batch_user_id]]\n",
    "            x_0 = x_0.to(args.device)\n",
    "            prediction = diffusion.sample_new_interaction(model, x_0, steps=args.sampling_steps, sampling_noise=False)\n",
    "            prediction[batch_consumed_items.nonzero()] = -np.inf\n",
    "\n",
    "            _, indices = torch.topk(prediction, topK[-1]) # shape (x_0[1].shape, topK[-1])\n",
    "            indices = indices.detach().cpu().numpy().tolist()\n",
    "            predict_items.extend(indices)\n",
    "\n",
    "        precisions, recalls, ndcgs, mrrs = compute_metric(target_items, predict_items, topK)\n",
    "    \n",
    "    return precisions, recalls, ndcgs, mrrs\n",
    "\n",
    "\n",
    "def print_metric_results(topK, results):\n",
    "    metric_list = ['Precision', 'Recall', 'nDCG', 'MRR']\n",
    "    for idx, metric in enumerate(metric_list):\n",
    "        str_result = ''\n",
    "        for k_idx, k in enumerate(topK):\n",
    "            str_metric = f'{metric}@{k}'\n",
    "            str_result += f'    {str_metric:14s}: {results[idx][k_idx]:.4f}'\n",
    "        print(str_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "Epoch 1  train loss 42.5007\n",
      "Epoch 2  train loss 37.5769\n",
      "Epoch 3  train loss 36.4889\n",
      "Epoch 4  train loss 33.4127\n",
      "Epoch 5  train loss 35.7734\n",
      "  Update Best\n",
      "  Validation data\n",
      "    Precision@10  : 0.0388    Precision@20  : 0.0352    Precision@50  : 0.0282    Precision@100 : 0.0223\n",
      "    Recall@10     : 0.0486    Recall@20     : 0.0859    Recall@50     : 0.1667    Recall@100    : 0.2536\n",
      "    nDCG@10       : 0.0429    nDCG@20       : 0.0604    nDCG@50       : 0.0908    nDCG@100      : 0.1183\n",
      "    MRR@10        : 0.1006    MRR@20        : 0.1096    MRR@50        : 0.1155    MRR@100       : 0.1174\n",
      "  Test data\n",
      "    Precision@10  : 0.0636    Precision@20  : 0.0590    Precision@50  : 0.0466    Precision@100 : 0.0381\n",
      "    Recall@10     : 0.0393    Recall@20     : 0.0754    Recall@50     : 0.1423    Recall@100    : 0.2250\n",
      "    nDCG@10       : 0.0438    nDCG@20       : 0.0647    nDCG@50       : 0.0967    nDCG@100      : 0.1295\n",
      "    MRR@10        : 0.1478    MRR@20        : 0.1588    MRR@50        : 0.1641    MRR@100       : 0.1658\n",
      "Epoch 6  train loss 37.0305\n",
      "Epoch 7  train loss 28.6668\n",
      "Epoch 8  train loss 37.7229\n",
      "Epoch 9  train loss 34.7354\n",
      "Epoch 10  train loss 31.0492\n",
      "  Update Best\n",
      "  Validation data\n",
      "    Precision@10  : 0.0406    Precision@20  : 0.0379    Precision@50  : 0.0298    Precision@100 : 0.0239\n",
      "    Recall@10     : 0.0497    Recall@20     : 0.0923    Recall@50     : 0.1771    Recall@100    : 0.2691\n",
      "    nDCG@10       : 0.0449    nDCG@20       : 0.0649    nDCG@50       : 0.0966    nDCG@100      : 0.1262\n",
      "    MRR@10        : 0.1065    MRR@20        : 0.1162    MRR@50        : 0.1225    MRR@100       : 0.1244\n",
      "  Test data\n",
      "    Precision@10  : 0.0645    Precision@20  : 0.0610    Precision@50  : 0.0494    Precision@100 : 0.0405\n",
      "    Recall@10     : 0.0390    Recall@20     : 0.0778    Recall@50     : 0.1535    Recall@100    : 0.2410\n",
      "    nDCG@10       : 0.0453    nDCG@20       : 0.0677    nDCG@50       : 0.1033    nDCG@100      : 0.1381\n",
      "    MRR@10        : 0.1530    MRR@20        : 0.1651    MRR@50        : 0.1711    MRR@100       : 0.1728\n",
      "Epoch 11  train loss 29.8084\n",
      "Epoch 12  train loss 27.5549\n",
      "Epoch 13  train loss 31.9958\n",
      "Epoch 14  train loss 24.7478\n",
      "Epoch 15  train loss 33.3325\n",
      "  Validation data\n",
      "    Precision@10  : 0.0419    Precision@20  : 0.0383    Precision@50  : 0.0303    Precision@100 : 0.0242\n",
      "    Recall@10     : 0.0501    Recall@20     : 0.0922    Recall@50     : 0.1770    Recall@100    : 0.2752\n",
      "    nDCG@10       : 0.0452    nDCG@20       : 0.0649    nDCG@50       : 0.0970    nDCG@100      : 0.1278\n",
      "    MRR@10        : 0.1061    MRR@20        : 0.1158    MRR@50        : 0.1218    MRR@100       : 0.1239\n",
      "  Test data\n",
      "    Precision@10  : 0.0661    Precision@20  : 0.0622    Precision@50  : 0.0500    Precision@100 : 0.0412\n",
      "    Recall@10     : 0.0404    Recall@20     : 0.0791    Recall@50     : 0.1530    Recall@100    : 0.2435\n",
      "    nDCG@10       : 0.0461    nDCG@20       : 0.0684    nDCG@50       : 0.1034    nDCG@100      : 0.1393\n",
      "    MRR@10        : 0.1530    MRR@20        : 0.1645    MRR@50        : 0.1701    MRR@100       : 0.1719\n",
      "Epoch 16  train loss 26.8194\n",
      "Epoch 17  train loss 29.0267\n",
      "Epoch 18  train loss 25.3659\n",
      "Epoch 19  train loss 27.3426\n",
      "Epoch 20  train loss 27.0490\n",
      "  Update Best\n",
      "  Validation data\n",
      "    Precision@10  : 0.0423    Precision@20  : 0.0388    Precision@50  : 0.0307    Precision@100 : 0.0248\n",
      "    Recall@10     : 0.0506    Recall@20     : 0.0945    Recall@50     : 0.1821    Recall@100    : 0.2854\n",
      "    nDCG@10       : 0.0460    nDCG@20       : 0.0663    nDCG@50       : 0.0991    nDCG@100      : 0.1315\n",
      "    MRR@10        : 0.1087    MRR@20        : 0.1183    MRR@50        : 0.1244    MRR@100       : 0.1265\n",
      "  Test data\n",
      "    Precision@10  : 0.0659    Precision@20  : 0.0621    Precision@50  : 0.0501    Precision@100 : 0.0420\n",
      "    Recall@10     : 0.0402    Recall@20     : 0.0790    Recall@50     : 0.1546    Recall@100    : 0.2499\n",
      "    nDCG@10       : 0.0459    nDCG@20       : 0.0683    nDCG@50       : 0.1040    nDCG@100      : 0.1416\n",
      "    MRR@10        : 0.1527    MRR@20        : 0.1646    MRR@50        : 0.1705    MRR@100       : 0.1723\n",
      "Epoch 21  train loss 22.0101\n",
      "Epoch 22  train loss 26.9434\n",
      "Epoch 23  train loss 28.6552\n",
      "Epoch 24  train loss 21.9420\n",
      "Epoch 25  train loss 23.7862\n",
      "  Update Best\n",
      "  Validation data\n",
      "    Precision@10  : 0.0437    Precision@20  : 0.0397    Precision@50  : 0.0314    Precision@100 : 0.0254\n",
      "    Recall@10     : 0.0520    Recall@20     : 0.0976    Recall@50     : 0.1873    Recall@100    : 0.2890\n",
      "    nDCG@10       : 0.0473    nDCG@20       : 0.0684    nDCG@50       : 0.1018    nDCG@100      : 0.1343\n",
      "    MRR@10        : 0.1121    MRR@20        : 0.1226    MRR@50        : 0.1285    MRR@100       : 0.1305\n",
      "  Test data\n",
      "    Precision@10  : 0.0672    Precision@20  : 0.0635    Precision@50  : 0.0509    Precision@100 : 0.0422\n",
      "    Recall@10     : 0.0412    Recall@20     : 0.0813    Recall@50     : 0.1589    Recall@100    : 0.2508\n",
      "    nDCG@10       : 0.0471    nDCG@20       : 0.0705    nDCG@50       : 0.1067    nDCG@100      : 0.1434\n",
      "    MRR@10        : 0.1566    MRR@20        : 0.1693    MRR@50        : 0.1753    MRR@100       : 0.1769\n",
      "Epoch 26  train loss 27.4085\n",
      "Epoch 27  train loss 25.6447\n",
      "Epoch 28  train loss 25.2618\n",
      "Epoch 29  train loss 22.5102\n",
      "Epoch 30  train loss 24.8852\n",
      "  Update Best\n",
      "  Validation data\n",
      "    Precision@10  : 0.0453    Precision@20  : 0.0406    Precision@50  : 0.0327    Precision@100 : 0.0264\n",
      "    Recall@10     : 0.0567    Recall@20     : 0.1024    Recall@50     : 0.1969    Recall@100    : 0.3022\n",
      "    nDCG@10       : 0.0500    nDCG@20       : 0.0712    nDCG@50       : 0.1066    nDCG@100      : 0.1404\n",
      "    MRR@10        : 0.1155    MRR@20        : 0.1258    MRR@50        : 0.1321    MRR@100       : 0.1339\n",
      "  Test data\n",
      "    Precision@10  : 0.0695    Precision@20  : 0.0627    Precision@50  : 0.0523    Precision@100 : 0.0437\n",
      "    Recall@10     : 0.0455    Recall@20     : 0.0824    Recall@50     : 0.1657    Recall@100    : 0.2620\n",
      "    nDCG@10       : 0.0499    nDCG@20       : 0.0713    nDCG@50       : 0.1101    nDCG@100      : 0.1488\n",
      "    MRR@10        : 0.1621    MRR@20        : 0.1732    MRR@50        : 0.1796    MRR@100       : 0.1811\n",
      "Epoch 31  train loss 26.6912\n",
      "Epoch 32  train loss 22.2995\n",
      "Epoch 33  train loss 22.2562\n",
      "Epoch 34  train loss 21.7904\n",
      "Epoch 35  train loss 23.3030\n",
      "  Update Best\n",
      "  Validation data\n",
      "    Precision@10  : 0.0464    Precision@20  : 0.0414    Precision@50  : 0.0335    Precision@100 : 0.0264\n",
      "    Recall@10     : 0.0596    Recall@20     : 0.1038    Recall@50     : 0.1968    Recall@100    : 0.2909\n",
      "    nDCG@10       : 0.0516    nDCG@20       : 0.0725    nDCG@50       : 0.1082    nDCG@100      : 0.1396\n",
      "    MRR@10        : 0.1169    MRR@20        : 0.1268    MRR@50        : 0.1330    MRR@100       : 0.1347\n",
      "  Test data\n",
      "    Precision@10  : 0.0698    Precision@20  : 0.0634    Precision@50  : 0.0526    Precision@100 : 0.0436\n",
      "    Recall@10     : 0.0473    Recall@20     : 0.0837    Recall@50     : 0.1626    Recall@100    : 0.2498\n",
      "    nDCG@10       : 0.0506    nDCG@20       : 0.0721    nDCG@50       : 0.1101    nDCG@100      : 0.1466\n",
      "    MRR@10        : 0.1619    MRR@20        : 0.1730    MRR@50        : 0.1791    MRR@100       : 0.1805\n",
      "Epoch 36  train loss 24.3471\n",
      "Epoch 37  train loss 26.2923\n",
      "Epoch 38  train loss 20.4561\n",
      "Epoch 39  train loss 22.7427\n",
      "Epoch 40  train loss 22.4935\n",
      "  Update Best\n",
      "  Validation data\n",
      "    Precision@10  : 0.0464    Precision@20  : 0.0423    Precision@50  : 0.0337    Precision@100 : 0.0272\n",
      "    Recall@10     : 0.0610    Recall@20     : 0.1087    Recall@50     : 0.2010    Recall@100    : 0.3038\n",
      "    nDCG@10       : 0.0529    nDCG@20       : 0.0751    nDCG@50       : 0.1106    nDCG@100      : 0.1444\n",
      "    MRR@10        : 0.1162    MRR@20        : 0.1267    MRR@50        : 0.1329    MRR@100       : 0.1346\n",
      "  Test data\n",
      "    Precision@10  : 0.0741    Precision@20  : 0.0660    Precision@50  : 0.0543    Precision@100 : 0.0447\n",
      "    Recall@10     : 0.0511    Recall@20     : 0.0875    Recall@50     : 0.1688    Recall@100    : 0.2591\n",
      "    nDCG@10       : 0.0541    nDCG@20       : 0.0757    nDCG@50       : 0.1146    nDCG@100      : 0.1520\n",
      "    MRR@10        : 0.1667    MRR@20        : 0.1769    MRR@50        : 0.1831    MRR@100       : 0.1845\n",
      "Epoch 41  train loss 23.2443\n",
      "Epoch 42  train loss 18.2543\n",
      "Epoch 43  train loss 18.4817\n",
      "Epoch 44  train loss 23.3486\n",
      "Epoch 45  train loss 17.6134\n",
      "  Update Best\n",
      "  Validation data\n",
      "    Precision@10  : 0.0483    Precision@20  : 0.0431    Precision@50  : 0.0343    Precision@100 : 0.0274\n",
      "    Recall@10     : 0.0630    Recall@20     : 0.1096    Recall@50     : 0.2022    Recall@100    : 0.2990\n",
      "    nDCG@10       : 0.0543    nDCG@20       : 0.0762    nDCG@50       : 0.1123    nDCG@100      : 0.1448\n",
      "    MRR@10        : 0.1194    MRR@20        : 0.1294    MRR@50        : 0.1357    MRR@100       : 0.1373\n",
      "  Test data\n",
      "    Precision@10  : 0.0737    Precision@20  : 0.0654    Precision@50  : 0.0539    Precision@100 : 0.0443\n",
      "    Recall@10     : 0.0503    Recall@20     : 0.0858    Recall@50     : 0.1652    Recall@100    : 0.2511\n",
      "    nDCG@10       : 0.0548    nDCG@20       : 0.0760    nDCG@50       : 0.1144    nDCG@100      : 0.1508\n",
      "    MRR@10        : 0.1733    MRR@20        : 0.1833    MRR@50        : 0.1894    MRR@100       : 0.1907\n",
      "Epoch 46  train loss 21.6201\n",
      "Epoch 47  train loss 21.0718\n",
      "Epoch 48  train loss 18.2615\n",
      "Epoch 49  train loss 17.5965\n",
      "Epoch 50  train loss 17.2199\n",
      "  Update Best\n",
      "  Validation data\n",
      "    Precision@10  : 0.0491    Precision@20  : 0.0431    Precision@50  : 0.0350    Precision@100 : 0.0283\n",
      "    Recall@10     : 0.0647    Recall@20     : 0.1111    Recall@50     : 0.2071    Recall@100    : 0.3120\n",
      "    nDCG@10       : 0.0558    nDCG@20       : 0.0775    nDCG@50       : 0.1148    nDCG@100      : 0.1497\n",
      "    MRR@10        : 0.1229    MRR@20        : 0.1327    MRR@50        : 0.1392    MRR@100       : 0.1409\n",
      "  Test data\n",
      "    Precision@10  : 0.0731    Precision@20  : 0.0663    Precision@50  : 0.0546    Precision@100 : 0.0453\n",
      "    Recall@10     : 0.0508    Recall@20     : 0.0884    Recall@50     : 0.1684    Recall@100    : 0.2586\n",
      "    nDCG@10       : 0.0547    nDCG@20       : 0.0770    nDCG@50       : 0.1158    nDCG@100      : 0.1538\n",
      "    MRR@10        : 0.1707    MRR@20        : 0.1817    MRR@50        : 0.1874    MRR@100       : 0.1887\n",
      "Epoch 51  train loss 21.9905\n",
      "Epoch 52  train loss 19.5093\n",
      "Epoch 53  train loss 18.8936\n",
      "Epoch 54  train loss 23.9104\n",
      "Epoch 55  train loss 21.5812\n",
      "  Update Best\n",
      "  Validation data\n",
      "    Precision@10  : 0.0498    Precision@20  : 0.0438    Precision@50  : 0.0353    Precision@100 : 0.0286\n",
      "    Recall@10     : 0.0660    Recall@20     : 0.1150    Recall@50     : 0.2145    Recall@100    : 0.3223\n",
      "    nDCG@10       : 0.0572    nDCG@20       : 0.0797    nDCG@50       : 0.1178    nDCG@100      : 0.1534\n",
      "    MRR@10        : 0.1263    MRR@20        : 0.1366    MRR@50        : 0.1429    MRR@100       : 0.1446\n",
      "  Test data\n",
      "    Precision@10  : 0.0755    Precision@20  : 0.0666    Precision@50  : 0.0549    Precision@100 : 0.0453\n",
      "    Recall@10     : 0.0525    Recall@20     : 0.0892    Recall@50     : 0.1738    Recall@100    : 0.2648\n",
      "    nDCG@10       : 0.0581    nDCG@20       : 0.0797    nDCG@50       : 0.1200    nDCG@100      : 0.1580\n",
      "    MRR@10        : 0.1865    MRR@20        : 0.1965    MRR@50        : 0.2026    MRR@100       : 0.2040\n",
      "Epoch 56  train loss 21.9679\n",
      "Epoch 57  train loss 23.6074\n",
      "Epoch 58  train loss 19.0604\n",
      "Epoch 59  train loss 18.2019\n",
      "Epoch 60  train loss 18.9698\n",
      "  Update Best\n",
      "  Validation data\n",
      "    Precision@10  : 0.0481    Precision@20  : 0.0437    Precision@50  : 0.0352    Precision@100 : 0.0285\n",
      "    Recall@10     : 0.0650    Recall@20     : 0.1155    Recall@50     : 0.2207    Recall@100    : 0.3350\n",
      "    nDCG@10       : 0.0567    nDCG@20       : 0.0801    nDCG@50       : 0.1194    nDCG@100      : 0.1562\n",
      "    MRR@10        : 0.1245    MRR@20        : 0.1351    MRR@50        : 0.1422    MRR@100       : 0.1439\n",
      "  Test data\n",
      "    Precision@10  : 0.0734    Precision@20  : 0.0658    Precision@50  : 0.0544    Precision@100 : 0.0453\n",
      "    Recall@10     : 0.0516    Recall@20     : 0.0905    Recall@50     : 0.1762    Recall@100    : 0.2720\n",
      "    nDCG@10       : 0.0565    nDCG@20       : 0.0790    nDCG@50       : 0.1195    nDCG@100      : 0.1589\n",
      "    MRR@10        : 0.1750    MRR@20        : 0.1856    MRR@50        : 0.1921    MRR@100       : 0.1934\n",
      "Epoch 61  train loss 15.8994\n",
      "Epoch 62  train loss 20.1854\n",
      "Epoch 63  train loss 16.5696\n",
      "Epoch 64  train loss 19.8666\n",
      "Epoch 65  train loss 17.2204\n",
      "  Update Best\n",
      "  Validation data\n",
      "    Precision@10  : 0.0500    Precision@20  : 0.0451    Precision@50  : 0.0364    Precision@100 : 0.0294\n",
      "    Recall@10     : 0.0676    Recall@20     : 0.1170    Recall@50     : 0.2220    Recall@100    : 0.3355\n",
      "    nDCG@10       : 0.0578    nDCG@20       : 0.0810    nDCG@50       : 0.1211    nDCG@100      : 0.1581\n",
      "    MRR@10        : 0.1250    MRR@20        : 0.1354    MRR@50        : 0.1423    MRR@100       : 0.1439\n",
      "  Test data\n",
      "    Precision@10  : 0.0747    Precision@20  : 0.0681    Precision@50  : 0.0562    Precision@100 : 0.0466\n",
      "    Recall@10     : 0.0521    Recall@20     : 0.0919    Recall@50     : 0.1797    Recall@100    : 0.2755\n",
      "    nDCG@10       : 0.0566    nDCG@20       : 0.0800    nDCG@50       : 0.1215    nDCG@100      : 0.1612\n",
      "    MRR@10        : 0.1768    MRR@20        : 0.1879    MRR@50        : 0.1943    MRR@100       : 0.1955\n",
      "Epoch 66  train loss 21.5331\n",
      "Epoch 67  train loss 17.9680\n",
      "Epoch 68  train loss 16.8553\n",
      "Epoch 69  train loss 14.0877\n",
      "Epoch 70  train loss 17.5485\n",
      "  Validation data\n",
      "    Precision@10  : 0.0508    Precision@20  : 0.0451    Precision@50  : 0.0364    Precision@100 : 0.0294\n",
      "    Recall@10     : 0.0668    Recall@20     : 0.1170    Recall@50     : 0.2235    Recall@100    : 0.3370\n",
      "    nDCG@10       : 0.0576    nDCG@20       : 0.0808    nDCG@50       : 0.1211    nDCG@100      : 0.1582\n",
      "    MRR@10        : 0.1255    MRR@20        : 0.1363    MRR@50        : 0.1432    MRR@100       : 0.1449\n",
      "  Test data\n",
      "    Precision@10  : 0.0757    Precision@20  : 0.0687    Precision@50  : 0.0565    Precision@100 : 0.0468\n",
      "    Recall@10     : 0.0513    Recall@20     : 0.0927    Recall@50     : 0.1801    Recall@100    : 0.2779\n",
      "    nDCG@10       : 0.0553    nDCG@20       : 0.0793    nDCG@50       : 0.1207    nDCG@100      : 0.1610\n",
      "    MRR@10        : 0.1719    MRR@20        : 0.1833    MRR@50        : 0.1896    MRR@100       : 0.1909\n",
      "Epoch 71  train loss 17.2503\n",
      "Epoch 72  train loss 19.7010\n",
      "Epoch 73  train loss 17.4446\n",
      "Epoch 74  train loss 16.9774\n",
      "Epoch 75  train loss 20.2607\n",
      "  Update Best\n",
      "  Validation data\n",
      "    Precision@10  : 0.0509    Precision@20  : 0.0447    Precision@50  : 0.0360    Precision@100 : 0.0287\n",
      "    Recall@10     : 0.0690    Recall@20     : 0.1182    Recall@50     : 0.2237    Recall@100    : 0.3344\n",
      "    nDCG@10       : 0.0582    nDCG@20       : 0.0810    nDCG@50       : 0.1208    nDCG@100      : 0.1567\n",
      "    MRR@10        : 0.1253    MRR@20        : 0.1358    MRR@50        : 0.1424    MRR@100       : 0.1441\n",
      "  Test data\n",
      "    Precision@10  : 0.0738    Precision@20  : 0.0655    Precision@50  : 0.0541    Precision@100 : 0.0450\n",
      "    Recall@10     : 0.0522    Recall@20     : 0.0895    Recall@50     : 0.1759    Recall@100    : 0.2699\n",
      "    nDCG@10       : 0.0561    nDCG@20       : 0.0780    nDCG@50       : 0.1186    nDCG@100      : 0.1573\n",
      "    MRR@10        : 0.1739    MRR@20        : 0.1843    MRR@50        : 0.1908    MRR@100       : 0.1922\n",
      "Epoch 76  train loss 17.7244\n",
      "Epoch 77  train loss 17.9896\n",
      "Epoch 78  train loss 17.7271\n",
      "Epoch 79  train loss 14.1264\n",
      "Epoch 80  train loss 13.5563\n",
      "  Validation data\n",
      "    Precision@10  : 0.0498    Precision@20  : 0.0437    Precision@50  : 0.0353    Precision@100 : 0.0283\n",
      "    Recall@10     : 0.0663    Recall@20     : 0.1106    Recall@50     : 0.1993    Recall@100    : 0.2922\n",
      "    nDCG@10       : 0.0570    nDCG@20       : 0.0783    nDCG@50       : 0.1142    nDCG@100      : 0.1467\n",
      "    MRR@10        : 0.1240    MRR@20        : 0.1342    MRR@50        : 0.1401    MRR@100       : 0.1415\n",
      "  Test data\n",
      "    Precision@10  : 0.0737    Precision@20  : 0.0662    Precision@50  : 0.0541    Precision@100 : 0.0445\n",
      "    Recall@10     : 0.0494    Recall@20     : 0.0847    Recall@50     : 0.1577    Recall@100    : 0.2365\n",
      "    nDCG@10       : 0.0546    nDCG@20       : 0.0761    nDCG@50       : 0.1128    nDCG@100      : 0.1476\n",
      "    MRR@10        : 0.1750    MRR@20        : 0.1852    MRR@50        : 0.1908    MRR@100       : 0.1918\n",
      "Epoch 81  train loss 14.6900\n",
      "Epoch 82  train loss 13.0713\n",
      "Epoch 83  train loss 15.8150\n",
      "Epoch 84  train loss 18.6680\n",
      "Epoch 85  train loss 16.6504\n",
      "  Update Best\n",
      "  Validation data\n",
      "    Precision@10  : 0.0502    Precision@20  : 0.0454    Precision@50  : 0.0368    Precision@100 : 0.0297\n",
      "    Recall@10     : 0.0684    Recall@20     : 0.1223    Recall@50     : 0.2298    Recall@100    : 0.3475\n",
      "    nDCG@10       : 0.0586    nDCG@20       : 0.0831    nDCG@50       : 0.1239    nDCG@100      : 0.1619\n",
      "    MRR@10        : 0.1272    MRR@20        : 0.1387    MRR@50        : 0.1458    MRR@100       : 0.1474\n",
      "  Test data\n",
      "    Precision@10  : 0.0731    Precision@20  : 0.0676    Precision@50  : 0.0563    Precision@100 : 0.0467\n",
      "    Recall@10     : 0.0505    Recall@20     : 0.0924    Recall@50     : 0.1840    Recall@100    : 0.2827\n",
      "    nDCG@10       : 0.0548    nDCG@20       : 0.0789    nDCG@50       : 0.1216    nDCG@100      : 0.1621\n",
      "    MRR@10        : 0.1707    MRR@20        : 0.1824    MRR@50        : 0.1890    MRR@100       : 0.1904\n",
      "Epoch 86  train loss 15.8533\n",
      "Epoch 87  train loss 20.4970\n",
      "Epoch 88  train loss 15.6379\n",
      "Epoch 89  train loss 15.6175\n",
      "Epoch 90  train loss 16.6557\n",
      "  Update Best\n",
      "  Validation data\n",
      "    Precision@10  : 0.0511    Precision@20  : 0.0459    Precision@50  : 0.0375    Precision@100 : 0.0304\n",
      "    Recall@10     : 0.0705    Recall@20     : 0.1251    Recall@50     : 0.2414    Recall@100    : 0.3653\n",
      "    nDCG@10       : 0.0597    nDCG@20       : 0.0845    nDCG@50       : 0.1277    nDCG@100      : 0.1674\n",
      "    MRR@10        : 0.1273    MRR@20        : 0.1384    MRR@50        : 0.1456    MRR@100       : 0.1474\n",
      "  Test data\n",
      "    Precision@10  : 0.0728    Precision@20  : 0.0681    Precision@50  : 0.0573    Precision@100 : 0.0476\n",
      "    Recall@10     : 0.0518    Recall@20     : 0.0961    Recall@50     : 0.1921    Recall@100    : 0.2970\n",
      "    nDCG@10       : 0.0553    nDCG@20       : 0.0806    nDCG@50       : 0.1251    nDCG@100      : 0.1673\n",
      "    MRR@10        : 0.1725    MRR@20        : 0.1847    MRR@50        : 0.1915    MRR@100       : 0.1928\n",
      "Epoch 91  train loss 16.2636\n",
      "Epoch 92  train loss 18.4835\n",
      "Epoch 93  train loss 14.2463\n",
      "Epoch 94  train loss 15.0942\n",
      "Epoch 95  train loss 14.1577\n",
      "  Update Best\n",
      "  Validation data\n",
      "    Precision@10  : 0.0504    Precision@20  : 0.0456    Precision@50  : 0.0378    Precision@100 : 0.0303\n",
      "    Recall@10     : 0.0714    Recall@20     : 0.1252    Recall@50     : 0.2377    Recall@100    : 0.3512\n",
      "    nDCG@10       : 0.0606    nDCG@20       : 0.0852    nDCG@50       : 0.1279    nDCG@100      : 0.1653\n",
      "    MRR@10        : 0.1303    MRR@20        : 0.1416    MRR@50        : 0.1486    MRR@100       : 0.1502\n",
      "  Test data\n",
      "    Precision@10  : 0.0743    Precision@20  : 0.0665    Precision@50  : 0.0563    Precision@100 : 0.0470\n",
      "    Recall@10     : 0.0543    Recall@20     : 0.0931    Recall@50     : 0.1821    Recall@100    : 0.2787\n",
      "    nDCG@10       : 0.0582    nDCG@20       : 0.0809    nDCG@50       : 0.1233    nDCG@100      : 0.1636\n",
      "    MRR@10        : 0.1783    MRR@20        : 0.1894    MRR@50        : 0.1959    MRR@100       : 0.1971\n",
      "Epoch 96  train loss 17.6493\n",
      "Epoch 97  train loss 16.1449\n",
      "Epoch 98  train loss 14.0813\n",
      "Epoch 99  train loss 14.2294\n",
      "Epoch 100  train loss 13.4139\n",
      "  Validation data\n",
      "    Precision@10  : 0.0506    Precision@20  : 0.0456    Precision@50  : 0.0372    Precision@100 : 0.0298\n",
      "    Recall@10     : 0.0695    Recall@20     : 0.1223    Recall@50     : 0.2326    Recall@100    : 0.3434\n",
      "    nDCG@10       : 0.0592    nDCG@20       : 0.0835    nDCG@50       : 0.1253    nDCG@100      : 0.1620\n",
      "    MRR@10        : 0.1251    MRR@20        : 0.1364    MRR@50        : 0.1433    MRR@100       : 0.1449\n",
      "  Test data\n",
      "    Precision@10  : 0.0757    Precision@20  : 0.0679    Precision@50  : 0.0564    Precision@100 : 0.0466\n",
      "    Recall@10     : 0.0537    Recall@20     : 0.0919    Recall@50     : 0.1786    Recall@100    : 0.2735\n",
      "    nDCG@10       : 0.0576    nDCG@20       : 0.0803    nDCG@50       : 0.1218    nDCG@100      : 0.1614\n",
      "    MRR@10        : 0.1771    MRR@20        : 0.1877    MRR@50        : 0.1940    MRR@100       : 0.1953\n",
      "Epoch 101  train loss 14.0205\n",
      "Epoch 102  train loss 15.2480\n",
      "Epoch 103  train loss 15.7256\n",
      "Epoch 104  train loss 12.9159\n",
      "Epoch 105  train loss 14.6987\n",
      "  Update Best\n",
      "  Validation data\n",
      "    Precision@10  : 0.0511    Precision@20  : 0.0468    Precision@50  : 0.0383    Precision@100 : 0.0307\n",
      "    Recall@10     : 0.0711    Recall@20     : 0.1267    Recall@50     : 0.2406    Recall@100    : 0.3574\n",
      "    nDCG@10       : 0.0592    nDCG@20       : 0.0847    nDCG@50       : 0.1277    nDCG@100      : 0.1661\n",
      "    MRR@10        : 0.1238    MRR@20        : 0.1353    MRR@50        : 0.1423    MRR@100       : 0.1439\n",
      "  Test data\n",
      "    Precision@10  : 0.0737    Precision@20  : 0.0682    Precision@50  : 0.0576    Precision@100 : 0.0477\n",
      "    Recall@10     : 0.0512    Recall@20     : 0.0938    Recall@50     : 0.1860    Recall@100    : 0.2838\n",
      "    nDCG@10       : 0.0556    nDCG@20       : 0.0802    nDCG@50       : 0.1239    nDCG@100      : 0.1646\n",
      "    MRR@10        : 0.1758    MRR@20        : 0.1874    MRR@50        : 0.1940    MRR@100       : 0.1952\n",
      "Epoch 106  train loss 19.4423\n",
      "Epoch 107  train loss 14.6695\n",
      "Epoch 108  train loss 11.5968\n",
      "Epoch 109  train loss 14.0591\n",
      "Epoch 110  train loss 14.6646\n",
      "  Validation data\n",
      "    Precision@10  : 0.0510    Precision@20  : 0.0458    Precision@50  : 0.0374    Precision@100 : 0.0298\n",
      "    Recall@10     : 0.0715    Recall@20     : 0.1260    Recall@50     : 0.2454    Recall@100    : 0.3693\n",
      "    nDCG@10       : 0.0599    nDCG@20       : 0.0846    nDCG@50       : 0.1282    nDCG@100      : 0.1671\n",
      "    MRR@10        : 0.1257    MRR@20        : 0.1370    MRR@50        : 0.1442    MRR@100       : 0.1460\n",
      "  Test data\n",
      "    Precision@10  : 0.0747    Precision@20  : 0.0685    Precision@50  : 0.0567    Precision@100 : 0.0467\n",
      "    Recall@10     : 0.0534    Recall@20     : 0.0967    Recall@50     : 0.1926    Recall@100    : 0.2978\n",
      "    nDCG@10       : 0.0576    nDCG@20       : 0.0824    nDCG@50       : 0.1263    nDCG@100      : 0.1680\n",
      "    MRR@10        : 0.1761    MRR@20        : 0.1878    MRR@50        : 0.1945    MRR@100       : 0.1959\n",
      "Epoch 111  train loss 13.0921\n",
      "Epoch 112  train loss 17.9511\n",
      "Epoch 113  train loss 14.7129\n",
      "Epoch 114  train loss 13.1666\n",
      "Epoch 115  train loss 13.5317\n",
      "  Validation data\n",
      "    Precision@10  : 0.0509    Precision@20  : 0.0464    Precision@50  : 0.0382    Precision@100 : 0.0306\n",
      "    Recall@10     : 0.0721    Recall@20     : 0.1262    Recall@50     : 0.2412    Recall@100    : 0.3560\n",
      "    nDCG@10       : 0.0607    nDCG@20       : 0.0858    nDCG@50       : 0.1292    nDCG@100      : 0.1671\n",
      "    MRR@10        : 0.1286    MRR@20        : 0.1400    MRR@50        : 0.1472    MRR@100       : 0.1487\n",
      "  Test data\n",
      "    Precision@10  : 0.0752    Precision@20  : 0.0684    Precision@50  : 0.0573    Precision@100 : 0.0474\n",
      "    Recall@10     : 0.0539    Recall@20     : 0.0951    Recall@50     : 0.1835    Recall@100    : 0.2796\n",
      "    nDCG@10       : 0.0585    nDCG@20       : 0.0825    nDCG@50       : 0.1249    nDCG@100      : 0.1649\n",
      "    MRR@10        : 0.1818    MRR@20        : 0.1932    MRR@50        : 0.1994    MRR@100       : 0.2006\n",
      "Epoch 116  train loss 15.1255\n",
      "Epoch 117  train loss 12.4880\n",
      "Epoch 118  train loss 15.2324\n",
      "Epoch 119  train loss 13.2172\n",
      "Epoch 120  train loss 16.4620\n",
      "  Validation data\n",
      "    Precision@10  : 0.0502    Precision@20  : 0.0462    Precision@50  : 0.0376    Precision@100 : 0.0297\n",
      "    Recall@10     : 0.0705    Recall@20     : 0.1245    Recall@50     : 0.2337    Recall@100    : 0.3426\n",
      "    nDCG@10       : 0.0601    nDCG@20       : 0.0853    nDCG@50       : 0.1268    nDCG@100      : 0.1628\n",
      "    MRR@10        : 0.1271    MRR@20        : 0.1388    MRR@50        : 0.1456    MRR@100       : 0.1470\n",
      "  Test data\n",
      "    Precision@10  : 0.0742    Precision@20  : 0.0672    Precision@50  : 0.0552    Precision@100 : 0.0458\n",
      "    Recall@10     : 0.0516    Recall@20     : 0.0915    Recall@50     : 0.1749    Recall@100    : 0.2678\n",
      "    nDCG@10       : 0.0575    nDCG@20       : 0.0808    nDCG@50       : 0.1209    nDCG@100      : 0.1598\n",
      "    MRR@10        : 0.1846    MRR@20        : 0.1961    MRR@50        : 0.2023    MRR@100       : 0.2035\n",
      "Epoch 121  train loss 13.0835\n",
      "Epoch 122  train loss 14.2311\n",
      "Epoch 123  train loss 14.0456\n",
      "Epoch 124  train loss 13.3528\n",
      "------------------\n",
      "Exiting from training early\n"
     ]
    }
   ],
   "source": [
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "dict_args = {}\n",
    "args = dotdict(dict_args)\n",
    "\n",
    "\n",
    "args.dataset_name = 'ml-1m'\n",
    "args.file_name = 'ratings.dat'\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "args.batch_size = 400\n",
    "args.lr = 1e-4\n",
    "args.weight_decay = 0.0\n",
    "args.epochs = 1000\n",
    "args.topK = [10, 20, 50, 100]\n",
    "\n",
    "# model hyper\n",
    "args.hidden_dims = [1000]\n",
    "args.norm = True\n",
    "\n",
    "# diffusion hyper\n",
    "args.beta_start = 1e-4\n",
    "args.beta_end = 0.02\n",
    "args.noise_scale = 0.1\n",
    "args.steps = 100\n",
    "args.snr = True # assign different weight to different timestep or not\n",
    "args.sampling_steps = 0\n",
    "\n",
    "\n",
    "\n",
    "dir_path = os.path.join(os.getcwd(), args.dataset_name)\n",
    "dataset = PreProcess(dir_path, args.file_name, use_cache=True)\n",
    "train_dataset = DataDiffusion(torch.FloatTensor(dataset.sp_train.toarray()))\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, pin_memory=True, shuffle=True)\n",
    "test_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "diffusion = Diffusion()\n",
    "encoder_dims = [dataset.num_items] + args.hidden_dims\n",
    "decoder_dims = encoder_dims[::-1]\n",
    "model = ReverseDiffusion(encoder_dims, decoder_dims).to(args.device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "best_recall, best_epoch = -100, 0\n",
    "best_test_result = None\n",
    "print(\"Start training\")\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    if epoch - best_epoch >= 20: # early stopping\n",
    "        print('-'*18)\n",
    "        print('Exiting from training early')\n",
    "        break\n",
    "\n",
    "    model.train()    \n",
    "    start_time = time.time()\n",
    "    avg_loss = train_one_epoch(args, model, diffusion, optimizer, train_loader)\n",
    "    print(f'Epoch {epoch}  train loss {avg_loss:.4f}')\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        val_results = evaluate(\n",
    "            args, model, diffusion, test_loader, dataset.sp_val, dataset.sp_train, args.topK)\n",
    "        test_results = evaluate(\n",
    "            args, model, diffusion, test_loader, dataset.sp_test, dataset.sp_train, args.topK)\n",
    "    \n",
    "        val_recalls = val_results[1]\n",
    "        if val_recalls[1] > best_recall:\n",
    "            best_recall, best_epoch = val_recalls[1], epoch\n",
    "            print('  Update Best')\n",
    "\n",
    "        print('  Validation data')\n",
    "        print_metric_results(args.topK, val_results)\n",
    "        print('  Test data')\n",
    "        print_metric_results(args.topK, test_results)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
