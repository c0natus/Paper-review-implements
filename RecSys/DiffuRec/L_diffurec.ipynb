{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Miniconda3\\envs\\rec\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from kmeans_pytorch import kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Load and Dataset\n",
    "\n",
    "학습된 item embedding이 필요하므로 공식 github에 있는 dataset과 item embedding 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load(dir_path):\n",
    "    train_path = os.path.join(dir_path, 'train_list.npy')\n",
    "    valid_path = os.path.join(dir_path, 'valid_list.npy')\n",
    "    test_path = os.path.join(dir_path, 'test_list.npy')\n",
    "    emb_path = os.path.join(dir_path, 'item_emb.npy')\n",
    "\n",
    "    train_list = np.load(train_path, allow_pickle=True)\n",
    "    valid_list = np.load(valid_path, allow_pickle=True)\n",
    "    test_list = np.load(test_path, allow_pickle=True)\n",
    "\n",
    "    uid_max = 0\n",
    "    iid_max = 0\n",
    "    train_dict = {}\n",
    "\n",
    "    for uid, iid in train_list:\n",
    "        if uid not in train_dict:\n",
    "            train_dict[uid] = []\n",
    "        train_dict[uid].append(iid)\n",
    "        if uid > uid_max:\n",
    "            uid_max = uid\n",
    "        if iid > iid_max:\n",
    "            iid_max = iid\n",
    "    \n",
    "    n_user = uid_max + 1\n",
    "    n_item = iid_max + 1\n",
    "    print(f'user num: {n_user}')\n",
    "    print(f'item num: {n_item}')\n",
    "\n",
    "    train_data = sp.csr_matrix((np.ones_like(train_list[:, 0]), \\\n",
    "        (train_list[:, 0], train_list[:, 1])), dtype='float64', \\\n",
    "        shape=(n_user, n_item))\n",
    "    \n",
    "    valid_y_data = sp.csr_matrix((np.ones_like(valid_list[:, 0]),\n",
    "                 (valid_list[:, 0], valid_list[:, 1])), dtype='float64',\n",
    "                 shape=(n_user, n_item))  # valid_groundtruth\n",
    "\n",
    "    test_y_data = sp.csr_matrix((np.ones_like(test_list[:, 0]),\n",
    "                 (test_list[:, 0], test_list[:, 1])), dtype='float64',\n",
    "                 shape=(n_user, n_item))  # test_groundtruth\n",
    "    \n",
    "\n",
    "    emb_items = torch.from_numpy(np.load(emb_path, allow_pickle=True))\n",
    "    \n",
    "    return train_data, valid_y_data, test_y_data, n_user, n_item, emb_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataDiffusion(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.data = dataset\n",
    "\n",
    "    def __getitem__(self, index): \n",
    "        item = self.data[index]\n",
    "        return item\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Diffusion and Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diffusion():\n",
    "    def __init__(self, steps=100, beta_start=1e-4, beta_end=0.02, device='cuda',\\\n",
    "            noise_scale=0.1, num_for_expectation=10):\n",
    "        \"\"\"\n",
    "        Forward diffusion 또는 주어진 model로 reverse diffusion한다.\n",
    "        Args:\n",
    "            steps               : reverse할 개수\n",
    "            beta_start          : Beta 시작 값, DDPM 논문에 적힌 1e-4 사용.\n",
    "            beta_end            : Beta 마지막 값, DDPM 논문에 적힌 0.02 사용. \n",
    "            noise_scale         : Beta를 생성할 때, noise의 정도를 조정하기 위해 사용.\n",
    "                                : 논문 3.4 personalized recommendation 1)의 마지막 문장 참고.\n",
    "            num_for_expectation : Importance sampling에 사용되는, expectation을 구하기 위해 필요한 loss의 수\n",
    "        \"\"\"\n",
    "        self.steps = steps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "        self.device = device\n",
    "        self.noise_scale = noise_scale\n",
    "\n",
    "        self.beta = torch.tensor(self.get_betas(), dtype=torch.float32).to(device)\n",
    "        self.alpha = 1.0 - self.beta\n",
    "        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n",
    "        self.alpha_bar_prev = torch.cat([torch.tensor([1.0]).to(device), self.alpha_bar[:-1]]).to(device) # 제일 처음 원소는 어차피 안 쓰임.\n",
    "        self.sqrt_alpha_bar = torch.sqrt(self.alpha_bar)\n",
    "        self.sqrt_one_minus_alpha_bar = torch.sqrt(1 - self.alpha_bar)\n",
    "\n",
    "        # 논문 수식 8\n",
    "        self.posterior_mean_coef1 = torch.sqrt(self.alpha) * (1 - self.alpha_bar_prev) / (1.0 - self.alpha_bar) # x_t 앞의 계수\n",
    "        self.posterior_mean_coef2 = torch.sqrt(self.alpha_bar_prev) * self.beta / (1.0 - self.alpha_bar) # batch 앞의 계수\n",
    "        # 아래 부분은 posterior variance를 사용하는 값인 것 같다.\n",
    "        # 원래 DDPM에서는 beta를 사용했지만, 학습할 수도 있고, 아래처럼 다양한 것을 사용할 수 있다.\n",
    "        # 본 논문에서는 따로 언급은 없지만 log_var_clipped를 사용했다.\n",
    "        self.posterior_variance = self.beta * (1.0 - self.alpha_bar_prev) / (1.0 - self.alpha_bar)\n",
    "        self.posterior_log_variance_clipped = torch.log(\n",
    "            torch.cat([self.posterior_variance[1].unsqueeze(0), self.posterior_variance[1:]])\n",
    "        )\n",
    "\n",
    "        # Step을 sampling 하는 방법 중 importance sampling에 필요한 variable.\n",
    "        # Importance sampling은 DDPM을 향상 시키는 technique 중 하나.\n",
    "        # Importance sampling은 각 step마다 optimization의 어려움 정도가 다르다는 것을 가정한다.\n",
    "        # 그래서 Loss가 큰 step에 대한 학습을 강조하기 위해 importance sampling을 고려한다.\n",
    "        # 간단히 말하면 loss가 큰 step에 대해 sampling 확률을 높인다.\n",
    "        self.num_for_expectation = num_for_expectation # Monte Calro를 사용하기 위한 개수.\n",
    "        self.Lt_history = torch.zeros(steps, num_for_expectation, dtype=torch.float32).to(device)\n",
    "        self.Lt_count = torch.zeros(steps, dtype=int).to(device)\n",
    "\n",
    "    def get_betas(self, max_beta=0.999):\n",
    "        # DDPM에서는 beta가 linear하게 증가하도록 하고 있는데,\n",
    "        # 본 논문 eq 4는 alpha_bar가 linear하도록 beta를 설정하고 있다.\n",
    "        # 풀어서 써보면 alpha_bar = 1 - np.linspace(...)의 값을 가지게 된다.\n",
    "\n",
    "        start = self.noise_scale * self.beta_start\n",
    "        end = self.noise_scale * self.beta_end\n",
    "        alpha_bar = 1 - np.linspace(start, end, self.steps) # 1 - \\beta\n",
    "        betas = []\n",
    "        betas.append(1 - alpha_bar[0])\n",
    "        for i in range(1, self.steps):\n",
    "            betas.append(min(1 - alpha_bar[i] / alpha_bar[i - 1], max_beta))\n",
    "        return np.array(betas)\n",
    "\n",
    "    def sample_steps(self, batch_size, method='uniform', uniform_prob=0.001):\n",
    "        if method == 'importance':\n",
    "            if not (self.Lt_count == self.num_for_expectation).all():\n",
    "                # 모든 steps에 대한 loss가 num_for_expectation만큼 없으면 uniform 방식으로 sampling\n",
    "                return self.sample_steps(batch_size, method='uniform')\n",
    "\n",
    "            # 수식 14에 따라 sampling한다.\n",
    "            Lt_sqrt = torch.sqrt(torch.mean(self.Lt_history ** 2, axis=-1)) \n",
    "            # 10개 loss의 제곱에 대한 평균의 루트값.\n",
    "            # Lt_sqrt shape: (steps,)\n",
    "            \n",
    "            pt_prob = Lt_sqrt / torch.sum(Lt_sqrt)\n",
    "            pt_prob *= 1 - uniform_prob \n",
    "            pt_prob += uniform_prob / len(pt_prob)\n",
    "            # Loss의 크기에 따라 sampling 확률을 다르게 준다.\n",
    "            # 어느 정도 uniform_prob 만큼 sampling 되는 것을 보장.\n",
    "\n",
    "            step = torch.multinomial(input=pt_prob, num_samples=batch_size, replacement=True) \n",
    "            # 중복 sampling\n",
    "            pt = pt_prob.gather(dim=0, index=step) * len(pt_prob)\n",
    "            # 각 step의 확률 값을 가져온다.\n",
    "            # 수식 14에 따라 training에서 Lt / pt를 하기 위함.\n",
    "\n",
    "        elif method == 'uniform':\n",
    "            steps = torch.randint(low=1, high=self.steps, size=(batch_size,)).long()\n",
    "            pt = torch.ones_like(steps).float()\n",
    "            # loss의 평균 값을 구하기 때문에 len(pt)로 안 나눠줘도 된다.\n",
    "        \n",
    "        else: raise ValueError\n",
    "        \n",
    "        return steps.to(self.device), pt.to(self.device)\n",
    "\n",
    "    def get_noised_interaction(self, batch, t):\n",
    "        \"\"\"\n",
    "        Noise를 추가한, 각 item에 대한 소비할 확률 값을 구하는 함수.\n",
    "        논문 수식 3 참고.\n",
    "        Args:\n",
    "            batch : Training dataset으로 만들어진 user interactions   (batch_size, num_items)\n",
    "            t   : noise step                                       (batch_size, )\n",
    "        \"\"\"\n",
    "        sqrt_alpha_bar = self.sqrt_alpha_bar[t][:, None]\n",
    "        mean_ = sqrt_alpha_bar * batch\n",
    "        std_ = self.sqrt_one_minus_alpha_bar[t][:, None]\n",
    "        noise = torch.randn_like(batch)\n",
    "        # 각 item마다 줄 noise를 sampling한다. -> reparameter trick에서 사용됨.\n",
    "\n",
    "        noised_interaction = mean_ + std_ * noise # reparmeter\n",
    "        return noised_interaction, noise\n",
    "\n",
    "\n",
    "    def sample_new_interaction(self, model, batch, steps: int, sampling_noise=False):\n",
    "        \"\"\"\n",
    "        batch부터 정해진 steps만큼 noise를 주고, \n",
    "        학습된 model을 가지고 reverse diffusion으로 추천을 생성한다.\n",
    "        Args:\n",
    "            model           : 학습된 model\n",
    "            batch             : 초기 users의 interactions, shape: (batch_size, num_items), shape (batch_size, num_items)\n",
    "            steps           : Forward를 할 step, x_T까지 forward하지 않는다. 이유는 논문 3.3 참고.\n",
    "            sampling_noise  : 추천을 생성할 때, noise 추가 유무. False이면 variance 없이 mean 값만 사용.\n",
    "        \"\"\"\n",
    "\n",
    "        if steps == 0: \n",
    "            # noise를 전혀 추가하지 않고, reverse를 T번 진행\n",
    "            # 기존의 user의 interaction이 noise하다고 가정.\n",
    "            x_T = batch\n",
    "        else:\n",
    "            T = torch.tensor([steps - 1] * batch.shape[0]).to(batch.device)\n",
    "            x_T, noise = self.get_noised_interaction(batch, T)\n",
    "\n",
    "        reverse_t = list(range(self.steps))[::-1]\n",
    "        \n",
    "        x_t = x_T\n",
    "        if self.noise_scale == 0:\n",
    "            # Denoise 과정이 없다.\n",
    "            # 즉, forward가 없다. 이러면 각 reverse는 x_t -> x_t를 복원하는 것이다.\n",
    "            # 결국 x_t -> x_t를 하는 AutoEndocer를 여러 개 쌓은 것과 같다.\n",
    "            for t_idx in reverse_t:\n",
    "                t = torch.tensor([t_idx] * x_t.shape[0]).to(batch.device) # Shape: (batch_size, )\n",
    "                x_t = model(x_t, t)\n",
    "        else:\n",
    "            # Denosing 과정이 있다.\n",
    "            # x_t와 t가 주어졌을 때, p(x_{t-1}|x_t)의 평균과 분산 값을 구한다.\n",
    "            # 이를 통해 reparameterzation trick으로 x_{t-1}을 얻는다.\n",
    "            # 우리는 p(x_{t-1}|x_t)를 모른다. 여러 수식을 유도하면, p(x_{t-1}|x_t)의 likelihodd는 \n",
    "            # q(x_{t-1}|x_t, batch)와 유사한 분포를 가지면 커진다.\n",
    "            # 이때, x_t는 알아도 batch를 모른다. 그래서 학습된 model을 통해 batch를 예측한 값을 사용한다.\n",
    "            for t_idx in reverse_t:\n",
    "                t = torch.tensor([t_idx] * x_t.shape[0]).to(batch.device)\n",
    "                batch_hat = model(x_t, t)\n",
    "                mean_hat = self.posterior_mean_coef1[t][:, None] * x_t + self.posterior_mean_coef2[t][:, None] * batch_hat\n",
    "                if sampling_noise:\n",
    "                    # 추천을 생성할 때 uncertainty 추가. 즉, variance 사용\n",
    "                    variance = self.posterior_log_variance_clipped[t][:, None]\n",
    "                    if t_idx > 1: noise = torch.randn_like(x_t)\n",
    "                    else: noise = torch.zeros_like(x_t) # t == 0일 때 noise를 주지 않는다.\n",
    "                    x_t = mean_hat + torch.exp(0.5 * variance) * noise\n",
    "                else:\n",
    "                    # 추천을 생성할 때 variance를 사용하지 않음.\n",
    "                    # 따라서 평균값만 사용.\n",
    "                    x_t = mean_hat\n",
    "        return x_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, num_items, emb_items, num_clusters=3, hidden_dims=[300], \\\n",
    "            device='cuda', dropout=0.1):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.encoder = nn.ModuleList([])\n",
    "        self.decoder = nn.ModuleList([])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.active = nn.Tanh()\n",
    "        self.num_clusters = num_clusters\n",
    "        self.hidden_dims = hidden_dims\n",
    "\n",
    "        if num_clusters == 1: \n",
    "            # no clustering\n",
    "            # 굳이 if 문을 안 둬도 되지 않나??\n",
    "            in_dims_tmp = [num_items] + hidden_dims[:-1] + [hidden_dims[-1] * 2]\n",
    "            out_dims_tmp = in_dims_tmp[::-1]\n",
    "            for d_in, d_out in zip(in_dims_tmp[:-1], in_dims_tmp[1:]):\n",
    "                self.encoder.append(nn.Linear(d_in, d_out))\n",
    "                self.encoder.append(self.active)\n",
    "            for d_in, d_out in zip(out_dims_tmp[:-1], out_dims_tmp[1:]):\n",
    "                self.decoder.append(nn.Linear(d_in, d_out))\n",
    "                self.decoder.append(self.active)\n",
    "            self.decoder = self.decoder[:-1]\n",
    "        else:\n",
    "            # Clustering\n",
    "            #### Build cluster map ####\n",
    "            self.cluster_ids, centers = kmeans(X=emb_items, num_clusters=num_clusters, distance='euclidean', device=device)\n",
    "            # cluster_ids(labels): [0, 1, 2, 2, 1, 0, 0, ...]\n",
    "            # item이 순서대로 어떤 cluster에 해당하는지 나타낸다.\n",
    "            self.cluster_idx = [] \n",
    "            # cluster_idx:\n",
    "            #   각 cluster 별 어떤 item이 담기는지 저장.\n",
    "            #   이후 batch interaction이 들어왔을 때, cluster로 분배해주기 위해 필요하다.\n",
    "            for idx in range(num_clusters):\n",
    "                indicies = np.argwhere(self.cluster_ids.numpy() == idx).squeeze().tolist()\n",
    "                self.cluster_idx.append(torch.tensor(indicies))\n",
    "            self.cluster_map = torch.cat(tuple(self.cluster_idx), dim=0)\n",
    "            # cluster_map:\n",
    "            #   Evaluation에서 생성된 items들이 순서대로 어떤 item id에 대한 점수인지를 나타낼 때 사용한다.\n",
    "            self.num_items_per_cluster = [len(self.cluster_idx[idx]) for idx in range(num_clusters)]\n",
    "            # num_items_per_cluster:\n",
    "            #   encoder, decoder dimension 계산에 사용됨.\n",
    "            \n",
    "            #### Build encoder and decoder ####\n",
    "            self.in_latent_dims_clusters = []\n",
    "            for idx in range(num_clusters):\n",
    "                # Cluster에 있는 item의 수에 따라 정해진 dimension(논문에선 300을 사용)을 나눠 가진다.\n",
    "                if idx == num_clusters - 1:\n",
    "                    # 마지막 cluster의 dimension은 나머지 값을 가진다.\n",
    "                    latent_dims = [hidden_dims[j] - np.array(self.in_latent_dims_clusters)[:,j].sum(axis=0) for j in range(len(hidden_dims))]\n",
    "                else:\n",
    "                    latent_dims = [int(self.num_items_per_cluster[idx] / num_items * hidden_dims[j]) \\\n",
    "                        for j in range(len(hidden_dims))] # Cluster에 있는 item의 수에 따라 j번째 hidden dimension을 나눈다.\n",
    "                    latent_dims = [latent_dims[j] if latent_dims[j] != 0 else 1 \\\n",
    "                        for j in range(len(hidden_dims))] # 만약 item의 수가 너무 적어 dimension 값이 0이 나오면, 1을 준다.\n",
    "                \n",
    "                self.in_latent_dims_clusters.append(latent_dims) \n",
    "                # latent_dims_clusters:\n",
    "                #   마지막 cluster의 dims를 구하기 위해 필요\n",
    "                #   그리고 decoder in, out dim에 필요\n",
    "                \n",
    "                in_dims_tmp = [self.num_items_per_cluster[idx]] + latent_dims[:-1] + [latent_dims[-1] * 2]\n",
    "                encoder_tmp = nn.ModuleList([])\n",
    "                for d_in, d_out in zip(in_dims_tmp[:-1], in_dims_tmp[1:]):\n",
    "                    encoder_tmp.append(nn.Linear(d_in, d_out))\n",
    "                    encoder_tmp.append(self.active)\n",
    "                self.encoder.append(nn.Sequential(*encoder_tmp))\n",
    "                del encoder_tmp\n",
    "            \n",
    "            self.out_latent_dims_clusters = []            \n",
    "            for idx in range(self.num_clusters):\n",
    "                out_dim_tmp = self.in_latent_dims_clusters[idx][::-1] + [self.num_items_per_cluster[idx]]\n",
    "                self.out_latent_dims_clusters.append(out_dim_tmp)\n",
    "                decoder_tmp = nn.ModuleList([])\n",
    "                for d_in, d_out in zip(out_dim_tmp[:-1], out_dim_tmp[1:]):\n",
    "                    decoder_tmp.append(nn.Linear(d_in, d_out))\n",
    "                    decoder_tmp.append(self.active)\n",
    "                self.decoder.append(nn.Sequential(*decoder_tmp[:-1])) # 마지막엔 activate function 추가 안 함.\n",
    "                del decoder_tmp\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.normal_(m.bias.data, mean=0.0, std=0.001)\n",
    "\n",
    "    def Encode(self, batch):\n",
    "        batch = self.dropout(batch)\n",
    "        if self.num_clusters == 1:\n",
    "            latent = self.encoder(batch)\n",
    "            mu = latent[:, :self.hidden_dims[-1]]\n",
    "            log_var = latent[:, self.hidden_dims[-1]:]\n",
    "\n",
    "            # self.training: model.train()일 때 True, model.eval()일 때 False\n",
    "            if self.training and self.reparam: noise = torch.randn_like(log_var) # log_var_clipped\n",
    "            else: noise = torch.zeros_like(log_var) # noise 안 줌.\n",
    "            latent = mu + torch.exp(0.5 * log_var) * noise\n",
    "\n",
    "            kl_divergence = -0.5 * torch.mean(torch.sum(1 + log_var - mu.pow(2) - log_var.exp(), dim=1))\n",
    "        else:\n",
    "            batch_per_cluster = []\n",
    "            for idx in range(self.num_clusters):\n",
    "                batch_per_cluster.append(batch[:, self.cluster_idx[idx]])\n",
    "            # (batch_size, num_items) -> ((batch_size, cluster1), (batch_size, cluster2), ...)\n",
    "            cluster_mu = []\n",
    "            cluster_log_var = []\n",
    "            for idx in range(self.num_clusters):\n",
    "                cluster_latent = self.encoder[idx](batch_per_cluster[idx])\n",
    "                cluster_mu.append(cluster_latent[:, :self.in_latent_dims_clusters[idx][-1]])\n",
    "                cluster_log_var.append(cluster_latent[:, self.in_latent_dims_clusters[idx][-1]:])\n",
    "\n",
    "            mu = torch.cat(tuple(cluster_mu), dim=-1)\n",
    "            log_var = torch.cat(tuple(cluster_log_var), dim=-1)\n",
    "\n",
    "            if self.training: noise = torch.randn_like(log_var) # log_var_clipped\n",
    "            else: noise = torch.zeros_like(log_var) # noise 안 줌.\n",
    "            latent = mu + torch.exp(0.5 * log_var) * noise\n",
    "\n",
    "            # Multinomial likelihood KL_D -> MultVAE 참고.\n",
    "            kl_divergence = -0.5 * torch.mean(torch.sum(1 + log_var - mu.pow(2) - log_var.exp(), dim=1))\n",
    "\n",
    "            batch_cluster = torch.cat(tuple(batch_per_cluster), dim=-1)\n",
    "        \n",
    "        # batch는 cluster 순서에 따라 정렬된 items로, decoder와의 loss 계산에 필요\n",
    "        # latent는 diffusion을 위해 필요\n",
    "        # kl_divergence는 loss계산에서 annealing한다. -> MultVAE와 같다.\n",
    "        return batch_cluster, latent, kl_divergence\n",
    "\n",
    "    def Decode(self, batch_latent):\n",
    "        if self.num_clusters == 1:\n",
    "            batch_recon = self.decoder(batch_latent)\n",
    "        else:\n",
    "            batch_cluster_recon = []\n",
    "            start = 0\n",
    "            for idx in range(self.num_clusters):\n",
    "                end = start + self.out_latent_dims_clusters[idx][0]\n",
    "                batch_cluster_recon.append(self.decoder[idx](batch_latent[:, start:end]))\n",
    "                start = end\n",
    "            batch_recon = torch.cat(tuple(batch_cluster_recon), dim=-1)\n",
    "        \n",
    "        return batch_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEmb(nn.Module):\n",
    "    def __init__(self, pos_dim):\n",
    "        super(PosEmb, self).__init__()\n",
    "        \"\"\"\n",
    "        Sinusoidal timestep positional encoding.\n",
    "        Args\n",
    "            pos_dim : embedding dimension\n",
    "        \"\"\"\n",
    "        self.pos_dim = pos_dim\n",
    "        self.time_mlp = nn.Linear(pos_dim, pos_dim)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.normal_(m.bias.data, mean=0.0, std=0.001)\n",
    "\n",
    "    def forward(self, t, max_period=10000):\n",
    "        t = t.unsqueeze(-1).type(torch.float)\n",
    "        half_dim = self.pos_dim // 2\n",
    "        w_k = 1.0 / (\n",
    "            max_period\n",
    "            ** (torch.arange(0, half_dim, 1, device=t.device).float() / (half_dim-1))\n",
    "        )\n",
    "\n",
    "        half_emb = t.repeat(1, half_dim)\n",
    "        pos_sin = torch.sin(half_emb * w_k)\n",
    "        pos_cos = torch.cos(half_emb * w_k)\n",
    "        pos_enc = torch.cat([pos_sin, pos_cos], dim=-1)\n",
    "\n",
    "        emb = self.time_mlp(pos_enc)\n",
    "        return emb\n",
    "\n",
    "\n",
    "class ReverseDiffusion(nn.Module):\n",
    "    def __init__(self, hidden_dims, latent_dim, step_dim=10, norm=False, droupout=0.5):\n",
    "        super(ReverseDiffusion, self).__init__()\n",
    "        \"\"\"\n",
    "        AutoEncoder 구조를 활용한다.\n",
    "        Args\n",
    "            hidden_dims : MLP dims for predict batch\n",
    "            latent_dim  : Latent vector dimension\n",
    "            step_dim    : timestep positional embedding dim.\n",
    "            norm        : batch normalization\n",
    "            dropout     : dropout\n",
    "        \"\"\"\n",
    "\n",
    "        self.norm = norm\n",
    "        self.pos_enc_layer = PosEmb(step_dim)\n",
    "        self.encoder = nn.ModuleList([])\n",
    "        self.decoder = nn.ModuleList([])\n",
    "        in_dims = [latent_dim] + hidden_dims\n",
    "        out_dims = in_dims[::-1]\n",
    "        # in_dims     : [latent_dim, ...]\n",
    "        # out_dims    : [..., latent_dim]\n",
    "\n",
    "        in_dims_w_step = [in_dims[0] + step_dim] + in_dims[1:] # [latent_dim + pos_emb_size] + hidden_dims\n",
    "        for d_in, d_out in zip(in_dims_w_step[:-1], in_dims_w_step[1:]):\n",
    "            self.encoder.append(nn.Linear(d_in, d_out))\n",
    "            self.encoder.append(nn.Tanh())\n",
    "        for d_in, d_out in zip(out_dims[:-1], out_dims[1:]):\n",
    "            self.decoder.append(nn.Linear(d_in, d_out))\n",
    "\n",
    "        self.drop = nn.Dropout(droupout)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.normal_(m.bias.data, mean=0.0, std=0.001)\n",
    "\n",
    "    def forward(self, x, timesteps):\n",
    "        time_emb = self.pos_enc_layer(timesteps)\n",
    "        if self.norm: x = F.normalize(x)\n",
    "        x = self.drop(x)\n",
    "        h = torch.cat([x, time_emb], dim=-1)\n",
    "        for idx, layer in enumerate(self.encoder):\n",
    "            h = layer(h)\n",
    "        for idx, layer in enumerate(self.decoder):\n",
    "            h = layer(h)\n",
    "            if idx != len(self.decoder) - 1:\n",
    "                h = torch.tanh(h)\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SNR(diffusion, t):\n",
    "    \"\"\"\n",
    "    Compute the signal-to-noise ratio for a single timestep.\n",
    "    \"\"\"\n",
    "    diffusion.alpha_bar = diffusion.alpha_bar.to(t.device)\n",
    "    return diffusion.alpha_bar[t] / (1 - diffusion.alpha_bar[t])\n",
    "\n",
    "\n",
    "def caculate_loss(args, model_reverse, model_vae, diffusion, batch, update_count, update_count_vae):\n",
    "    batch_cluster, latent, kl_divergence = model_vae.Encode(batch)\n",
    "    loss_diffusion, latent_hat = caculate_diffusion_loss(args, model_reverse, diffusion, latent)\n",
    "    batch_cluster_hat = model_vae.Decode(latent_hat)\n",
    "    loss_vae = caculate_vae_loss(args, batch_cluster, batch_cluster_hat, kl_divergence, update_count_vae)\n",
    "    \n",
    "    # 해당 부분은 나중에 더 자세히 봐야겠다.\n",
    "    # SNR과 관련 있나...?\n",
    "    if args.anneal_steps > 0: \n",
    "        # anneal 한다.\n",
    "        # 훈련이 진행될수록, diffusion 반영 정도가 줄어든다.\n",
    "        # 최대는 args.lmbda이고 최소는 args.anneal_cap이다.\n",
    "        anneal = max((1. - update_count / args.anneal_steps) * args.lamda, args.anneal_cap)\n",
    "    else:\n",
    "        anneal = args.lamda\n",
    "    \n",
    "    loss = loss_diffusion * anneal + loss_vae # 여기도 원래 논문의 코드와 다르다. 나중에 더 자세히 보자.\n",
    "    \n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def caculate_vae_loss(args, batch_cluster, batch_cluster_hat, kl_divergence, update_count_vae):\n",
    "    loss = -torch.mean(torch.sum(F.log_softmax(batch_cluster_hat, 1) * batch_cluster, -1))  # multinomial log likelihood in MultVAE\n",
    "    if args.vae_anneal_steps > 0: \n",
    "        # anneal 한다.\n",
    "        # 훈련이 진행될수록, KL 반영정도를 높여야 한다.\n",
    "        # args.vae_anneal_cap은 KL 반영정도의 최대값이다.\n",
    "        anneal = min(args.vae_anneal_cap, 1. * update_count_vae / args.vae_anneal_steps)\n",
    "    else:\n",
    "        # anneal 안 한다.\n",
    "        anneal = args.vae_anneal_ap\n",
    "    \n",
    "    return loss + anneal * kl_divergence\n",
    "\n",
    "\n",
    "def caculate_diffusion_loss(args, model, diffusion, batch):\n",
    "    \"\"\"\n",
    "    Importance sampling을 사용하기 위해 nn.MSE로 loss를 계산하지 않는다.\n",
    "    Batch별 즉, user별 loss를 계산한 다음 importance sampling을 위해 Lt_history에 저장한다.\n",
    "    그 다음, batch별 평균 값을 loss로 활용한다.\n",
    "    \"\"\"\n",
    "\n",
    "    timesteps, pt = diffusion.sample_steps(batch.shape[0])\n",
    "    if args.noise_scale != 0:\n",
    "        x_t, noise = diffusion.get_noised_interaction(batch, timesteps)\n",
    "    else:\n",
    "        # Denoise 과정이 없다.\n",
    "        # 즉, forward가 없다. 이러면 각 reverse는 x_t -> x_t를 복원하는 것이다.\n",
    "        # 결국 x_t -> x_t를 하는 AutoEndocer를 여러 개 쌓은 것과 같다.\n",
    "        x_t = batch\n",
    "\n",
    "\n",
    "    batch_hat = model(x_t, timesteps)\n",
    "\n",
    "    loss_batch_item = (batch_hat - batch) ** 2\n",
    "    loss_batch = loss_batch_item.mean(dim=1)\n",
    "\n",
    "    if args.snr is True:\n",
    "        # timestep마다 loss weight를 다르게 둔다.\n",
    "        weight = SNR(diffusion, timesteps - 1) - SNR(diffusion, timesteps)\n",
    "        weight = torch.where((timesteps == 0), 1.0, weight)\n",
    "    else:\n",
    "        weight = torch.tensor([1.0] * batch.shape[0]).to(args.device)\n",
    "    \n",
    "    weighted_loss_batch = weight * loss_batch\n",
    "\n",
    "    # Update Lt_history & Lt_count for importance sampling\n",
    "    for timestep, loss in zip(timesteps, weighted_loss_batch):\n",
    "        # loss는 timestep에 해당하는 loss 값이다.\n",
    "        if diffusion.Lt_count[timestep] == diffusion.num_for_expectation:\n",
    "            # 만약 history가 꽉 찼으면 old한 것을 버리고 새 것으로 채운다.\n",
    "            Lt_history_old = diffusion.Lt_history.clone()\n",
    "            diffusion.Lt_history[timestep, :-1] = Lt_history_old[timestep, 1:]\n",
    "            diffusion.Lt_history[timestep, -1] = loss.detach()\n",
    "        else:\n",
    "            try:\n",
    "                diffusion.Lt_history[timestep, diffusion.Lt_count[timestep]] = loss.detach()\n",
    "                diffusion.Lt_count[timestep] += 1\n",
    "            except:\n",
    "                print(timestep)\n",
    "                print(diffusion.Lt_count[timestep])\n",
    "                print(loss)\n",
    "                raise ValueError\n",
    "    \n",
    "    weighted_loss_batch /= pt # 논문 수식 14 참고.\n",
    "    weighted_loss = weighted_loss_batch.mean()\n",
    "    \n",
    "    # weighted_loss : back propa에 사용\n",
    "    # batch_hat: decoder input으로 사용.\n",
    "    return weighted_loss, batch_hat \n",
    "\n",
    "\n",
    "def train_one_epoch(args, model_reverse, model_vae, diffusion, \\\n",
    "        optimizer_reverse, optimizer_vae, dataloader, update_count, update_count_vae):\n",
    "    total_loss = 0.0\n",
    "    for batch in dataloader:\n",
    "        batch = batch.to(args.device)\n",
    "\n",
    "        loss = caculate_loss(args, \\\n",
    "            model_reverse, model_vae, diffusion, batch, update_count, update_count_vae)\n",
    "\n",
    "        total_loss += loss\n",
    "\n",
    "        optimizer_reverse.zero_grad()\n",
    "        optimizer_vae.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_reverse.step()\n",
    "        optimizer_vae.step()\n",
    "        update_count_vae += 1\n",
    "\n",
    "\n",
    "    return total_loss / len(dataloader), update_count_vae\n",
    "\n",
    "\n",
    "def compute_metric(target_items, predict_items, topK):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    ndcgs = []\n",
    "    mrrs = []\n",
    "    num_users = len(predict_items)\n",
    "\n",
    "    for k in topK:\n",
    "        sum_precision = sum_recall = sum_ndcg = sum_mrr = 0.0\n",
    "        for user_id in range(num_users):\n",
    "            if len(target_items[user_id]) == 0: continue\n",
    "            mrr_flag = True\n",
    "            num_hit = user_mrr = dcg = 0\n",
    "            \n",
    "            for rank_idx in range(k):\n",
    "                if predict_items[user_id][rank_idx] in target_items[user_id]:\n",
    "                    num_hit += 1 # precision, recall에 사용\n",
    "                    dcg += 1.0 / np.log2(rank_idx + 2)                    \n",
    "                    if mrr_flag:\n",
    "                        user_mrr = 1.0 / (rank_idx+1.0)\n",
    "                        mrr_flag = False\n",
    "            \n",
    "            idcg = 0.0\n",
    "            for rank_idx in range(len(target_items[user_id])):\n",
    "                idcg += 1.0/np.log2(rank_idx+2)\n",
    "            ndcg = (dcg/idcg)\n",
    "\n",
    "            sum_precision += num_hit / k\n",
    "            sum_recall += num_hit / len(target_items[user_id])\n",
    "            sum_ndcg += ndcg\n",
    "            sum_mrr += user_mrr\n",
    "\n",
    "        precision = round(sum_precision / num_users, 4)\n",
    "        recall = round(sum_recall / num_users, 4)\n",
    "        ndcg = round(sum_ndcg / num_users, 4)\n",
    "        mrr = round(sum_mrr / num_users, 4)\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        ndcgs.append(ndcg)\n",
    "        mrrs.append(mrr)\n",
    "\n",
    "    return precisions, recalls, ndcgs, mrrs\n",
    "\n",
    "\n",
    "def evaluate(args, model_reverse, model_vae, diffusion, loader, \\\n",
    "    label_items: sp.csr_matrix, consumed_items_mapped: sp.csr_matrix, topK: list):\n",
    "    \"\"\"\n",
    "    Args\n",
    "        args                    : hyper-parameters\n",
    "        model_reverse           : 학습된 reverse diffusion model\n",
    "        model_vae               : 학습된 MultVAE model\n",
    "        diffsuion               : Diffusion\n",
    "        loader                  : Test data loader // no_shffule\n",
    "        label_items             : Ground Truth, shape: (num_users, num_items) 중에서 target item에만 1\n",
    "        consumed_items          : training data에서 사용된 이미 user가 선호도를 보인 items. 나눠진 cluster 대로 mapping되어 있다.\n",
    "        topK                    : top K list ex) [10, 20, 50]\n",
    "    \"\"\"\n",
    "    model_reverse.eval()\n",
    "    model_vae.eval()\n",
    "    num_user = label_items.shape[0]\n",
    "    user_idx_list = list(range(label_items.shape[0]))\n",
    "    # target_items.shape[0] 대신 consumed_items.shape[0]도 ㄱㅊ\n",
    "\n",
    "    predict_items = []\n",
    "    target_items = []\n",
    "\n",
    "    for user_id in range(num_user):\n",
    "        # user_id에 해당하는, sp.csr_matrix로 저장되어 있는 user의 label item id를 list로 저장.\n",
    "        # nonzero()하면 (row array, col array) 반환.\n",
    "        # col array: np.ndarray의 idx 값이 item id임.\n",
    "        target_items.append(label_items[user_id,:].nonzero()[1].tolist())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(loader):\n",
    "            start_batch_user_id = batch_idx * args.batch_size\n",
    "            end_batch_user_id = start_batch_user_id + len(batch)\n",
    "            batch_consumed_items_mapped = consumed_items_mapped[user_idx_list[start_batch_user_id:end_batch_user_id]]\n",
    "            batch = batch.to(args.device)\n",
    "            _, batch_latent, _ = model_vae.Encode(batch)\n",
    "            batch_latent_hat = diffusion.sample_new_interaction(model_reverse, batch_latent, steps=args.sampling_steps, sampling_noise=False)\n",
    "            prediction_mapped = model_vae.Decode(batch_latent_hat)\n",
    "            prediction_mapped[batch_consumed_items_mapped.nonzero()] = -np.inf\n",
    "\n",
    "            _, indices_mapped = torch.topk(prediction_mapped, topK[-1]) # shape (batch[1].shape, topK[-1])\n",
    "            indices = model_vae.cluster_map[indices_mapped]\n",
    "            indices = indices.detach().cpu().numpy().tolist()\n",
    "            predict_items.extend(indices)\n",
    "\n",
    "        precisions, recalls, ndcgs, mrrs = compute_metric(target_items, predict_items, topK)\n",
    "    \n",
    "    return precisions, recalls, ndcgs, mrrs\n",
    "\n",
    "\n",
    "def change_cols_accrd_to_map(sp_matrix, col_order_list, num_users, num_items):\n",
    "    reverse_map = {col_order_list[i]:i for i in range(len(col_order_list))}\n",
    "    id_users, id_items = sp_matrix.nonzero() # non_zero 값을 가지는 (rows), (cols) pair 반환.\n",
    "    num_interactions = len(id_users)\n",
    "    id_items_mapped = np.array([reverse_map[id_items[idx]] for idx in range(num_interactions)])\n",
    "\n",
    "    data = np.ones(shape=(num_interactions,))\n",
    "    row = id_users\n",
    "    col = id_items_mapped\n",
    "    sp_matrix_mapped = sp.csr_matrix(\n",
    "        (data, (row, col)), dtype='float32', shape=(num_users, num_items)\n",
    "    )\n",
    "\n",
    "    return sp_matrix_mapped\n",
    "\n",
    "\n",
    "def print_metric_results(topK, results):\n",
    "    metric_list = ['Precision', 'Recall', 'nDCG', 'MRR']\n",
    "    for idx, metric in enumerate(metric_list):\n",
    "        str_result = ''\n",
    "        for k_idx, k in enumerate(topK):\n",
    "            str_metric = f'{metric}@{k}'\n",
    "            str_result += f'    {str_metric:14s}: {results[idx][k_idx]:.4f}'\n",
    "        print(str_result)\n",
    "\n",
    "\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user num: 5949\n",
      "item num: 2810\n",
      "running k-means on cuda..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[running kmeans]: 10it [00:00, 281.61it/s, center_shift=0.000061, iteration=10, tol=0.000100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 -  train loss:   992.6836,  time: 0:00:02\n",
      "Epoch 2 -  train loss:   995.6473,  time: 0:00:01\n",
      "Epoch 3 -  train loss:   990.9977,  time: 0:00:01\n",
      "Epoch 4 -  train loss:  1026.1639,  time: 0:00:02\n",
      "Epoch 5 -  train loss:   953.3059,  time: 0:00:02\n",
      "  Update Best\n",
      "  Validation data\n",
      "    Precision@10  : 0.0217    Precision@20  : 0.0179    Precision@50  : 0.0143    Precision@100 : 0.0120\n",
      "    Recall@10     : 0.0160    Recall@20     : 0.0238    Recall@50     : 0.0439    Recall@100    : 0.0688\n",
      "    nDCG@10       : 0.0152    nDCG@20       : 0.0200    nDCG@50       : 0.0294    nDCG@100      : 0.0395\n",
      "    MRR@10        : 0.0568    MRR@20        : 0.0631    MRR@50        : 0.0682    MRR@100       : 0.0700\n",
      "  Test data\n",
      "    Precision@10  : 0.0129    Precision@20  : 0.0101    Precision@50  : 0.0081    Precision@100 : 0.0067\n",
      "    Recall@10     : 0.0202    Recall@20     : 0.0282    Recall@50     : 0.0511    Recall@100    : 0.0783\n",
      "    nDCG@10       : 0.0144    nDCG@20       : 0.0182    nDCG@50       : 0.0268    nDCG@100      : 0.0353\n",
      "    MRR@10        : 0.0344    MRR@20        : 0.0383    MRR@50        : 0.0425    MRR@100       : 0.0441\n",
      "Epoch 6 -  train loss:  1002.2491,  time: 0:00:02\n",
      "Epoch 7 -  train loss:   994.9319,  time: 0:00:03\n",
      "Epoch 8 -  train loss:  1023.2197,  time: 0:00:02\n",
      "Epoch 9 -  train loss:  1020.4690,  time: 0:00:02\n",
      "Epoch 10 -  train loss:   987.4713,  time: 0:00:03\n",
      "  Update Best\n",
      "  Validation data\n",
      "    Precision@10  : 0.0270    Precision@20  : 0.0238    Precision@50  : 0.0198    Precision@100 : 0.0173\n",
      "    Recall@10     : 0.0181    Recall@20     : 0.0310    Recall@50     : 0.0604    Recall@100    : 0.1011\n",
      "    nDCG@10       : 0.0195    nDCG@20       : 0.0270    nDCG@50       : 0.0408    nDCG@100      : 0.0566\n",
      "    MRR@10        : 0.0810    MRR@20        : 0.0890    MRR@50        : 0.0948    MRR@100       : 0.0967\n",
      "  Test data\n",
      "    Precision@10  : 0.0152    Precision@20  : 0.0134    Precision@50  : 0.0113    Precision@100 : 0.0099\n",
      "    Recall@10     : 0.0216    Recall@20     : 0.0366    Recall@50     : 0.0690    Recall@100    : 0.1143\n",
      "    nDCG@10       : 0.0171    nDCG@20       : 0.0239    nDCG@50       : 0.0361    nDCG@100      : 0.0501\n",
      "    MRR@10        : 0.0461    MRR@20        : 0.0520    MRR@50        : 0.0571    MRR@100       : 0.0591\n",
      "Epoch 11 -  train loss:   936.0771,  time: 0:00:02\n",
      "Epoch 12 -  train loss:   884.4165,  time: 0:00:03\n",
      "Epoch 13 -  train loss:   968.7155,  time: 0:00:03\n",
      "Epoch 14 -  train loss:   914.3732,  time: 0:00:01\n",
      "Epoch 15 -  train loss:   929.6485,  time: 0:00:01\n",
      "  Update Best\n",
      "  Validation data\n",
      "    Precision@10  : 0.0315    Precision@20  : 0.0275    Precision@50  : 0.0247    Precision@100 : 0.0222\n",
      "    Recall@10     : 0.0191    Recall@20     : 0.0331    Recall@50     : 0.0741    Recall@100    : 0.1275\n",
      "    nDCG@10       : 0.0205    nDCG@20       : 0.0287    nDCG@50       : 0.0475    nDCG@100      : 0.0686\n",
      "    MRR@10        : 0.0838    MRR@20        : 0.0920    MRR@50        : 0.0985    MRR@100       : 0.1005\n",
      "  Test data\n",
      "    Precision@10  : 0.0167    Precision@20  : 0.0155    Precision@50  : 0.0143    Precision@100 : 0.0127\n",
      "    Recall@10     : 0.0200    Recall@20     : 0.0378    Recall@50     : 0.0848    Recall@100    : 0.1443\n",
      "    nDCG@10       : 0.0167    nDCG@20       : 0.0247    nDCG@50       : 0.0420    nDCG@100      : 0.0606\n",
      "    MRR@10        : 0.0466    MRR@20        : 0.0534    MRR@50        : 0.0595    MRR@100       : 0.0618\n",
      "Epoch 16 -  train loss:   907.8008,  time: 0:00:02\n",
      "Epoch 17 -  train loss:   954.6503,  time: 0:00:02\n",
      "Epoch 18 -  train loss:   909.9961,  time: 0:00:02\n",
      "Epoch 19 -  train loss:   910.3677,  time: 0:00:02\n",
      "Epoch 20 -  train loss:   871.2571,  time: 0:00:02\n",
      "  Validation data\n",
      "    Precision@10  : 0.0308    Precision@20  : 0.0293    Precision@50  : 0.0274    Precision@100 : 0.0246\n",
      "    Recall@10     : 0.0169    Recall@20     : 0.0328    Recall@50     : 0.0783    Recall@100    : 0.1367\n",
      "    nDCG@10       : 0.0192    nDCG@20       : 0.0287    nDCG@50       : 0.0498    nDCG@100      : 0.0730\n",
      "    MRR@10        : 0.0829    MRR@20        : 0.0921    MRR@50        : 0.0989    MRR@100       : 0.1008\n",
      "  Test data\n",
      "    Precision@10  : 0.0176    Precision@20  : 0.0170    Precision@50  : 0.0158    Precision@100 : 0.0141\n",
      "    Recall@10     : 0.0189    Recall@20     : 0.0360    Recall@50     : 0.0868    Recall@100    : 0.1545\n",
      "    nDCG@10       : 0.0164    nDCG@20       : 0.0248    nDCG@50       : 0.0434    nDCG@100      : 0.0645\n",
      "    MRR@10        : 0.0468    MRR@20        : 0.0540    MRR@50        : 0.0600    MRR@100       : 0.0623\n",
      "Epoch 21 -  train loss:   901.4724,  time: 0:00:02\n",
      "Epoch 22 -  train loss:   855.7778,  time: 0:00:02\n",
      "Epoch 23 -  train loss:   827.6324,  time: 0:00:02\n",
      "Epoch 24 -  train loss:   848.3100,  time: 0:00:02\n",
      "Epoch 25 -  train loss:   876.1969,  time: 0:00:02\n",
      "  Update Best\n",
      "  Validation data\n",
      "    Precision@10  : 0.0313    Precision@20  : 0.0307    Precision@50  : 0.0279    Precision@100 : 0.0266\n",
      "    Recall@10     : 0.0171    Recall@20     : 0.0346    Recall@50     : 0.0753    Recall@100    : 0.1460\n",
      "    nDCG@10       : 0.0197    nDCG@20       : 0.0302    nDCG@50       : 0.0500    nDCG@100      : 0.0775\n",
      "    MRR@10        : 0.0856    MRR@20        : 0.0956    MRR@50        : 0.1018    MRR@100       : 0.1041\n",
      "  Test data\n",
      "    Precision@10  : 0.0183    Precision@20  : 0.0177    Precision@50  : 0.0165    Precision@100 : 0.0153\n",
      "    Recall@10     : 0.0191    Recall@20     : 0.0375    Recall@50     : 0.0863    Recall@100    : 0.1625\n",
      "    nDCG@10       : 0.0170    nDCG@20       : 0.0259    nDCG@50       : 0.0446    nDCG@100      : 0.0682\n",
      "    MRR@10        : 0.0497    MRR@20        : 0.0567    MRR@50        : 0.0629    MRR@100       : 0.0653\n",
      "Epoch 26 -  train loss:   866.3075,  time: 0:00:02\n",
      "Epoch 27 -  train loss:   850.3272,  time: 0:00:02\n",
      "Epoch 28 -  train loss:   799.4141,  time: 0:00:02\n",
      "Epoch 29 -  train loss:   799.2834,  time: 0:00:02\n",
      "Epoch 30 -  train loss:   825.5317,  time: 0:00:02\n",
      "  Update Best\n",
      "  Validation data\n",
      "    Precision@10  : 0.0322    Precision@20  : 0.0318    Precision@50  : 0.0294    Precision@100 : 0.0277\n",
      "    Recall@10     : 0.0172    Recall@20     : 0.0360    Recall@50     : 0.0800    Recall@100    : 0.1501\n",
      "    nDCG@10       : 0.0202    nDCG@20       : 0.0313    nDCG@50       : 0.0527    nDCG@100      : 0.0804\n",
      "    MRR@10        : 0.0878    MRR@20        : 0.0978    MRR@50        : 0.1042    MRR@100       : 0.1063\n",
      "  Test data\n",
      "    Precision@10  : 0.0190    Precision@20  : 0.0191    Precision@50  : 0.0175    Precision@100 : 0.0159\n",
      "    Recall@10     : 0.0191    Recall@20     : 0.0405    Recall@50     : 0.0917    Recall@100    : 0.1649\n",
      "    nDCG@10       : 0.0174    nDCG@20       : 0.0276    nDCG@50       : 0.0471    nDCG@100      : 0.0703\n",
      "    MRR@10        : 0.0500    MRR@20        : 0.0577    MRR@50        : 0.0638    MRR@100       : 0.0660\n",
      "Epoch 31 -  train loss:   897.8160,  time: 0:00:02\n",
      "Epoch 32 -  train loss:   826.8094,  time: 0:00:02\n",
      "Epoch 33 -  train loss:   841.1296,  time: 0:00:02\n",
      "Epoch 34 -  train loss:   843.5850,  time: 0:00:02\n",
      "Epoch 35 -  train loss:   814.1401,  time: 0:00:02\n",
      "  Update Best\n",
      "  Validation data\n",
      "    Precision@10  : 0.0340    Precision@20  : 0.0326    Precision@50  : 0.0307    Precision@100 : 0.0291\n",
      "    Recall@10     : 0.0189    Recall@20     : 0.0364    Recall@50     : 0.0826    Recall@100    : 0.1597\n",
      "    nDCG@10       : 0.0210    nDCG@20       : 0.0317    nDCG@50       : 0.0543    nDCG@100      : 0.0843\n",
      "    MRR@10        : 0.0876    MRR@20        : 0.0969    MRR@50        : 0.1033    MRR@100       : 0.1055\n",
      "  Test data\n",
      "    Precision@10  : 0.0203    Precision@20  : 0.0196    Precision@50  : 0.0182    Precision@100 : 0.0167\n",
      "    Recall@10     : 0.0209    Recall@20     : 0.0397    Recall@50     : 0.0929    Recall@100    : 0.1744\n",
      "    nDCG@10       : 0.0186    nDCG@20       : 0.0281    nDCG@50       : 0.0486    nDCG@100      : 0.0741\n",
      "    MRR@10        : 0.0527    MRR@20        : 0.0599    MRR@50        : 0.0661    MRR@100       : 0.0685\n",
      "Epoch 36 -  train loss:   822.2693,  time: 0:00:02\n",
      "Epoch 37 -  train loss:   817.7991,  time: 0:00:02\n",
      "Epoch 38 -  train loss:   794.8790,  time: 0:00:02\n",
      "Epoch 39 -  train loss:   789.4745,  time: 0:00:02\n",
      "Epoch 40 -  train loss:   757.9499,  time: 0:00:02\n",
      "  Update Best\n",
      "  Validation data\n",
      "    Precision@10  : 0.0384    Precision@20  : 0.0370    Precision@50  : 0.0348    Precision@100 : 0.0330\n",
      "    Recall@10     : 0.0236    Recall@20     : 0.0428    Recall@50     : 0.0993    Recall@100    : 0.1900\n",
      "    nDCG@10       : 0.0240    nDCG@20       : 0.0358    nDCG@50       : 0.0625    nDCG@100      : 0.0974\n",
      "    MRR@10        : 0.0864    MRR@20        : 0.0957    MRR@50        : 0.1020    MRR@100       : 0.1042\n",
      "  Test data\n",
      "    Precision@10  : 0.0222    Precision@20  : 0.0218    Precision@50  : 0.0208    Precision@100 : 0.0195\n",
      "    Recall@10     : 0.0254    Recall@20     : 0.0457    Recall@50     : 0.1094    Recall@100    : 0.2154\n",
      "    nDCG@10       : 0.0219    nDCG@20       : 0.0323    nDCG@50       : 0.0567    nDCG@100      : 0.0888\n",
      "    MRR@10        : 0.0576    MRR@20        : 0.0655    MRR@50        : 0.0719    MRR@100       : 0.0745\n",
      "Epoch 41 -  train loss:   764.3939,  time: 0:00:02\n",
      "Epoch 42 -  train loss:   749.9564,  time: 0:00:02\n",
      "Epoch 43 -  train loss:   781.6900,  time: 0:00:02\n",
      "Epoch 44 -  train loss:   762.6741,  time: 0:00:02\n",
      "Epoch 45 -  train loss:   805.3878,  time: 0:00:02\n",
      "  Update Best\n",
      "  Validation data\n",
      "    Precision@10  : 0.0490    Precision@20  : 0.0475    Precision@50  : 0.0409    Precision@100 : 0.0366\n",
      "    Recall@10     : 0.0316    Recall@20     : 0.0604    Recall@50     : 0.1236    Recall@100    : 0.2168\n",
      "    nDCG@10       : 0.0343    nDCG@20       : 0.0509    nDCG@50       : 0.0807    nDCG@100      : 0.1168\n",
      "    MRR@10        : 0.1216    MRR@20        : 0.1316    MRR@50        : 0.1375    MRR@100       : 0.1392\n",
      "  Test data\n",
      "    Precision@10  : 0.0305    Precision@20  : 0.0288    Precision@50  : 0.0249    Precision@100 : 0.0223\n",
      "    Recall@10     : 0.0360    Recall@20     : 0.0687    Recall@50     : 0.1423    Recall@100    : 0.2517\n",
      "    nDCG@10       : 0.0316    nDCG@20       : 0.0468    nDCG@50       : 0.0746    nDCG@100      : 0.1085\n",
      "    MRR@10        : 0.0786    MRR@20        : 0.0870    MRR@50        : 0.0932    MRR@100       : 0.0955\n",
      "Epoch 46 -  train loss:   736.4919,  time: 0:00:02\n",
      "Epoch 47 -  train loss:   761.9234,  time: 0:00:02\n",
      "Epoch 48 -  train loss:   735.8900,  time: 0:00:02\n",
      "Epoch 49 -  train loss:   755.7050,  time: 0:00:02\n",
      "Epoch 50 -  train loss:   762.7495,  time: 0:00:02\n",
      "  Update Best\n",
      "  Validation data\n",
      "    Precision@10  : 0.0533    Precision@20  : 0.0508    Precision@50  : 0.0448    Precision@100 : 0.0386\n",
      "    Recall@10     : 0.0338    Recall@20     : 0.0646    Recall@50     : 0.1405    Recall@100    : 0.2411\n",
      "    nDCG@10       : 0.0373    nDCG@20       : 0.0551    nDCG@50       : 0.0898    nDCG@100      : 0.1274\n",
      "    MRR@10        : 0.1301    MRR@20        : 0.1401    MRR@50        : 0.1458    MRR@100       : 0.1477\n",
      "  Test data\n",
      "    Precision@10  : 0.0325    Precision@20  : 0.0314    Precision@50  : 0.0279    Precision@100 : 0.0234\n",
      "    Recall@10     : 0.0385    Recall@20     : 0.0756    Recall@50     : 0.1653    Recall@100    : 0.2737\n",
      "    nDCG@10       : 0.0342    nDCG@20       : 0.0514    nDCG@50       : 0.0845    nDCG@100      : 0.1178\n",
      "    MRR@10        : 0.0847    MRR@20        : 0.0937    MRR@50        : 0.1001    MRR@100       : 0.1023\n",
      "Epoch 51 -  train loss:   724.6131,  time: 0:00:02\n",
      "Epoch 52 -  train loss:   746.6681,  time: 0:00:02\n",
      "Epoch 53 -  train loss:   744.7361,  time: 0:00:02\n",
      "Epoch 54 -  train loss:   726.2070,  time: 0:00:02\n",
      "Epoch 55 -  train loss:   738.5532,  time: 0:00:02\n",
      "  Update Best\n",
      "  Validation data\n",
      "    Precision@10  : 0.0555    Precision@20  : 0.0533    Precision@50  : 0.0463    Precision@100 : 0.0388\n",
      "    Recall@10     : 0.0357    Recall@20     : 0.0671    Recall@50     : 0.1499    Recall@100    : 0.2439\n",
      "    nDCG@10       : 0.0396    nDCG@20       : 0.0582    nDCG@50       : 0.0949    nDCG@100      : 0.1307\n",
      "    MRR@10        : 0.1341    MRR@20        : 0.1439    MRR@50        : 0.1500    MRR@100       : 0.1517\n",
      "  Test data\n",
      "    Precision@10  : 0.0349    Precision@20  : 0.0338    Precision@50  : 0.0289    Precision@100 : 0.0236\n",
      "    Recall@10     : 0.0418    Recall@20     : 0.0827    Recall@50     : 0.1781    Recall@100    : 0.2767\n",
      "    nDCG@10       : 0.0372    nDCG@20       : 0.0560    nDCG@50       : 0.0904    nDCG@100      : 0.1213\n",
      "    MRR@10        : 0.0899    MRR@20        : 0.0991    MRR@50        : 0.1056    MRR@100       : 0.1075\n",
      "Epoch 56 -  train loss:   733.9121,  time: 0:00:02\n",
      "Epoch 57 -  train loss:   754.0455,  time: 0:00:02\n",
      "Epoch 58 -  train loss:   736.9380,  time: 0:00:02\n",
      "Epoch 59 -  train loss:   697.8333,  time: 0:00:02\n",
      "Epoch 60 -  train loss:   697.7650,  time: 0:00:02\n",
      "  Update Best\n",
      "  Validation data\n",
      "    Precision@10  : 0.0582    Precision@20  : 0.0551    Precision@50  : 0.0465    Precision@100 : 0.0390\n",
      "    Recall@10     : 0.0374    Recall@20     : 0.0705    Recall@50     : 0.1513    Recall@100    : 0.2453\n",
      "    nDCG@10       : 0.0408    nDCG@20       : 0.0601    nDCG@50       : 0.0960    nDCG@100      : 0.1320\n",
      "    MRR@10        : 0.1335    MRR@20        : 0.1428    MRR@50        : 0.1491    MRR@100       : 0.1508\n",
      "  Test data\n",
      "    Precision@10  : 0.0367    Precision@20  : 0.0351    Precision@50  : 0.0292    Precision@100 : 0.0237\n",
      "    Recall@10     : 0.0442    Recall@20     : 0.0870    Recall@50     : 0.1790    Recall@100    : 0.2782\n",
      "    nDCG@10       : 0.0386    nDCG@20       : 0.0583    nDCG@50       : 0.0916    nDCG@100      : 0.1227\n",
      "    MRR@10        : 0.0897    MRR@20        : 0.0990    MRR@50        : 0.1054    MRR@100       : 0.1072\n",
      "Epoch 61 -  train loss:   703.1373,  time: 0:00:02\n",
      "Epoch 62 -  train loss:   736.7652,  time: 0:00:02\n",
      "Epoch 63 -  train loss:   729.8080,  time: 0:00:02\n",
      "Epoch 64 -  train loss:   711.8214,  time: 0:00:02\n",
      "Epoch 65 -  train loss:   708.4597,  time: 0:00:02\n",
      "  Update Best\n",
      "  Validation data\n",
      "    Precision@10  : 0.0589    Precision@20  : 0.0556    Precision@50  : 0.0469    Precision@100 : 0.0390\n",
      "    Recall@10     : 0.0376    Recall@20     : 0.0713    Recall@50     : 0.1542    Recall@100    : 0.2453\n",
      "    nDCG@10       : 0.0412    nDCG@20       : 0.0607    nDCG@50       : 0.0972    nDCG@100      : 0.1323\n",
      "    MRR@10        : 0.1331    MRR@20        : 0.1428    MRR@50        : 0.1491    MRR@100       : 0.1508\n",
      "  Test data\n",
      "    Precision@10  : 0.0380    Precision@20  : 0.0352    Precision@50  : 0.0293    Precision@100 : 0.0238\n",
      "    Recall@10     : 0.0473    Recall@20     : 0.0864    Recall@50     : 0.1798    Recall@100    : 0.2794\n",
      "    nDCG@10       : 0.0404    nDCG@20       : 0.0586    nDCG@50       : 0.0926    nDCG@100      : 0.1236\n",
      "    MRR@10        : 0.0904    MRR@20        : 0.0991    MRR@50        : 0.1057    MRR@100       : 0.1076\n",
      "Epoch 66 -  train loss:   723.2394,  time: 0:00:02\n",
      "Epoch 67 -  train loss:   712.2277,  time: 0:00:02\n",
      "Epoch 68 -  train loss:   739.5388,  time: 0:00:02\n",
      "Epoch 69 -  train loss:   706.9243,  time: 0:00:02\n",
      "Epoch 70 -  train loss:   735.5410,  time: 0:00:02\n",
      "  Update Best\n",
      "  Validation data\n",
      "    Precision@10  : 0.0605    Precision@20  : 0.0558    Precision@50  : 0.0469    Precision@100 : 0.0390\n",
      "    Recall@10     : 0.0396    Recall@20     : 0.0720    Recall@50     : 0.1536    Recall@100    : 0.2455\n",
      "    nDCG@10       : 0.0426    nDCG@20       : 0.0613    nDCG@50       : 0.0977    nDCG@100      : 0.1329\n",
      "    MRR@10        : 0.1343    MRR@20        : 0.1440    MRR@50        : 0.1502    MRR@100       : 0.1518\n",
      "  Test data\n",
      "    Precision@10  : 0.0387    Precision@20  : 0.0354    Precision@50  : 0.0294    Precision@100 : 0.0238\n",
      "    Recall@10     : 0.0485    Recall@20     : 0.0867    Recall@50     : 0.1797    Recall@100    : 0.2794\n",
      "    nDCG@10       : 0.0417    nDCG@20       : 0.0596    nDCG@50       : 0.0936    nDCG@100      : 0.1247\n",
      "    MRR@10        : 0.0929    MRR@20        : 0.1019    MRR@50        : 0.1084    MRR@100       : 0.1103\n",
      "Epoch 71 -  train loss:   690.8014,  time: 0:00:02\n",
      "Epoch 72 -  train loss:   668.0010,  time: 0:00:02\n",
      "Epoch 73 -  train loss:   713.3008,  time: 0:00:02\n",
      "Epoch 74 -  train loss:   698.3497,  time: 0:00:02\n",
      "Epoch 75 -  train loss:   712.2023,  time: 0:00:02\n",
      "  Update Best\n",
      "  Validation data\n",
      "    Precision@10  : 0.0605    Precision@20  : 0.0568    Precision@50  : 0.0469    Precision@100 : 0.0390\n",
      "    Recall@10     : 0.0397    Recall@20     : 0.0759    Recall@50     : 0.1540    Recall@100    : 0.2456\n",
      "    nDCG@10       : 0.0426    nDCG@20       : 0.0630    nDCG@50       : 0.0981    nDCG@100      : 0.1332\n",
      "    MRR@10        : 0.1337    MRR@20        : 0.1446    MRR@50        : 0.1503    MRR@100       : 0.1520\n",
      "  Test data\n",
      "    Precision@10  : 0.0392    Precision@20  : 0.0362    Precision@50  : 0.0295    Precision@100 : 0.0238\n",
      "    Recall@10     : 0.0486    Recall@20     : 0.0917    Recall@50     : 0.1818    Recall@100    : 0.2791\n",
      "    nDCG@10       : 0.0423    nDCG@20       : 0.0617    nDCG@50       : 0.0947    nDCG@100      : 0.1253\n",
      "    MRR@10        : 0.0938    MRR@20        : 0.1032    MRR@50        : 0.1094    MRR@100       : 0.1112\n",
      "Epoch 76 -  train loss:   689.0377,  time: 0:00:02\n",
      "Epoch 77 -  train loss:   695.7532,  time: 0:00:02\n",
      "Epoch 78 -  train loss:   711.7550,  time: 0:00:02\n",
      "Epoch 79 -  train loss:   683.9995,  time: 0:00:02\n",
      "Epoch 80 -  train loss:   674.7437,  time: 0:00:02\n",
      "  Update Best\n",
      "  Validation data\n",
      "    Precision@10  : 0.0609    Precision@20  : 0.0569    Precision@50  : 0.0468    Precision@100 : 0.0391\n",
      "    Recall@10     : 0.0403    Recall@20     : 0.0763    Recall@50     : 0.1519    Recall@100    : 0.2450\n",
      "    nDCG@10       : 0.0435    nDCG@20       : 0.0637    nDCG@50       : 0.0981    nDCG@100      : 0.1336\n",
      "    MRR@10        : 0.1396    MRR@20        : 0.1501    MRR@50        : 0.1557    MRR@100       : 0.1574\n",
      "  Test data\n",
      "    Precision@10  : 0.0390    Precision@20  : 0.0362    Precision@50  : 0.0293    Precision@100 : 0.0239\n",
      "    Recall@10     : 0.0477    Recall@20     : 0.0921    Recall@50     : 0.1799    Recall@100    : 0.2785\n",
      "    nDCG@10       : 0.0419    nDCG@20       : 0.0617    nDCG@50       : 0.0941    nDCG@100      : 0.1251\n",
      "    MRR@10        : 0.0939    MRR@20        : 0.1033    MRR@50        : 0.1093    MRR@100       : 0.1113\n",
      "Epoch 81 -  train loss:   693.8958,  time: 0:00:02\n",
      "Epoch 82 -  train loss:   668.2509,  time: 0:00:02\n",
      "Epoch 83 -  train loss:   665.6014,  time: 0:00:02\n",
      "Epoch 84 -  train loss:   666.2552,  time: 0:00:02\n",
      "Epoch 85 -  train loss:   702.1295,  time: 0:00:02\n",
      "  Validation data\n",
      "    Precision@10  : 0.0613    Precision@20  : 0.0566    Precision@50  : 0.0469    Precision@100 : 0.0390\n",
      "    Recall@10     : 0.0407    Recall@20     : 0.0761    Recall@50     : 0.1521    Recall@100    : 0.2436\n",
      "    nDCG@10       : 0.0441    nDCG@20       : 0.0639    nDCG@50       : 0.0985    nDCG@100      : 0.1335\n",
      "    MRR@10        : 0.1417    MRR@20        : 0.1524    MRR@50        : 0.1580    MRR@100       : 0.1596\n",
      "  Test data\n",
      "    Precision@10  : 0.0394    Precision@20  : 0.0362    Precision@50  : 0.0294    Precision@100 : 0.0237\n",
      "    Recall@10     : 0.0493    Recall@20     : 0.0919    Recall@50     : 0.1800    Recall@100    : 0.2756\n",
      "    nDCG@10       : 0.0428    nDCG@20       : 0.0620    nDCG@50       : 0.0945    nDCG@100      : 0.1247\n",
      "    MRR@10        : 0.0951    MRR@20        : 0.1044    MRR@50        : 0.1104    MRR@100       : 0.1123\n",
      "Epoch 86 -  train loss:   657.9160,  time: 0:00:02\n",
      "Epoch 87 -  train loss:   675.4625,  time: 0:00:02\n",
      "Epoch 88 -  train loss:   695.7512,  time: 0:00:02\n",
      "Epoch 89 -  train loss:   676.9339,  time: 0:00:02\n",
      "Epoch 90 -  train loss:   691.7277,  time: 0:00:02\n",
      "  Update Best\n",
      "  Validation data\n",
      "    Precision@10  : 0.0616    Precision@20  : 0.0568    Precision@50  : 0.0468    Precision@100 : 0.0391\n",
      "    Recall@10     : 0.0414    Recall@20     : 0.0767    Recall@50     : 0.1519    Recall@100    : 0.2451\n",
      "    nDCG@10       : 0.0446    nDCG@20       : 0.0644    nDCG@50       : 0.0988    nDCG@100      : 0.1342\n",
      "    MRR@10        : 0.1436    MRR@20        : 0.1542    MRR@50        : 0.1597    MRR@100       : 0.1614\n",
      "  Test data\n",
      "    Precision@10  : 0.0395    Precision@20  : 0.0362    Precision@50  : 0.0293    Precision@100 : 0.0238\n",
      "    Recall@10     : 0.0495    Recall@20     : 0.0927    Recall@50     : 0.1785    Recall@100    : 0.2769\n",
      "    nDCG@10       : 0.0431    nDCG@20       : 0.0625    nDCG@50       : 0.0945    nDCG@100      : 0.1254\n",
      "    MRR@10        : 0.0968    MRR@20        : 0.1062    MRR@50        : 0.1121    MRR@100       : 0.1141\n",
      "Epoch 91 -  train loss:   665.8064,  time: 0:00:02\n",
      "Epoch 92 -  train loss:   645.7542,  time: 0:00:02\n",
      "Epoch 93 -  train loss:   676.2661,  time: 0:00:02\n",
      "Epoch 94 -  train loss:   651.0629,  time: 0:00:02\n",
      "Epoch 95 -  train loss:   665.1912,  time: 0:00:02\n",
      "  Update Best\n",
      "  Validation data\n",
      "    Precision@10  : 0.0608    Precision@20  : 0.0567    Precision@50  : 0.0467    Precision@100 : 0.0389\n",
      "    Recall@10     : 0.0408    Recall@20     : 0.0768    Recall@50     : 0.1517    Recall@100    : 0.2413\n",
      "    nDCG@10       : 0.0440    nDCG@20       : 0.0641    nDCG@50       : 0.0984    nDCG@100      : 0.1330\n",
      "    MRR@10        : 0.1424    MRR@20        : 0.1529    MRR@50        : 0.1586    MRR@100       : 0.1602\n",
      "  Test data\n",
      "    Precision@10  : 0.0393    Precision@20  : 0.0360    Precision@50  : 0.0293    Precision@100 : 0.0237\n",
      "    Recall@10     : 0.0487    Recall@20     : 0.0925    Recall@50     : 0.1792    Recall@100    : 0.2747\n",
      "    nDCG@10       : 0.0428    nDCG@20       : 0.0623    nDCG@50       : 0.0945    nDCG@100      : 0.1248\n",
      "    MRR@10        : 0.0975    MRR@20        : 0.1069    MRR@50        : 0.1128    MRR@100       : 0.1147\n",
      "Epoch 96 -  train loss:   672.6663,  time: 0:00:02\n",
      "Epoch 97 -  train loss:   684.4453,  time: 0:00:02\n",
      "Epoch 98 -  train loss:   662.3703,  time: 0:00:02\n",
      "Epoch 99 -  train loss:   655.5536,  time: 0:00:02\n",
      "Epoch 100 -  train loss:   663.6791,  time: 0:00:02\n",
      "  Validation data\n",
      "    Precision@10  : 0.0604    Precision@20  : 0.0565    Precision@50  : 0.0465    Precision@100 : 0.0390\n",
      "    Recall@10     : 0.0398    Recall@20     : 0.0761    Recall@50     : 0.1506    Recall@100    : 0.2405\n",
      "    nDCG@10       : 0.0437    nDCG@20       : 0.0639    nDCG@50       : 0.0980    nDCG@100      : 0.1328\n",
      "    MRR@10        : 0.1438    MRR@20        : 0.1541    MRR@50        : 0.1597    MRR@100       : 0.1614\n",
      "  Test data\n",
      "    Precision@10  : 0.0390    Precision@20  : 0.0360    Precision@50  : 0.0293    Precision@100 : 0.0238\n",
      "    Recall@10     : 0.0477    Recall@20     : 0.0918    Recall@50     : 0.1784    Recall@100    : 0.2739\n",
      "    nDCG@10       : 0.0424    nDCG@20       : 0.0620    nDCG@50       : 0.0943    nDCG@100      : 0.1246\n",
      "    MRR@10        : 0.0979    MRR@20        : 0.1072    MRR@50        : 0.1132    MRR@100       : 0.1151\n",
      "Epoch 101 -  train loss:   642.9111,  time: 0:00:02\n",
      "Epoch 102 -  train loss:   660.8240,  time: 0:00:02\n",
      "Epoch 103 -  train loss:   640.5331,  time: 0:00:02\n",
      "Epoch 104 -  train loss:   671.4965,  time: 0:00:02\n",
      "Epoch 105 -  train loss:   660.1649,  time: 0:00:02\n",
      "  Validation data\n",
      "    Precision@10  : 0.0601    Precision@20  : 0.0561    Precision@50  : 0.0463    Precision@100 : 0.0390\n",
      "    Recall@10     : 0.0397    Recall@20     : 0.0762    Recall@50     : 0.1495    Recall@100    : 0.2397\n",
      "    nDCG@10       : 0.0435    nDCG@20       : 0.0637    nDCG@50       : 0.0975    nDCG@100      : 0.1324\n",
      "    MRR@10        : 0.1435    MRR@20        : 0.1535    MRR@50        : 0.1592    MRR@100       : 0.1608\n",
      "  Test data\n",
      "    Precision@10  : 0.0385    Precision@20  : 0.0360    Precision@50  : 0.0292    Precision@100 : 0.0238\n",
      "    Recall@10     : 0.0471    Recall@20     : 0.0918    Recall@50     : 0.1772    Recall@100    : 0.2738\n",
      "    nDCG@10       : 0.0417    nDCG@20       : 0.0616    nDCG@50       : 0.0935    nDCG@100      : 0.1241\n",
      "    MRR@10        : 0.0964    MRR@20        : 0.1059    MRR@50        : 0.1120    MRR@100       : 0.1138\n",
      "Epoch 106 -  train loss:   653.1814,  time: 0:00:02\n",
      "Epoch 107 -  train loss:   660.8026,  time: 0:00:02\n",
      "Epoch 108 -  train loss:   642.9017,  time: 0:00:02\n",
      "Epoch 109 -  train loss:   638.9182,  time: 0:00:02\n",
      "Epoch 110 -  train loss:   677.5181,  time: 0:00:02\n",
      "  Validation data\n",
      "    Precision@10  : 0.0603    Precision@20  : 0.0552    Precision@50  : 0.0460    Precision@100 : 0.0390\n",
      "    Recall@10     : 0.0399    Recall@20     : 0.0749    Recall@50     : 0.1478    Recall@100    : 0.2394\n",
      "    nDCG@10       : 0.0435    nDCG@20       : 0.0630    nDCG@50       : 0.0967    nDCG@100      : 0.1320\n",
      "    MRR@10        : 0.1426    MRR@20        : 0.1522    MRR@50        : 0.1580    MRR@100       : 0.1597\n",
      "  Test data\n",
      "    Precision@10  : 0.0389    Precision@20  : 0.0354    Precision@50  : 0.0293    Precision@100 : 0.0238\n",
      "    Recall@10     : 0.0477    Recall@20     : 0.0897    Recall@50     : 0.1766    Recall@100    : 0.2740\n",
      "    nDCG@10       : 0.0421    nDCG@20       : 0.0609    nDCG@50       : 0.0934    nDCG@100      : 0.1241\n",
      "    MRR@10        : 0.0968    MRR@20        : 0.1059    MRR@50        : 0.1121    MRR@100       : 0.1139\n",
      "Epoch 111 -  train loss:   624.4733,  time: 0:00:02\n",
      "Epoch 112 -  train loss:   662.7532,  time: 0:00:02\n",
      "Epoch 113 -  train loss:   634.6915,  time: 0:00:01\n",
      "Epoch 114 -  train loss:   631.3134,  time: 0:00:02\n",
      "------------------\n",
      "Exiting from training early\n"
     ]
    }
   ],
   "source": [
    "dict_args = {}\n",
    "args = dotdict(dict_args)\n",
    "\n",
    "# Training hyper\n",
    "args.dataset_name = 'ml-1m_clean'\n",
    "args.file_name = 'ratings.dat'\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "args.batch_size = 400\n",
    "args.lr = 1e-4\n",
    "args.weight_decay = 0.0\n",
    "args.epochs = 1000\n",
    "args.topK = [10, 20, 50, 100]\n",
    "args.num_cluster = 3\n",
    "# diffusion의 denosing matching term도 KL_D를 줄이는 loss이다.\n",
    "# 그렇기 때문에 VAE_loss는 recon error, diffsuion loss는 KL_D로 볼 수 있고,\n",
    "# MultVAE처럼 VAE_loss와 Diffusion loss를 annealing할 수 있다.\n",
    "args.anneal_cap = 0.005 \n",
    "args.anneal_steps = 500\n",
    "args.lamda = 0.3\n",
    "\n",
    "# reverse hyper\n",
    "args.hidden_dims_diffusion = [300]\n",
    "args.norm = True\n",
    "\n",
    "# vae hyper\n",
    "args.hidden_dims_vae = [300]\n",
    "args.num_clusters = 3\n",
    "args.prob = 0.03 # for multinomial log-likelihood\n",
    "args.vae_anneal_cap = 0.3\n",
    "args.vae_anneal_steps = 200\n",
    "\n",
    "# diffusion hyper\n",
    "args.beta_start = 1e-4\n",
    "args.beta_end = 0.02\n",
    "args.noise_scale = 0.1\n",
    "args.steps = 100\n",
    "args.snr = True # assign different weight to different timestep or not\n",
    "args.sampling_steps = 10\n",
    "\n",
    "\n",
    "\n",
    "dir_path = os.path.join(os.getcwd(), args.dataset_name)\n",
    "sp_train, sp_valid, sp_test, num_users, num_items, emb_items = data_load(dir_path)\n",
    "train_dataset = DataDiffusion(torch.FloatTensor(sp_train.toarray()))\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, pin_memory=True, shuffle=True)\n",
    "test_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "# Build Diffusion and ReverseDiffusion\n",
    "diffusion = Diffusion()\n",
    "latent_dim = args.hidden_dims_vae[-1]\n",
    "model_reverse = ReverseDiffusion(args.hidden_dims_diffusion, latent_dim).to(args.device)\n",
    "optimizer_reverse = torch.optim.AdamW(model_reverse.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "# Build VAE\n",
    "model_vae = VAE(num_items=num_items, emb_items=emb_items, \\\n",
    "    num_clusters=args.num_clusters, hidden_dims=args.hidden_dims_vae, device=args.device).to(args.device)\n",
    "optimizer_vae = torch.optim.AdamW(model_vae.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "if args.num_clusters > 1:\n",
    "    # VAE model에서 생성한 cluster map에 따라 sp_train의 item 순서를 바꾼다.\n",
    "    sp_train_mapped = change_cols_accrd_to_map(sp_train, model_vae.cluster_map.detach().cpu().numpy(), num_users, num_items)\n",
    "else:\n",
    "    sp_train_mapped = sp_train\n",
    "\n",
    "best_recall, best_epoch = -100, 0\n",
    "best_test_result = None\n",
    "print(\"Start training\")\n",
    "update_count_vae = 0\n",
    "for epoch in range(args.epochs):\n",
    "    if epoch - best_epoch >= 20: # early stopping\n",
    "        print('-'*18)\n",
    "        print('Exiting from training early')\n",
    "        break\n",
    "\n",
    "    model_reverse.train()\n",
    "    model_vae.train()  \n",
    "    start = time.time()\n",
    "    avg_loss, update_count_vae = train_one_epoch(args, model_reverse, model_vae, diffusion, \\\n",
    "        optimizer_reverse, optimizer_vae, train_loader, epoch, update_count_vae)\n",
    "    print(f'Epoch {epoch+1} -  train loss: {avg_loss: >10.4f},  time: {str(timedelta(seconds=int(time.time() - start)))}')\n",
    "\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        val_results = evaluate(\n",
    "            args, model_reverse, model_vae, diffusion, test_loader, sp_valid, sp_train_mapped, args.topK)\n",
    "        test_results = evaluate(\n",
    "            args, model_reverse, model_vae, diffusion, test_loader, sp_test, sp_train_mapped, args.topK)\n",
    "    \n",
    "        val_recalls = val_results[1]\n",
    "        if val_recalls[1] > best_recall:\n",
    "            best_recall, best_epoch = val_recalls[1], epoch\n",
    "            print('  Update Best')\n",
    "\n",
    "        print('  Validation data')\n",
    "        print_metric_results(args.topK, val_results)\n",
    "        print('  Test data')\n",
    "        print_metric_results(args.topK, test_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
