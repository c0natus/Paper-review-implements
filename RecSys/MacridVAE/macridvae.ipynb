{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Miniconda3\\envs\\rec\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from datetime import timedelta\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load(dir_path):\n",
    "    train_path = os.path.join(dir_path, 'train_list.npy')\n",
    "    valid_path = os.path.join(dir_path, 'valid_list.npy')\n",
    "    test_path = os.path.join(dir_path, 'test_list.npy')\n",
    "\n",
    "    train_list = np.load(train_path, allow_pickle=True)\n",
    "    valid_list = np.load(valid_path, allow_pickle=True)\n",
    "    test_list = np.load(test_path, allow_pickle=True)\n",
    "\n",
    "    uid_max = 0\n",
    "    iid_max = 0\n",
    "    train_dict = {}\n",
    "\n",
    "    for uid, iid in train_list:\n",
    "        if uid not in train_dict:\n",
    "            train_dict[uid] = []\n",
    "        train_dict[uid].append(iid)\n",
    "        if uid > uid_max:\n",
    "            uid_max = uid\n",
    "        if iid > iid_max:\n",
    "            iid_max = iid\n",
    "    \n",
    "    n_user = uid_max + 1\n",
    "    n_item = iid_max + 1\n",
    "    print(f'user num: {n_user}')\n",
    "    print(f'item num: {n_item}')\n",
    "\n",
    "    train_data = sp.csr_matrix((np.ones_like(train_list[:, 0]), \\\n",
    "        (train_list[:, 0], train_list[:, 1])), dtype='float64', \\\n",
    "        shape=(n_user, n_item))\n",
    "    \n",
    "    valid_y_data = sp.csr_matrix((np.ones_like(valid_list[:, 0]),\n",
    "                 (valid_list[:, 0], valid_list[:, 1])), dtype='float64',\n",
    "                 shape=(n_user, n_item))  # valid_groundtruth\n",
    "\n",
    "    test_y_data = sp.csr_matrix((np.ones_like(test_list[:, 0]),\n",
    "                 (test_list[:, 0], test_list[:, 1])), dtype='float64',\n",
    "                 shape=(n_user, n_item))  # test_groundtruth\n",
    "    \n",
    "    return train_data, valid_y_data, test_y_data, n_user, n_item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataMacridVAE(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.data = dataset\n",
    "\n",
    "    def __getitem__(self, index): \n",
    "        item = self.data[index]\n",
    "        return item\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MacridVAE(nn.Module):\n",
    "    def __init__(self, args, num_items, dropout=0.5):\n",
    "        super(MacridVAE, self).__init__()\n",
    "        self.args = args\n",
    "        if args.dropout: dropout = args.dropout\n",
    "        self.num_items = num_items\n",
    "    \n",
    "        self.proto_type = nn.Parameter(torch.Tensor(args.num_concepts, args.dim_item))\n",
    "        # Item의 concept에 대한 representation이다.\n",
    "        # Item들은 가장 가까운 concept으로 배정된다.\n",
    "        # Consine similarity로 거리가 계산되기 때문에, proto type의 emb dim은 item의 emb dim과 같아야 한다.\n",
    "        self.emb_items = nn.Parameter(torch.Tensor(num_items, args.dim_item))\n",
    "\n",
    "        dims_encoder = [num_items, args.dim_item, args.dim_item * 2]\n",
    "        # 첫 번째 layer는 논문에 있는 context embedding을 만드는 것이다.\n",
    "        # 두 번째 layer는 논문에서 f_{nn}에 해당하는 shallow network이다.\n",
    "        # Decoder에서 item과 내적을 하기 때문에 평균과 분산의 dimension은 item과 같아야 한다.\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.encoder_list = nn.ModuleList([])\n",
    "        for d_in, d_out in zip(dims_encoder[:-1], dims_encoder[1:]):\n",
    "            self.encoder_list.append(nn.Linear(d_in, d_out))\n",
    "            self.encoder_list.append(nn.Tanh())\n",
    "        self.encoder = nn.Sequential(*self.encoder_list[:-1]) # 마지막엔 activation function 추가 안 한다.\n",
    "        del self.encoder_list\n",
    "\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.normal_(m.bias.data, mean=0.0, std=0.001)\n",
    "\n",
    "        nn.init.xavier_normal_(self.proto_type)\n",
    "        nn.init.xavier_normal_(self.emb_items)\n",
    "\n",
    "    def compute_cosin_sim(self, mat1, mat2, tau=0.1, eps=1e-8):\n",
    "        norm_mat1 = mat1.norm(dim=1)[:, None]\n",
    "        norm_mat2 = mat2.norm(dim=1)[:, None]\n",
    "        mat1_normed = mat1 / torch.max(norm_mat1, eps * torch.ones_like(norm_mat1))\n",
    "        mat2_normed = mat2 / torch.max(norm_mat2, eps * torch.ones_like(norm_mat2))\n",
    "        cos_sim = torch.einsum('ix, jx -> ij', [mat1_normed, mat2_normed])\n",
    "\n",
    "        return cos_sim / tau # shape (len(mat1[0]), len(mat2[0]))\n",
    "\n",
    "    def Encode(self, batch):\n",
    "        # User의 k개 concept(preference)를 encoding한다.\n",
    "        # batch - shape: (batch_size, num_items: interaction이 있으면 1)\n",
    "        batch_size = batch.shape[0]\n",
    "\n",
    "        probs = self.compute_cosin_sim(self.emb_items, self.proto_type) # shape: (self.M, num_concepts)\n",
    "        if self.training: concept_mask = F.gumbel_softmax(logits=probs, hard=False) # shape: (self.M, num_concept: one-hot) -> 논문에서는 hard, 구현은 soft...\n",
    "        else: concept_mask = F.softmax(probs, dim=-1) # Test 때 mode 사용.즉, sampling 안 함.\n",
    "        \n",
    "        concept_mask = concept_mask.t().expand(batch_size, self.args.num_concepts, self.num_items)\n",
    "        # shape: (batch_size, num_concepts, num_items)\n",
    "        # concept_mask글 batch_size 만큼 반복해서 생성한다.\n",
    "        # 각 concept에 해당하는 item들이 1로 표시되어 있는데, 이를 batch_size만큼 반복해서 생성한다.\n",
    "        # concept_mask[0][0]은 concept 0에 해당하는 item들이 1로 표시되어 있다.\n",
    "        batch = self.dropout(batch)\n",
    "        batch = batch.reshape(batch_size, 1, self.num_items) * concept_mask\n",
    "        batch = F.normalize(batch, dim=-1) # 논문 eq. 7의 첫 번재 term에서 L2_norm을 하고 있다.\n",
    "        # shape: (batch_size, num_concepts, num_items)\n",
    "        # batch.reshape(): \n",
    "        #   Batch user 중 한 user의 interactions는 [0, 1, 0, ..., 1, 1] 등으로 되어 있다.\n",
    "        # 이를 concept_mask과 곱해 interaction이 있는 item 들을 k concept으로 나누게 된다.\n",
    "        #   예를 들어, 2개의 concept이 있을 때, concept_mask이 [[1, 0, 0, ..., 0, 1], [0, 1, 1, ..., 1, 0]]와 같다면,\n",
    "        #   한 user의 interaction은 이것과 곱해져 [[0, 0, 0, ..., 0, 1], [0, 1, 0, ..., 1, 0]]을 나눠지게 된다.\n",
    "        # 즉, concept_mask은 masking 역할이다.\n",
    "\n",
    "        batch = batch.reshape(batch_size * self.args.num_concepts, self.num_items)\n",
    "        # shape: (batch_size * num_concepts, num_items)\n",
    "        # Decoder에서 cosine similarity 그리고 reconsturciton, KL loss를 편하게 구하기 위해서 먼저 이렇게 만든다.\n",
    "    \n",
    "        h = self.encoder(batch)\n",
    "\n",
    "        # shape: (batch_size * num_concepts, dim_item * 2)\n",
    "        # Interactions를 이용해 k 개 concept latent vector의 평균, 분산을 얻었다.\n",
    "        mu, log_var = h[:, :self.args.dim_item], h[:, self.args.dim_item:] # shape: (batch_size * num_concepts, dim_item)\n",
    "        latent = self.gaussian_sampling(mu, log_var) # shape: (batch_size * num_concepts, dim_item)\n",
    "        return latent, concept_mask, mu, log_var\n",
    "        # shape: (batch_size * num_concepts, dim_item), (batch_size, num_items, num_concept), \n",
    "        #        (batch_size * num_concepts, dim_item), (batch_size * num_concepts, dim_item)\n",
    "\n",
    "    def Decode(self, latent, concept_mask):\n",
    "        # latent - shape: (batch_size * num_concepts, dim_item), \n",
    "        # concept_mask - shape: (batch_size, num_items, num_concept)\n",
    "        batch_size = latent.shape[0] // self.args.num_concepts\n",
    "        logits = self.compute_cosin_sim(latent, self.emb_items) # shape: (batch_size * num_concepts, num_items)\n",
    "        probs_concept = torch.exp(logits).reshape(batch_size, self.args.num_concepts, self.num_items) # shape: (batch_size, num_concepts, num_items)\n",
    "        # 각 batch user 마다 concept 별 각 concept에 있는 items을 소비할 확률값.\n",
    "        batch_hat_concept = probs_concept * concept_mask # shape: (batch_size, num_concepts, num_items)\n",
    "        batch_hat = torch.sum(batch_hat_concept, dim=1) # shape: (batch_size, num_items)\n",
    "        batch_hat = torch.log(batch_hat) # 논문 Algorithm 1의 아랫부분을 보면 log를 두번 해준다... 왜 그렇지??\n",
    "        batch_hat = F.log_softmax(batch_hat, dim=-1)\n",
    "        return batch_hat\n",
    "\n",
    "\n",
    "    def gaussian_sampling(self, mu, log_var):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * log_var)\n",
    "            noise = torch.randn_like(std)\n",
    "            return mu + std * noise\n",
    "        else:\n",
    "            return mu\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        # batch - shape: (batch_size, num_items)\n",
    "        latent, concept_mask, mu, log_var = self.Encode(batch)\n",
    "        batch_hat = self.Decode(latent, concept_mask)\n",
    "        loss_recon, loss_kl = self.compute_loss(batch, batch_hat, mu, log_var)\n",
    "        return batch_hat, loss_recon, loss_kl\n",
    "    \n",
    "    def compute_loss(self, batch, batch_hat, mu, log_var):\n",
    "        loss_recon = torch.mean(torch.sum(-batch_hat * batch, dim=1)) # Reconstruction error: \\sum_{u,i == 1} -ln p_{u,i}\n",
    "        loss_kl = torch.mean(0.5 * torch.sum(torch.exp(log_var) + mu ** 2 - 1 - log_var, dim=1)) # Multinomial KL divergence 계산.\n",
    "        return loss_recon, loss_kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_one_epoch(args, model, optimizer, dataloader, update_count_vae):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in dataloader:\n",
    "        batch = batch.to(args.device)\n",
    "\n",
    "        prediction, loss_recon, loss_kl = model(batch) # model 자체에서 loss들이 계산됨. Prediction은 나중에 evaluation할 때 필요.\n",
    "        annealing = min(args.beta, update_count_vae / (500 * args.batch_size)) \n",
    "        # annealing: beta 값을 넘어가면 안됨. -> 논문에서는 micro disentangle 때문에, 1 넘게 준다고 하는데 이상하네\n",
    "        loss = loss_recon + annealing * loss_kl\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        update_count_vae += 1\n",
    "\n",
    "\n",
    "    return total_loss / len(dataloader), update_count_vae\n",
    "\n",
    "\n",
    "def recall_kth(preds, labels, k=100):\n",
    "    # _, preds = torch.topk(outputs, k, sorted=False) # top k index\n",
    "    rows = torch.arange(len(labels[0])).view(-1, 1)\n",
    "\n",
    "    recall = torch.sum(labels[rows, preds], dim=1) \\\n",
    "           / torch.min(torch.Tensor([k]), torch.sum(labels, dim=1))\n",
    "    recall[torch.isnan(recall)] = 0\n",
    "    return recall\n",
    "\n",
    "\n",
    "def compute_metric(target_items, predict_items, topK):\n",
    "    # print(f'Temp Recall@100: {recall_kth(predict_items, target_items)}')\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    ndcgs = []\n",
    "    mrrs = []\n",
    "    num_users = len(predict_items)\n",
    "\n",
    "    for k in topK:\n",
    "        sum_precision = sum_recall = sum_ndcg = sum_mrr = 0.0\n",
    "        for user_id in range(num_users):\n",
    "            if len(target_items[user_id]) == 0: continue\n",
    "            mrr_flag = True\n",
    "            num_hit = user_mrr = dcg = 0\n",
    "            \n",
    "            for rank_idx in range(k):\n",
    "                if predict_items[user_id][rank_idx] in target_items[user_id]:\n",
    "                    num_hit += 1 # precision, recall에 사용\n",
    "                    dcg += 1.0 / np.log2(rank_idx + 2)                    \n",
    "                    if mrr_flag:\n",
    "                        user_mrr = 1.0 / (rank_idx+1.0)\n",
    "                        mrr_flag = False\n",
    "            \n",
    "            idcg = 0.0\n",
    "            for rank_idx in range(len(target_items[user_id])):\n",
    "                idcg += 1.0/np.log2(rank_idx+2)\n",
    "            ndcg = (dcg/idcg)\n",
    "\n",
    "            sum_precision += num_hit / k\n",
    "            sum_recall += num_hit / len(target_items[user_id])\n",
    "            sum_ndcg += ndcg\n",
    "            sum_mrr += user_mrr\n",
    "\n",
    "        precision = round(sum_precision / num_users, 4)\n",
    "        recall = round(sum_recall / num_users, 4)\n",
    "        ndcg = round(sum_ndcg / num_users, 4)\n",
    "        mrr = round(sum_mrr / num_users, 4)\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        ndcgs.append(ndcg)\n",
    "        mrrs.append(mrr)\n",
    "\n",
    "    return precisions, recalls, ndcgs, mrrs\n",
    "\n",
    "\n",
    "def evaluate(args, model, loader, label_items: sp.csr_matrix, consumed_items: sp.csr_matrix):\n",
    "    \"\"\"\n",
    "    Args\n",
    "        args                    : hyper-parameters\n",
    "        model                   : 학습된 model\n",
    "        diffsuion               : Diffusion\n",
    "        loader                  : Test data loader // no_shffule\n",
    "        label_items             : Ground Truth, shape: (num_users, num_items) 중에서 target item에만 1\n",
    "        consumed_items          : training data에서 사용된 이미 user가 선호도를 보인 items\n",
    "        topK                    : top K list ex) [10, 20, 50]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    num_user = label_items.shape[0]\n",
    "    user_idx_list = list(range(label_items.shape[0]))\n",
    "    # target_items.shape[0] 대신 consumed_items.shape[0]도 ㄱㅊ\n",
    "\n",
    "    predict_items = []\n",
    "    target_items = []\n",
    "\n",
    "    for user_id in range(num_user):\n",
    "        # user_id에 해당하는, sp.csr_matrix로 저장되어 있는 user의 label item id를 list로 저장.\n",
    "        # nonzero()하면 (row array, col array) 반환.\n",
    "        # col array: np.ndarray의 idx 값이 item id임.\n",
    "        target_items.append(label_items[user_id,:].nonzero()[1].tolist())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, x_0 in enumerate(loader):\n",
    "            start_batch_user_id = batch_idx*args.batch_size\n",
    "            end_batch_user_id = start_batch_user_id + len(x_0)\n",
    "            batch_consumed_items = consumed_items[user_idx_list[start_batch_user_id:end_batch_user_id]]\n",
    "            x_0 = x_0.to(args.device)\n",
    "            prediction, _, _ = model(x_0)\n",
    "            prediction[batch_consumed_items.nonzero()] = -np.inf\n",
    "\n",
    "            _, indices = torch.topk(prediction, args.topK[-1]) # shape (x_0[1].shape, topK[-1])\n",
    "            indices = indices.detach().cpu().numpy().tolist()\n",
    "            predict_items.extend(indices)\n",
    "\n",
    "        precisions, recalls, ndcgs, mrrs = compute_metric(target_items, predict_items, args.topK)\n",
    "    \n",
    "    return precisions, recalls, ndcgs, mrrs\n",
    "\n",
    "\n",
    "def print_metric_results(topK, results):\n",
    "    metric_list = ['Precision', 'Recall', 'nDCG', 'MRR']\n",
    "    for idx, metric in enumerate(metric_list):\n",
    "        str_result = ''\n",
    "        for k_idx, k in enumerate(topK):\n",
    "            str_metric = f'{metric}@{k}'\n",
    "            str_result += f'    {str_metric:14s}: {results[idx][k_idx]:.4f}'\n",
    "        print(str_result)\n",
    "\n",
    "\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user num: 5949\n",
      "item num: 2810\n",
      "Start training\n",
      "Epoch   1 -  train loss:   552.2536,  time: 0:00:01\n",
      "Epoch   2 -  train loss:   535.7915,  time: 0:00:00\n",
      "Epoch   3 -  train loss:   501.6090,  time: 0:00:00\n",
      "Epoch   4 -  train loss:   495.8006,  time: 0:00:00\n",
      "Epoch   5 -  train loss:   491.9161,  time: 0:00:00\n",
      "Epoch   6 -  train loss:   489.7462,  time: 0:00:00\n",
      "Epoch   7 -  train loss:   486.7854,  time: 0:00:00\n",
      "Epoch   8 -  train loss:   484.2441,  time: 0:00:00\n",
      "Epoch   9 -  train loss:   481.8403,  time: 0:00:00\n",
      "Epoch  10 -  train loss:   478.8764,  time: 0:00:00\n",
      "Epoch  11 -  train loss:   474.9220,  time: 0:00:00\n",
      "Epoch  12 -  train loss:   470.5658,  time: 0:00:00\n",
      "Epoch  13 -  train loss:   467.7573,  time: 0:00:00\n",
      "Epoch  14 -  train loss:   464.3559,  time: 0:00:00\n",
      "Epoch  15 -  train loss:   461.4891,  time: 0:00:00\n",
      "Epoch  16 -  train loss:   458.6869,  time: 0:00:00\n",
      "Epoch  17 -  train loss:   456.2356,  time: 0:00:00\n",
      "Epoch  18 -  train loss:   454.3573,  time: 0:00:00\n",
      "Epoch  19 -  train loss:   452.7599,  time: 0:00:00\n",
      "Epoch  20 -  train loss:   450.7619,  time: 0:00:00\n",
      "  Update Best\n",
      "  Validation data\n",
      "    Precision@10  : 0.0633    Precision@20  : 0.0582    Precision@50  : 0.0496    Precision@100 : 0.0426\n",
      "    Recall@10     : 0.0536    Recall@20     : 0.0949    Recall@50     : 0.1923    Recall@100    : 0.3072\n",
      "    nDCG@10       : 0.0531    nDCG@20       : 0.0752    nDCG@50       : 0.1171    nDCG@100      : 0.1598\n",
      "    MRR@10        : 0.1524    MRR@20        : 0.1631    MRR@50        : 0.1699    MRR@100       : 0.1716\n",
      "  Test data\n",
      "    Precision@10  : 0.0429    Precision@20  : 0.0400    Precision@50  : 0.0341    Precision@100 : 0.0281\n",
      "    Recall@10     : 0.0702    Recall@20     : 0.1265    Recall@50     : 0.2505    Recall@100    : 0.3840\n",
      "    nDCG@10       : 0.0554    nDCG@20       : 0.0797    nDCG@50       : 0.1232    nDCG@100      : 0.1633\n",
      "    MRR@10        : 0.1120    MRR@20        : 0.1230    MRR@50        : 0.1305    MRR@100       : 0.1326\n",
      "Epoch  21 -  train loss:   448.4213,  time: 0:00:00\n",
      "Epoch  22 -  train loss:   446.3279,  time: 0:00:00\n",
      "Epoch  23 -  train loss:   443.9285,  time: 0:00:00\n",
      "Epoch  24 -  train loss:   441.6062,  time: 0:00:00\n",
      "Epoch  25 -  train loss:   439.0440,  time: 0:00:00\n",
      "Epoch  26 -  train loss:   437.2139,  time: 0:00:00\n",
      "Epoch  27 -  train loss:   435.1352,  time: 0:00:00\n",
      "Epoch  28 -  train loss:   433.7489,  time: 0:00:00\n",
      "Epoch  29 -  train loss:   431.7502,  time: 0:00:00\n",
      "Epoch  30 -  train loss:   430.6327,  time: 0:00:00\n",
      "Epoch  31 -  train loss:   429.1836,  time: 0:00:00\n",
      "Epoch  32 -  train loss:   427.3058,  time: 0:00:00\n",
      "Epoch  33 -  train loss:   425.7887,  time: 0:00:00\n",
      "Epoch  34 -  train loss:   424.2641,  time: 0:00:00\n",
      "Epoch  35 -  train loss:   423.0510,  time: 0:00:00\n",
      "Epoch  36 -  train loss:   421.9423,  time: 0:00:00\n",
      "Epoch  37 -  train loss:   421.1367,  time: 0:00:00\n",
      "Epoch  38 -  train loss:   419.4165,  time: 0:00:00\n",
      "Epoch  39 -  train loss:   418.4911,  time: 0:00:00\n",
      "Epoch  40 -  train loss:   417.4764,  time: 0:00:00\n",
      "  Update Best\n",
      "  Validation data\n",
      "    Precision@10  : 0.0608    Precision@20  : 0.0561    Precision@50  : 0.0494    Precision@100 : 0.0430\n",
      "    Recall@10     : 0.0593    Recall@20     : 0.1029    Recall@50     : 0.2103    Recall@100    : 0.3350\n",
      "    nDCG@10       : 0.0550    nDCG@20       : 0.0776    nDCG@50       : 0.1224    nDCG@100      : 0.1681\n",
      "    MRR@10        : 0.1528    MRR@20        : 0.1648    MRR@50        : 0.1724    MRR@100       : 0.1741\n",
      "  Test data\n",
      "    Precision@10  : 0.0469    Precision@20  : 0.0429    Precision@50  : 0.0363    Precision@100 : 0.0304\n",
      "    Recall@10     : 0.0884    Recall@20     : 0.1564    Recall@50     : 0.2931    Recall@100    : 0.4434\n",
      "    nDCG@10       : 0.0661    nDCG@20       : 0.0939    nDCG@50       : 0.1414    nDCG@100      : 0.1868\n",
      "    MRR@10        : 0.1205    MRR@20        : 0.1326    MRR@50        : 0.1407    MRR@100       : 0.1427\n",
      "Epoch  41 -  train loss:   417.4144,  time: 0:00:00\n",
      "Epoch  42 -  train loss:   416.1920,  time: 0:00:00\n",
      "Epoch  43 -  train loss:   416.0427,  time: 0:00:00\n",
      "Epoch  44 -  train loss:   415.1689,  time: 0:00:00\n",
      "Epoch  45 -  train loss:   414.5888,  time: 0:00:00\n",
      "Epoch  46 -  train loss:   414.0922,  time: 0:00:00\n",
      "Epoch  47 -  train loss:   413.5372,  time: 0:00:00\n",
      "Epoch  48 -  train loss:   413.0354,  time: 0:00:00\n",
      "Epoch  49 -  train loss:   412.2714,  time: 0:00:00\n",
      "Epoch  50 -  train loss:   412.0846,  time: 0:00:00\n",
      "Epoch  51 -  train loss:   411.8096,  time: 0:00:00\n",
      "Epoch  52 -  train loss:   411.6280,  time: 0:00:00\n",
      "Epoch  53 -  train loss:   411.2362,  time: 0:00:00\n",
      "Epoch  54 -  train loss:   410.8990,  time: 0:00:00\n",
      "Epoch  55 -  train loss:   410.6410,  time: 0:00:00\n",
      "Epoch  56 -  train loss:   410.0967,  time: 0:00:00\n",
      "Epoch  57 -  train loss:   409.6773,  time: 0:00:00\n",
      "Epoch  58 -  train loss:   409.6027,  time: 0:00:00\n",
      "Epoch  59 -  train loss:   409.7239,  time: 0:00:00\n",
      "Epoch  60 -  train loss:   408.9097,  time: 0:00:00\n",
      "  Update Best\n",
      "  Validation data\n",
      "    Precision@10  : 0.0612    Precision@20  : 0.0560    Precision@50  : 0.0491    Precision@100 : 0.0427\n",
      "    Recall@10     : 0.0598    Recall@20     : 0.1049    Recall@50     : 0.2102    Recall@100    : 0.3363\n",
      "    nDCG@10       : 0.0546    nDCG@20       : 0.0776    nDCG@50       : 0.1218    nDCG@100      : 0.1675\n",
      "    MRR@10        : 0.1478    MRR@20        : 0.1596    MRR@50        : 0.1670    MRR@100       : 0.1687\n",
      "  Test data\n",
      "    Precision@10  : 0.0462    Precision@20  : 0.0428    Precision@50  : 0.0364    Precision@100 : 0.0304\n",
      "    Recall@10     : 0.0889    Recall@20     : 0.1551    Recall@50     : 0.2972    Recall@100    : 0.4460\n",
      "    nDCG@10       : 0.0663    nDCG@20       : 0.0938    nDCG@50       : 0.1425    nDCG@100      : 0.1874\n",
      "    MRR@10        : 0.1203    MRR@20        : 0.1328    MRR@50        : 0.1411    MRR@100       : 0.1430\n",
      "Epoch  61 -  train loss:   408.7637,  time: 0:00:00\n",
      "Epoch  62 -  train loss:   408.6933,  time: 0:00:00\n",
      "Epoch  63 -  train loss:   408.4826,  time: 0:00:00\n",
      "Epoch  64 -  train loss:   407.8773,  time: 0:00:00\n",
      "Epoch  65 -  train loss:   408.3423,  time: 0:00:00\n",
      "Epoch  66 -  train loss:   408.2499,  time: 0:00:00\n",
      "Epoch  67 -  train loss:   407.6529,  time: 0:00:00\n",
      "Epoch  68 -  train loss:   407.6697,  time: 0:00:00\n",
      "Epoch  69 -  train loss:   407.2044,  time: 0:00:00\n",
      "Epoch  70 -  train loss:   407.2992,  time: 0:00:00\n",
      "Epoch  71 -  train loss:   406.9050,  time: 0:00:00\n",
      "Epoch  72 -  train loss:   407.0756,  time: 0:00:00\n",
      "Epoch  73 -  train loss:   406.6730,  time: 0:00:00\n",
      "Epoch  74 -  train loss:   406.8094,  time: 0:00:00\n",
      "Epoch  75 -  train loss:   406.8940,  time: 0:00:00\n",
      "Epoch  76 -  train loss:   406.7745,  time: 0:00:00\n",
      "Epoch  77 -  train loss:   406.5817,  time: 0:00:00\n",
      "Epoch  78 -  train loss:   406.3610,  time: 0:00:00\n",
      "Epoch  79 -  train loss:   406.0123,  time: 0:00:00\n",
      "Epoch  80 -  train loss:   406.2283,  time: 0:00:00\n",
      "  Update Best\n",
      "  Validation data\n",
      "    Precision@10  : 0.0601    Precision@20  : 0.0557    Precision@50  : 0.0486    Precision@100 : 0.0426\n",
      "    Recall@10     : 0.0588    Recall@20     : 0.1052    Recall@50     : 0.2089    Recall@100    : 0.3350\n",
      "    nDCG@10       : 0.0539    nDCG@20       : 0.0774    nDCG@50       : 0.1210    nDCG@100      : 0.1668\n",
      "    MRR@10        : 0.1455    MRR@20        : 0.1575    MRR@50        : 0.1650    MRR@100       : 0.1668\n",
      "  Test data\n",
      "    Precision@10  : 0.0449    Precision@20  : 0.0418    Precision@50  : 0.0361    Precision@100 : 0.0303\n",
      "    Recall@10     : 0.0869    Recall@20     : 0.1513    Recall@50     : 0.2934    Recall@100    : 0.4461\n",
      "    nDCG@10       : 0.0646    nDCG@20       : 0.0916    nDCG@50       : 0.1404    nDCG@100      : 0.1860\n",
      "    MRR@10        : 0.1168    MRR@20        : 0.1297    MRR@50        : 0.1379    MRR@100       : 0.1399\n",
      "Epoch  81 -  train loss:   406.0386,  time: 0:00:00\n",
      "Epoch  82 -  train loss:   406.4223,  time: 0:00:00\n",
      "Epoch  83 -  train loss:   405.7445,  time: 0:00:00\n",
      "Epoch  84 -  train loss:   406.1126,  time: 0:00:00\n",
      "Epoch  85 -  train loss:   406.0558,  time: 0:00:00\n",
      "Epoch  86 -  train loss:   405.5463,  time: 0:00:00\n",
      "Epoch  87 -  train loss:   405.2150,  time: 0:00:00\n",
      "Epoch  88 -  train loss:   405.5431,  time: 0:00:00\n",
      "Epoch  89 -  train loss:   405.4237,  time: 0:00:00\n",
      "Epoch  90 -  train loss:   405.3429,  time: 0:00:00\n",
      "Epoch  91 -  train loss:   405.3719,  time: 0:00:00\n",
      "Epoch  92 -  train loss:   405.1145,  time: 0:00:00\n",
      "Epoch  93 -  train loss:   405.2586,  time: 0:00:00\n",
      "Epoch  94 -  train loss:   405.1754,  time: 0:00:00\n",
      "Epoch  95 -  train loss:   404.8475,  time: 0:00:00\n",
      "Epoch  96 -  train loss:   405.0606,  time: 0:00:00\n",
      "Epoch  97 -  train loss:   404.8024,  time: 0:00:00\n",
      "Epoch  98 -  train loss:   404.8326,  time: 0:00:00\n",
      "Epoch  99 -  train loss:   404.7112,  time: 0:00:00\n",
      "Epoch 100 -  train loss:   404.9825,  time: 0:00:00\n",
      "  Validation data\n",
      "    Precision@10  : 0.0576    Precision@20  : 0.0544    Precision@50  : 0.0481    Precision@100 : 0.0421\n",
      "    Recall@10     : 0.0569    Recall@20     : 0.1031    Recall@50     : 0.2079    Recall@100    : 0.3331\n",
      "    nDCG@10       : 0.0528    nDCG@20       : 0.0763    nDCG@50       : 0.1201    nDCG@100      : 0.1656\n",
      "    MRR@10        : 0.1429    MRR@20        : 0.1554    MRR@50        : 0.1630    MRR@100       : 0.1647\n",
      "  Test data\n",
      "    Precision@10  : 0.0445    Precision@20  : 0.0416    Precision@50  : 0.0359    Precision@100 : 0.0302\n",
      "    Recall@10     : 0.0879    Recall@20     : 0.1529    Recall@50     : 0.2930    Recall@100    : 0.4435\n",
      "    nDCG@10       : 0.0646    nDCG@20       : 0.0917    nDCG@50       : 0.1400    nDCG@100      : 0.1852\n",
      "    MRR@10        : 0.1156    MRR@20        : 0.1281    MRR@50        : 0.1367    MRR@100       : 0.1386\n",
      "Epoch 101 -  train loss:   404.5496,  time: 0:00:00\n",
      "Epoch 102 -  train loss:   404.4981,  time: 0:00:00\n",
      "Epoch 103 -  train loss:   404.5837,  time: 0:00:00\n",
      "Epoch 104 -  train loss:   404.3352,  time: 0:00:00\n",
      "Epoch 105 -  train loss:   404.5672,  time: 0:00:00\n",
      "Epoch 106 -  train loss:   404.8367,  time: 0:00:00\n",
      "Epoch 107 -  train loss:   404.7775,  time: 0:00:00\n",
      "Epoch 108 -  train loss:   404.7637,  time: 0:00:00\n",
      "Epoch 109 -  train loss:   404.5168,  time: 0:00:00\n",
      "Epoch 110 -  train loss:   404.5799,  time: 0:00:00\n",
      "Epoch 111 -  train loss:   404.2043,  time: 0:00:00\n",
      "Epoch 112 -  train loss:   404.1902,  time: 0:00:00\n",
      "Epoch 113 -  train loss:   404.1313,  time: 0:00:00\n",
      "Epoch 114 -  train loss:   404.4314,  time: 0:00:00\n",
      "Epoch 115 -  train loss:   404.0458,  time: 0:00:00\n",
      "Epoch 116 -  train loss:   404.6840,  time: 0:00:00\n",
      "Epoch 117 -  train loss:   404.3040,  time: 0:00:00\n",
      "Epoch 118 -  train loss:   404.3730,  time: 0:00:00\n",
      "Epoch 119 -  train loss:   403.9904,  time: 0:00:00\n",
      "Epoch 120 -  train loss:   404.1466,  time: 0:00:00\n",
      "  Validation data\n",
      "    Precision@10  : 0.0572    Precision@20  : 0.0552    Precision@50  : 0.0491    Precision@100 : 0.0424\n",
      "    Recall@10     : 0.0562    Recall@20     : 0.1032    Recall@50     : 0.2107    Recall@100    : 0.3329\n",
      "    nDCG@10       : 0.0527    nDCG@20       : 0.0769    nDCG@50       : 0.1218    nDCG@100      : 0.1663\n",
      "    MRR@10        : 0.1449    MRR@20        : 0.1579    MRR@50        : 0.1655    MRR@100       : 0.1670\n",
      "  Test data\n",
      "    Precision@10  : 0.0454    Precision@20  : 0.0413    Precision@50  : 0.0361    Precision@100 : 0.0303\n",
      "    Recall@10     : 0.0882    Recall@20     : 0.1495    Recall@50     : 0.2931    Recall@100    : 0.4431\n",
      "    nDCG@10       : 0.0657    nDCG@20       : 0.0915    nDCG@50       : 0.1408    nDCG@100      : 0.1859\n",
      "    MRR@10        : 0.1191    MRR@20        : 0.1312    MRR@50        : 0.1397    MRR@100       : 0.1416\n",
      "Epoch 121 -  train loss:   404.2029,  time: 0:00:00\n",
      "Epoch 122 -  train loss:   403.3844,  time: 0:00:00\n",
      "Epoch 123 -  train loss:   404.4835,  time: 0:00:00\n",
      "Epoch 124 -  train loss:   403.7066,  time: 0:00:00\n",
      "Epoch 125 -  train loss:   404.1262,  time: 0:00:00\n",
      "Epoch 126 -  train loss:   403.9268,  time: 0:00:00\n",
      "Epoch 127 -  train loss:   403.5826,  time: 0:00:00\n",
      "Epoch 128 -  train loss:   403.9691,  time: 0:00:00\n",
      "Epoch 129 -  train loss:   403.9404,  time: 0:00:00\n",
      "Epoch 130 -  train loss:   403.9208,  time: 0:00:00\n",
      "Epoch 131 -  train loss:   404.2952,  time: 0:00:00\n",
      "Epoch 132 -  train loss:   404.0107,  time: 0:00:00\n",
      "Epoch 133 -  train loss:   403.4864,  time: 0:00:00\n",
      "Epoch 134 -  train loss:   403.9582,  time: 0:00:00\n",
      "Epoch 135 -  train loss:   403.8768,  time: 0:00:00\n",
      "Epoch 136 -  train loss:   403.7433,  time: 0:00:00\n",
      "Epoch 137 -  train loss:   404.1902,  time: 0:00:00\n",
      "Epoch 138 -  train loss:   403.4600,  time: 0:00:00\n",
      "Epoch 139 -  train loss:   403.5883,  time: 0:00:00\n",
      "Epoch 140 -  train loss:   404.0095,  time: 0:00:00\n",
      "  Validation data\n",
      "    Precision@10  : 0.0580    Precision@20  : 0.0551    Precision@50  : 0.0486    Precision@100 : 0.0420\n",
      "    Recall@10     : 0.0581    Recall@20     : 0.1038    Recall@50     : 0.2120    Recall@100    : 0.3323\n",
      "    nDCG@10       : 0.0533    nDCG@20       : 0.0768    nDCG@50       : 0.1215    nDCG@100      : 0.1655\n",
      "    MRR@10        : 0.1438    MRR@20        : 0.1564    MRR@50        : 0.1640    MRR@100       : 0.1655\n",
      "  Test data\n",
      "    Precision@10  : 0.0446    Precision@20  : 0.0416    Precision@50  : 0.0359    Precision@100 : 0.0301\n",
      "    Recall@10     : 0.0867    Recall@20     : 0.1498    Recall@50     : 0.2934    Recall@100    : 0.4414\n",
      "    nDCG@10       : 0.0640    nDCG@20       : 0.0906    nDCG@50       : 0.1396    nDCG@100      : 0.1844\n",
      "    MRR@10        : 0.1149    MRR@20        : 0.1274    MRR@50        : 0.1359    MRR@100       : 0.1378\n",
      "Epoch 141 -  train loss:   403.9394,  time: 0:00:00\n",
      "Epoch 142 -  train loss:   403.7404,  time: 0:00:00\n",
      "Epoch 143 -  train loss:   403.3770,  time: 0:00:00\n",
      "Epoch 144 -  train loss:   403.7880,  time: 0:00:00\n",
      "Epoch 145 -  train loss:   403.3474,  time: 0:00:00\n",
      "Epoch 146 -  train loss:   403.4766,  time: 0:00:00\n",
      "Epoch 147 -  train loss:   403.5153,  time: 0:00:00\n",
      "Epoch 148 -  train loss:   403.7402,  time: 0:00:00\n",
      "Epoch 149 -  train loss:   403.3251,  time: 0:00:00\n",
      "Epoch 150 -  train loss:   403.8009,  time: 0:00:00\n",
      "Epoch 151 -  train loss:   403.1499,  time: 0:00:00\n",
      "Epoch 152 -  train loss:   403.6869,  time: 0:00:00\n",
      "Epoch 153 -  train loss:   403.6898,  time: 0:00:00\n",
      "Epoch 154 -  train loss:   403.4726,  time: 0:00:00\n",
      "Epoch 155 -  train loss:   403.2190,  time: 0:00:00\n",
      "Epoch 156 -  train loss:   403.4500,  time: 0:00:00\n",
      "Epoch 157 -  train loss:   403.1877,  time: 0:00:00\n",
      "Epoch 158 -  train loss:   403.6421,  time: 0:00:00\n",
      "Epoch 159 -  train loss:   403.4518,  time: 0:00:00\n",
      "Epoch 160 -  train loss:   403.7128,  time: 0:00:00\n",
      "  Validation data\n",
      "    Precision@10  : 0.0576    Precision@20  : 0.0548    Precision@50  : 0.0488    Precision@100 : 0.0421\n",
      "    Recall@10     : 0.0577    Recall@20     : 0.1035    Recall@50     : 0.2121    Recall@100    : 0.3316\n",
      "    nDCG@10       : 0.0527    nDCG@20       : 0.0762    nDCG@50       : 0.1215    nDCG@100      : 0.1653\n",
      "    MRR@10        : 0.1419    MRR@20        : 0.1546    MRR@50        : 0.1623    MRR@100       : 0.1639\n",
      "  Test data\n",
      "    Precision@10  : 0.0457    Precision@20  : 0.0423    Precision@50  : 0.0360    Precision@100 : 0.0303\n",
      "    Recall@10     : 0.0886    Recall@20     : 0.1547    Recall@50     : 0.2933    Recall@100    : 0.4448\n",
      "    nDCG@10       : 0.0659    nDCG@20       : 0.0934    nDCG@50       : 0.1414    nDCG@100      : 0.1869\n",
      "    MRR@10        : 0.1204    MRR@20        : 0.1323    MRR@50        : 0.1407    MRR@100       : 0.1426\n",
      "Epoch 161 -  train loss:   403.3372,  time: 0:00:00\n",
      "Epoch 162 -  train loss:   403.6471,  time: 0:00:00\n",
      "Epoch 163 -  train loss:   403.2740,  time: 0:00:00\n",
      "Epoch 164 -  train loss:   403.6601,  time: 0:00:00\n",
      "Epoch 165 -  train loss:   403.1983,  time: 0:00:00\n",
      "Epoch 166 -  train loss:   403.2736,  time: 0:00:00\n",
      "Epoch 167 -  train loss:   403.3472,  time: 0:00:00\n",
      "Epoch 168 -  train loss:   403.5306,  time: 0:00:00\n",
      "Epoch 169 -  train loss:   402.9179,  time: 0:00:00\n",
      "Epoch 170 -  train loss:   403.3018,  time: 0:00:00\n",
      "Epoch 171 -  train loss:   403.5707,  time: 0:00:00\n",
      "Epoch 172 -  train loss:   403.4818,  time: 0:00:00\n",
      "Epoch 173 -  train loss:   403.2834,  time: 0:00:00\n",
      "Epoch 174 -  train loss:   403.4552,  time: 0:00:00\n",
      "Epoch 175 -  train loss:   403.3678,  time: 0:00:00\n",
      "Epoch 176 -  train loss:   403.4024,  time: 0:00:00\n",
      "Epoch 177 -  train loss:   403.3097,  time: 0:00:00\n",
      "Epoch 178 -  train loss:   403.4822,  time: 0:00:00\n",
      "Epoch 179 -  train loss:   403.3751,  time: 0:00:00\n",
      "Epoch 180 -  train loss:   403.2762,  time: 0:00:00\n",
      "  Validation data\n",
      "    Precision@10  : 0.0580    Precision@20  : 0.0552    Precision@50  : 0.0487    Precision@100 : 0.0422\n",
      "    Recall@10     : 0.0578    Recall@20     : 0.1048    Recall@50     : 0.2097    Recall@100    : 0.3323\n",
      "    nDCG@10       : 0.0532    nDCG@20       : 0.0770    nDCG@50       : 0.1210    nDCG@100      : 0.1657\n",
      "    MRR@10        : 0.1427    MRR@20        : 0.1553    MRR@50        : 0.1628    MRR@100       : 0.1645\n",
      "  Test data\n",
      "    Precision@10  : 0.0450    Precision@20  : 0.0415    Precision@50  : 0.0358    Precision@100 : 0.0301\n",
      "    Recall@10     : 0.0878    Recall@20     : 0.1516    Recall@50     : 0.2931    Recall@100    : 0.4428\n",
      "    nDCG@10       : 0.0648    nDCG@20       : 0.0915    nDCG@50       : 0.1401    nDCG@100      : 0.1852\n",
      "    MRR@10        : 0.1177    MRR@20        : 0.1300    MRR@50        : 0.1384    MRR@100       : 0.1403\n",
      "Epoch 181 -  train loss:   403.3978,  time: 0:00:00\n",
      "Epoch 182 -  train loss:   403.6098,  time: 0:00:00\n",
      "Epoch 183 -  train loss:   403.8534,  time: 0:00:00\n",
      "Epoch 184 -  train loss:   403.5803,  time: 0:00:00\n",
      "Epoch 185 -  train loss:   403.8104,  time: 0:00:00\n",
      "Epoch 186 -  train loss:   403.1569,  time: 0:00:00\n",
      "Epoch 187 -  train loss:   403.1873,  time: 0:00:00\n",
      "Epoch 188 -  train loss:   402.9489,  time: 0:00:00\n",
      "Epoch 189 -  train loss:   403.2883,  time: 0:00:00\n",
      "Epoch 190 -  train loss:   403.1787,  time: 0:00:00\n",
      "Epoch 191 -  train loss:   403.1962,  time: 0:00:00\n",
      "Epoch 192 -  train loss:   403.0434,  time: 0:00:00\n",
      "Epoch 193 -  train loss:   402.6876,  time: 0:00:00\n",
      "Epoch 194 -  train loss:   403.2023,  time: 0:00:00\n",
      "Epoch 195 -  train loss:   403.0596,  time: 0:00:00\n",
      "Epoch 196 -  train loss:   402.7850,  time: 0:00:00\n",
      "Epoch 197 -  train loss:   403.3956,  time: 0:00:00\n",
      "Epoch 198 -  train loss:   403.2087,  time: 0:00:00\n",
      "Epoch 199 -  train loss:   403.1606,  time: 0:00:00\n",
      "Epoch 200 -  train loss:   402.7742,  time: 0:00:00\n",
      "  Update Best\n",
      "  Validation data\n",
      "    Precision@10  : 0.0587    Precision@20  : 0.0556    Precision@50  : 0.0491    Precision@100 : 0.0423\n",
      "    Recall@10     : 0.0586    Recall@20     : 0.1053    Recall@50     : 0.2116    Recall@100    : 0.3334\n",
      "    nDCG@10       : 0.0533    nDCG@20       : 0.0772    nDCG@50       : 0.1217    nDCG@100      : 0.1661\n",
      "    MRR@10        : 0.1417    MRR@20        : 0.1545    MRR@50        : 0.1620    MRR@100       : 0.1636\n",
      "  Test data\n",
      "    Precision@10  : 0.0457    Precision@20  : 0.0422    Precision@50  : 0.0363    Precision@100 : 0.0303\n",
      "    Recall@10     : 0.0880    Recall@20     : 0.1544    Recall@50     : 0.2939    Recall@100    : 0.4431\n",
      "    nDCG@10       : 0.0651    nDCG@20       : 0.0927    nDCG@50       : 0.1409    nDCG@100      : 0.1859\n",
      "    MRR@10        : 0.1190    MRR@20        : 0.1315    MRR@50        : 0.1397    MRR@100       : 0.1416\n",
      "Epoch 201 -  train loss:   403.2626,  time: 0:00:00\n",
      "Epoch 202 -  train loss:   403.4474,  time: 0:00:00\n",
      "Epoch 203 -  train loss:   403.0946,  time: 0:00:00\n",
      "Epoch 204 -  train loss:   402.8512,  time: 0:00:00\n",
      "Epoch 205 -  train loss:   403.2803,  time: 0:00:00\n",
      "Epoch 206 -  train loss:   403.2639,  time: 0:00:00\n",
      "Epoch 207 -  train loss:   403.1129,  time: 0:00:00\n",
      "Epoch 208 -  train loss:   403.3696,  time: 0:00:00\n",
      "Epoch 209 -  train loss:   403.2818,  time: 0:00:00\n",
      "Epoch 210 -  train loss:   402.9617,  time: 0:00:00\n",
      "Epoch 211 -  train loss:   402.8349,  time: 0:00:00\n",
      "Epoch 212 -  train loss:   403.1215,  time: 0:00:00\n",
      "Epoch 213 -  train loss:   403.3056,  time: 0:00:00\n",
      "Epoch 214 -  train loss:   403.3483,  time: 0:00:00\n",
      "Epoch 215 -  train loss:   402.6707,  time: 0:00:00\n",
      "Epoch 216 -  train loss:   403.0857,  time: 0:00:00\n",
      "Epoch 217 -  train loss:   402.9297,  time: 0:00:00\n",
      "Epoch 218 -  train loss:   402.9480,  time: 0:00:00\n",
      "Epoch 219 -  train loss:   403.0901,  time: 0:00:00\n",
      "Epoch 220 -  train loss:   403.6227,  time: 0:00:00\n",
      "  Validation data\n",
      "    Precision@10  : 0.0575    Precision@20  : 0.0546    Precision@50  : 0.0488    Precision@100 : 0.0421\n",
      "    Recall@10     : 0.0582    Recall@20     : 0.1047    Recall@50     : 0.2113    Recall@100    : 0.3317\n",
      "    nDCG@10       : 0.0530    nDCG@20       : 0.0766    nDCG@50       : 0.1213    nDCG@100      : 0.1654\n",
      "    MRR@10        : 0.1415    MRR@20        : 0.1543    MRR@50        : 0.1620    MRR@100       : 0.1636\n",
      "  Test data\n",
      "    Precision@10  : 0.0462    Precision@20  : 0.0422    Precision@50  : 0.0362    Precision@100 : 0.0302\n",
      "    Recall@10     : 0.0887    Recall@20     : 0.1536    Recall@50     : 0.2948    Recall@100    : 0.4451\n",
      "    nDCG@10       : 0.0657    nDCG@20       : 0.0928    nDCG@50       : 0.1413    nDCG@100      : 0.1864\n",
      "    MRR@10        : 0.1197    MRR@20        : 0.1316    MRR@50        : 0.1401    MRR@100       : 0.1420\n",
      "Epoch 221 -  train loss:   403.3319,  time: 0:00:00\n",
      "Epoch 222 -  train loss:   403.1765,  time: 0:00:00\n",
      "Epoch 223 -  train loss:   402.8189,  time: 0:00:00\n",
      "Epoch 224 -  train loss:   403.2407,  time: 0:00:00\n",
      "Epoch 225 -  train loss:   403.1081,  time: 0:00:00\n",
      "Epoch 226 -  train loss:   402.9282,  time: 0:00:00\n",
      "Epoch 227 -  train loss:   403.0866,  time: 0:00:00\n",
      "Epoch 228 -  train loss:   402.8614,  time: 0:00:00\n",
      "Epoch 229 -  train loss:   403.0693,  time: 0:00:00\n",
      "Epoch 230 -  train loss:   403.4397,  time: 0:00:00\n",
      "Epoch 231 -  train loss:   403.3750,  time: 0:00:00\n",
      "Epoch 232 -  train loss:   402.9550,  time: 0:00:00\n",
      "Epoch 233 -  train loss:   402.7561,  time: 0:00:00\n",
      "Epoch 234 -  train loss:   403.2294,  time: 0:00:00\n",
      "Epoch 235 -  train loss:   402.9773,  time: 0:00:00\n",
      "Epoch 236 -  train loss:   403.6135,  time: 0:00:00\n",
      "Epoch 237 -  train loss:   403.1071,  time: 0:00:00\n",
      "Epoch 238 -  train loss:   402.8756,  time: 0:00:00\n",
      "Epoch 239 -  train loss:   402.6551,  time: 0:00:00\n",
      "Epoch 240 -  train loss:   403.2766,  time: 0:00:00\n",
      "  Validation data\n",
      "    Precision@10  : 0.0577    Precision@20  : 0.0548    Precision@50  : 0.0491    Precision@100 : 0.0425\n",
      "    Recall@10     : 0.0575    Recall@20     : 0.1045    Recall@50     : 0.2122    Recall@100    : 0.3334\n",
      "    nDCG@10       : 0.0528    nDCG@20       : 0.0766    nDCG@50       : 0.1216    nDCG@100      : 0.1660\n",
      "    MRR@10        : 0.1427    MRR@20        : 0.1553    MRR@50        : 0.1630    MRR@100       : 0.1646\n",
      "  Test data\n",
      "    Precision@10  : 0.0459    Precision@20  : 0.0425    Precision@50  : 0.0366    Precision@100 : 0.0304\n",
      "    Recall@10     : 0.0878    Recall@20     : 0.1546    Recall@50     : 0.2960    Recall@100    : 0.4459\n",
      "    nDCG@10       : 0.0655    nDCG@20       : 0.0933    nDCG@50       : 0.1421    nDCG@100      : 0.1871\n",
      "    MRR@10        : 0.1203    MRR@20        : 0.1330    MRR@50        : 0.1414    MRR@100       : 0.1432\n",
      "Epoch 241 -  train loss:   403.2548,  time: 0:00:00\n",
      "Epoch 242 -  train loss:   403.3196,  time: 0:00:00\n",
      "Epoch 243 -  train loss:   403.0641,  time: 0:00:00\n",
      "Epoch 244 -  train loss:   402.9486,  time: 0:00:00\n",
      "Epoch 245 -  train loss:   402.8278,  time: 0:00:00\n",
      "Epoch 246 -  train loss:   403.3790,  time: 0:00:00\n",
      "Epoch 247 -  train loss:   402.9967,  time: 0:00:00\n",
      "Epoch 248 -  train loss:   403.1291,  time: 0:00:00\n",
      "Epoch 249 -  train loss:   403.5197,  time: 0:00:00\n",
      "Epoch 250 -  train loss:   402.9732,  time: 0:00:00\n",
      "Epoch 251 -  train loss:   402.9621,  time: 0:00:00\n",
      "Epoch 252 -  train loss:   403.2726,  time: 0:00:00\n",
      "Epoch 253 -  train loss:   403.4896,  time: 0:00:00\n",
      "Epoch 254 -  train loss:   403.4917,  time: 0:00:00\n",
      "Epoch 255 -  train loss:   403.2076,  time: 0:00:00\n",
      "Epoch 256 -  train loss:   402.9080,  time: 0:00:00\n",
      "Epoch 257 -  train loss:   402.7994,  time: 0:00:00\n",
      "Epoch 258 -  train loss:   403.1238,  time: 0:00:00\n",
      "Epoch 259 -  train loss:   403.2861,  time: 0:00:00\n",
      "Epoch 260 -  train loss:   403.3487,  time: 0:00:00\n",
      "  Validation data\n",
      "    Precision@10  : 0.0587    Precision@20  : 0.0554    Precision@50  : 0.0495    Precision@100 : 0.0425\n",
      "    Recall@10     : 0.0580    Recall@20     : 0.1045    Recall@50     : 0.2126    Recall@100    : 0.3334\n",
      "    nDCG@10       : 0.0536    nDCG@20       : 0.0773    nDCG@50       : 0.1225    nDCG@100      : 0.1666\n",
      "    MRR@10        : 0.1455    MRR@20        : 0.1582    MRR@50        : 0.1657    MRR@100       : 0.1673\n",
      "  Test data\n",
      "    Precision@10  : 0.0453    Precision@20  : 0.0421    Precision@50  : 0.0363    Precision@100 : 0.0306\n",
      "    Recall@10     : 0.0885    Recall@20     : 0.1538    Recall@50     : 0.2957    Recall@100    : 0.4474\n",
      "    nDCG@10       : 0.0648    nDCG@20       : 0.0920    nDCG@50       : 0.1409    nDCG@100      : 0.1867\n",
      "    MRR@10        : 0.1152    MRR@20        : 0.1280    MRR@50        : 0.1364    MRR@100       : 0.1383\n",
      "Epoch 261 -  train loss:   403.0783,  time: 0:00:00\n",
      "Epoch 262 -  train loss:   402.8564,  time: 0:00:00\n",
      "Epoch 263 -  train loss:   402.9942,  time: 0:00:00\n",
      "Epoch 264 -  train loss:   402.7899,  time: 0:00:00\n",
      "Epoch 265 -  train loss:   402.9908,  time: 0:00:00\n",
      "Epoch 266 -  train loss:   402.8839,  time: 0:00:00\n",
      "Epoch 267 -  train loss:   402.9130,  time: 0:00:00\n",
      "Epoch 268 -  train loss:   402.7906,  time: 0:00:00\n",
      "Epoch 269 -  train loss:   403.1734,  time: 0:00:00\n",
      "Epoch 270 -  train loss:   403.0079,  time: 0:00:00\n",
      "Epoch 271 -  train loss:   403.1235,  time: 0:00:00\n",
      "Epoch 272 -  train loss:   403.1077,  time: 0:00:00\n",
      "Epoch 273 -  train loss:   403.5366,  time: 0:00:00\n",
      "Epoch 274 -  train loss:   403.2180,  time: 0:00:00\n",
      "Epoch 275 -  train loss:   403.2685,  time: 0:00:00\n",
      "Epoch 276 -  train loss:   402.9367,  time: 0:00:00\n",
      "Epoch 277 -  train loss:   403.5082,  time: 0:00:00\n",
      "Epoch 278 -  train loss:   403.3891,  time: 0:00:00\n",
      "Epoch 279 -  train loss:   403.3898,  time: 0:00:00\n",
      "Epoch 280 -  train loss:   402.7905,  time: 0:00:00\n",
      "  Validation data\n",
      "    Precision@10  : 0.0583    Precision@20  : 0.0552    Precision@50  : 0.0488    Precision@100 : 0.0424\n",
      "    Recall@10     : 0.0569    Recall@20     : 0.1045    Recall@50     : 0.2114    Recall@100    : 0.3338\n",
      "    nDCG@10       : 0.0528    nDCG@20       : 0.0768    nDCG@50       : 0.1215    nDCG@100      : 0.1663\n",
      "    MRR@10        : 0.1442    MRR@20        : 0.1569    MRR@50        : 0.1644    MRR@100       : 0.1660\n",
      "  Test data\n",
      "    Precision@10  : 0.0459    Precision@20  : 0.0425    Precision@50  : 0.0366    Precision@100 : 0.0304\n",
      "    Recall@10     : 0.0880    Recall@20     : 0.1553    Recall@50     : 0.2964    Recall@100    : 0.4453\n",
      "    nDCG@10       : 0.0654    nDCG@20       : 0.0933    nDCG@50       : 0.1420    nDCG@100      : 0.1869\n",
      "    MRR@10        : 0.1206    MRR@20        : 0.1332    MRR@50        : 0.1415    MRR@100       : 0.1433\n",
      "Epoch 281 -  train loss:   402.8627,  time: 0:00:00\n",
      "Epoch 282 -  train loss:   402.7917,  time: 0:00:00\n",
      "Epoch 283 -  train loss:   402.9571,  time: 0:00:00\n",
      "Epoch 284 -  train loss:   402.8229,  time: 0:00:00\n",
      "Epoch 285 -  train loss:   403.3152,  time: 0:00:00\n",
      "Epoch 286 -  train loss:   402.9192,  time: 0:00:00\n",
      "Epoch 287 -  train loss:   402.9882,  time: 0:00:00\n",
      "Epoch 288 -  train loss:   402.6971,  time: 0:00:00\n",
      "Epoch 289 -  train loss:   403.0855,  time: 0:00:00\n",
      "Epoch 290 -  train loss:   402.9722,  time: 0:00:00\n",
      "Epoch 291 -  train loss:   403.2449,  time: 0:00:00\n",
      "Epoch 292 -  train loss:   402.9821,  time: 0:00:00\n",
      "Epoch 293 -  train loss:   402.8314,  time: 0:00:00\n",
      "Epoch 294 -  train loss:   403.0879,  time: 0:00:00\n",
      "Epoch 295 -  train loss:   403.2485,  time: 0:00:00\n",
      "Epoch 296 -  train loss:   402.9264,  time: 0:00:00\n",
      "Epoch 297 -  train loss:   403.0255,  time: 0:00:00\n",
      "Epoch 298 -  train loss:   403.2709,  time: 0:00:00\n",
      "Epoch 299 -  train loss:   402.9206,  time: 0:00:00\n",
      "Epoch 300 -  train loss:   402.7953,  time: 0:00:00\n",
      "  Validation data\n",
      "    Precision@10  : 0.0589    Precision@20  : 0.0558    Precision@50  : 0.0493    Precision@100 : 0.0426\n",
      "    Recall@10     : 0.0582    Recall@20     : 0.1049    Recall@50     : 0.2114    Recall@100    : 0.3345\n",
      "    nDCG@10       : 0.0533    nDCG@20       : 0.0772    nDCG@50       : 0.1220    nDCG@100      : 0.1668\n",
      "    MRR@10        : 0.1454    MRR@20        : 0.1583    MRR@50        : 0.1658    MRR@100       : 0.1674\n",
      "  Test data\n",
      "    Precision@10  : 0.0461    Precision@20  : 0.0429    Precision@50  : 0.0367    Precision@100 : 0.0305\n",
      "    Recall@10     : 0.0879    Recall@20     : 0.1549    Recall@50     : 0.2953    Recall@100    : 0.4450\n",
      "    nDCG@10       : 0.0657    nDCG@20       : 0.0936    nDCG@50       : 0.1421    nDCG@100      : 0.1871\n",
      "    MRR@10        : 0.1214    MRR@20        : 0.1340    MRR@50        : 0.1423    MRR@100       : 0.1441\n",
      "Epoch 301 -  train loss:   402.4897,  time: 0:00:00\n",
      "Epoch 302 -  train loss:   403.2123,  time: 0:00:00\n",
      "Epoch 303 -  train loss:   402.9838,  time: 0:00:00\n",
      "Epoch 304 -  train loss:   403.1232,  time: 0:00:00\n",
      "Epoch 305 -  train loss:   403.1141,  time: 0:00:00\n",
      "Epoch 306 -  train loss:   403.1891,  time: 0:00:00\n",
      "Epoch 307 -  train loss:   402.9830,  time: 0:00:00\n",
      "Epoch 308 -  train loss:   403.1653,  time: 0:00:00\n",
      "Epoch 309 -  train loss:   403.1458,  time: 0:00:00\n",
      "Epoch 310 -  train loss:   403.5656,  time: 0:00:00\n",
      "Epoch 311 -  train loss:   402.7311,  time: 0:00:00\n",
      "Epoch 312 -  train loss:   403.1104,  time: 0:00:00\n",
      "Epoch 313 -  train loss:   402.8453,  time: 0:00:00\n",
      "Epoch 314 -  train loss:   403.5124,  time: 0:00:00\n",
      "Epoch 315 -  train loss:   402.8032,  time: 0:00:00\n",
      "Epoch 316 -  train loss:   403.0943,  time: 0:00:00\n",
      "Epoch 317 -  train loss:   402.6383,  time: 0:00:00\n",
      "Epoch 318 -  train loss:   403.2309,  time: 0:00:00\n",
      "Epoch 319 -  train loss:   403.0751,  time: 0:00:00\n",
      "Epoch 320 -  train loss:   403.0798,  time: 0:00:00\n",
      "  Validation data\n",
      "    Precision@10  : 0.0593    Precision@20  : 0.0562    Precision@50  : 0.0492    Precision@100 : 0.0425\n",
      "    Recall@10     : 0.0570    Recall@20     : 0.1050    Recall@50     : 0.2119    Recall@100    : 0.3339\n",
      "    nDCG@10       : 0.0528    nDCG@20       : 0.0771    nDCG@50       : 0.1216    nDCG@100      : 0.1662\n",
      "    MRR@10        : 0.1450    MRR@20        : 0.1578    MRR@50        : 0.1650    MRR@100       : 0.1666\n",
      "  Test data\n",
      "    Precision@10  : 0.0462    Precision@20  : 0.0426    Precision@50  : 0.0368    Precision@100 : 0.0303\n",
      "    Recall@10     : 0.0877    Recall@20     : 0.1542    Recall@50     : 0.2967    Recall@100    : 0.4443\n",
      "    nDCG@10       : 0.0653    nDCG@20       : 0.0930    nDCG@50       : 0.1422    nDCG@100      : 0.1866\n",
      "    MRR@10        : 0.1209    MRR@20        : 0.1330    MRR@50        : 0.1415    MRR@100       : 0.1432\n",
      "Epoch 321 -  train loss:   403.0086,  time: 0:00:00\n",
      "Epoch 322 -  train loss:   403.2048,  time: 0:00:00\n",
      "Epoch 323 -  train loss:   402.8217,  time: 0:00:00\n",
      "Epoch 324 -  train loss:   403.1992,  time: 0:00:00\n",
      "Epoch 325 -  train loss:   403.1245,  time: 0:00:00\n",
      "Epoch 326 -  train loss:   403.1170,  time: 0:00:00\n",
      "Epoch 327 -  train loss:   403.2674,  time: 0:00:00\n",
      "Epoch 328 -  train loss:   403.2600,  time: 0:00:00\n",
      "Epoch 329 -  train loss:   403.0419,  time: 0:00:00\n",
      "Epoch 330 -  train loss:   402.9989,  time: 0:00:00\n",
      "Epoch 331 -  train loss:   403.0296,  time: 0:00:00\n",
      "Epoch 332 -  train loss:   402.9554,  time: 0:00:00\n",
      "Epoch 333 -  train loss:   403.3929,  time: 0:00:00\n",
      "Epoch 334 -  train loss:   403.0664,  time: 0:00:00\n",
      "Epoch 335 -  train loss:   402.9943,  time: 0:00:00\n",
      "Epoch 336 -  train loss:   402.9762,  time: 0:00:00\n",
      "Epoch 337 -  train loss:   403.3108,  time: 0:00:00\n",
      "Epoch 338 -  train loss:   402.8578,  time: 0:00:00\n",
      "Epoch 339 -  train loss:   403.5646,  time: 0:00:00\n",
      "Epoch 340 -  train loss:   403.3294,  time: 0:00:00\n",
      "  Validation data\n",
      "    Precision@10  : 0.0591    Precision@20  : 0.0558    Precision@50  : 0.0495    Precision@100 : 0.0429\n",
      "    Recall@10     : 0.0569    Recall@20     : 0.1041    Recall@50     : 0.2116    Recall@100    : 0.3356\n",
      "    nDCG@10       : 0.0526    nDCG@20       : 0.0764    nDCG@50       : 0.1215    nDCG@100      : 0.1667\n",
      "    MRR@10        : 0.1458    MRR@20        : 0.1587    MRR@50        : 0.1660    MRR@100       : 0.1676\n",
      "  Test data\n",
      "    Precision@10  : 0.0457    Precision@20  : 0.0432    Precision@50  : 0.0368    Precision@100 : 0.0306\n",
      "    Recall@10     : 0.0863    Recall@20     : 0.1545    Recall@50     : 0.2960    Recall@100    : 0.4459\n",
      "    nDCG@10       : 0.0647    nDCG@20       : 0.0932    nDCG@50       : 0.1420    nDCG@100      : 0.1871\n",
      "    MRR@10        : 0.1195    MRR@20        : 0.1327    MRR@50        : 0.1407    MRR@100       : 0.1425\n",
      "Epoch 341 -  train loss:   402.9079,  time: 0:00:00\n",
      "Epoch 342 -  train loss:   403.4831,  time: 0:00:00\n",
      "Epoch 343 -  train loss:   403.2243,  time: 0:00:00\n",
      "Epoch 344 -  train loss:   403.8151,  time: 0:00:00\n",
      "Epoch 345 -  train loss:   403.4787,  time: 0:00:00\n",
      "Epoch 346 -  train loss:   403.0858,  time: 0:00:00\n",
      "Epoch 347 -  train loss:   403.2801,  time: 0:00:00\n",
      "Epoch 348 -  train loss:   403.2836,  time: 0:00:00\n",
      "Epoch 349 -  train loss:   402.8655,  time: 0:00:00\n",
      "Epoch 350 -  train loss:   402.8639,  time: 0:00:00\n",
      "Epoch 351 -  train loss:   403.1441,  time: 0:00:00\n",
      "Epoch 352 -  train loss:   403.5917,  time: 0:00:00\n",
      "Epoch 353 -  train loss:   403.1262,  time: 0:00:00\n",
      "Epoch 354 -  train loss:   403.2552,  time: 0:00:00\n",
      "Epoch 355 -  train loss:   403.4133,  time: 0:00:00\n",
      "Epoch 356 -  train loss:   403.2013,  time: 0:00:00\n",
      "Epoch 357 -  train loss:   402.9863,  time: 0:00:00\n",
      "Epoch 358 -  train loss:   403.0308,  time: 0:00:00\n",
      "Epoch 359 -  train loss:   402.9030,  time: 0:00:00\n",
      "Epoch 360 -  train loss:   403.5788,  time: 0:00:00\n",
      "  Validation data\n",
      "    Precision@10  : 0.0593    Precision@20  : 0.0557    Precision@50  : 0.0493    Precision@100 : 0.0424\n",
      "    Recall@10     : 0.0568    Recall@20     : 0.1045    Recall@50     : 0.2127    Recall@100    : 0.3342\n",
      "    nDCG@10       : 0.0531    nDCG@20       : 0.0771    nDCG@50       : 0.1223    nDCG@100      : 0.1666\n",
      "    MRR@10        : 0.1450    MRR@20        : 0.1576    MRR@50        : 0.1653    MRR@100       : 0.1668\n",
      "  Test data\n",
      "    Precision@10  : 0.0458    Precision@20  : 0.0428    Precision@50  : 0.0367    Precision@100 : 0.0304\n",
      "    Recall@10     : 0.0876    Recall@20     : 0.1570    Recall@50     : 0.2968    Recall@100    : 0.4442\n",
      "    nDCG@10       : 0.0653    nDCG@20       : 0.0939    nDCG@50       : 0.1423    nDCG@100      : 0.1868\n",
      "    MRR@10        : 0.1215    MRR@20        : 0.1344    MRR@50        : 0.1426    MRR@100       : 0.1444\n",
      "Epoch 361 -  train loss:   403.4123,  time: 0:00:00\n",
      "Epoch 362 -  train loss:   403.0590,  time: 0:00:00\n",
      "Epoch 363 -  train loss:   403.3872,  time: 0:00:00\n",
      "Epoch 364 -  train loss:   403.0262,  time: 0:00:00\n",
      "Epoch 365 -  train loss:   403.7118,  time: 0:00:00\n",
      "Epoch 366 -  train loss:   403.1328,  time: 0:00:00\n",
      "Epoch 367 -  train loss:   403.3257,  time: 0:00:00\n",
      "Epoch 368 -  train loss:   403.0968,  time: 0:00:00\n",
      "Epoch 369 -  train loss:   403.1866,  time: 0:00:00\n",
      "Epoch 370 -  train loss:   403.3959,  time: 0:00:00\n",
      "Epoch 371 -  train loss:   402.9312,  time: 0:00:00\n",
      "Epoch 372 -  train loss:   403.0420,  time: 0:00:00\n",
      "Epoch 373 -  train loss:   403.1151,  time: 0:00:00\n",
      "Epoch 374 -  train loss:   403.1923,  time: 0:00:00\n",
      "Epoch 375 -  train loss:   403.0339,  time: 0:00:00\n",
      "Epoch 376 -  train loss:   403.3084,  time: 0:00:00\n",
      "Epoch 377 -  train loss:   403.1383,  time: 0:00:00\n",
      "Epoch 378 -  train loss:   403.5227,  time: 0:00:00\n",
      "Epoch 379 -  train loss:   403.0059,  time: 0:00:00\n",
      "Epoch 380 -  train loss:   403.2790,  time: 0:00:00\n",
      "  Validation data\n",
      "    Precision@10  : 0.0604    Precision@20  : 0.0564    Precision@50  : 0.0500    Precision@100 : 0.0428\n",
      "    Recall@10     : 0.0581    Recall@20     : 0.1049    Recall@50     : 0.2133    Recall@100    : 0.3345\n",
      "    nDCG@10       : 0.0536    nDCG@20       : 0.0773    nDCG@50       : 0.1228    nDCG@100      : 0.1671\n",
      "    MRR@10        : 0.1464    MRR@20        : 0.1586    MRR@50        : 0.1661    MRR@100       : 0.1677\n",
      "  Test data\n",
      "    Precision@10  : 0.0464    Precision@20  : 0.0437    Precision@50  : 0.0370    Precision@100 : 0.0308\n",
      "    Recall@10     : 0.0868    Recall@20     : 0.1559    Recall@50     : 0.2977    Recall@100    : 0.4485\n",
      "    nDCG@10       : 0.0646    nDCG@20       : 0.0934    nDCG@50       : 0.1424    nDCG@100      : 0.1877\n",
      "    MRR@10        : 0.1198    MRR@20        : 0.1330    MRR@50        : 0.1411    MRR@100       : 0.1429\n",
      "Epoch 381 -  train loss:   403.0902,  time: 0:00:00\n",
      "Epoch 382 -  train loss:   403.1145,  time: 0:00:00\n",
      "Epoch 383 -  train loss:   403.0595,  time: 0:00:00\n",
      "Epoch 384 -  train loss:   403.2882,  time: 0:00:00\n",
      "Epoch 385 -  train loss:   403.0093,  time: 0:00:00\n",
      "Epoch 386 -  train loss:   403.4836,  time: 0:00:00\n",
      "Epoch 387 -  train loss:   402.7334,  time: 0:00:00\n",
      "Epoch 388 -  train loss:   403.3432,  time: 0:00:00\n",
      "Epoch 389 -  train loss:   402.8488,  time: 0:00:00\n",
      "Epoch 390 -  train loss:   403.5191,  time: 0:00:00\n",
      "Epoch 391 -  train loss:   403.2589,  time: 0:00:00\n",
      "Epoch 392 -  train loss:   403.2613,  time: 0:00:00\n",
      "Epoch 393 -  train loss:   403.1748,  time: 0:00:00\n",
      "Epoch 394 -  train loss:   403.3112,  time: 0:00:00\n",
      "Epoch 395 -  train loss:   403.2848,  time: 0:00:00\n",
      "Epoch 396 -  train loss:   403.3271,  time: 0:00:00\n",
      "Epoch 397 -  train loss:   402.9272,  time: 0:00:00\n",
      "Epoch 398 -  train loss:   403.2831,  time: 0:00:00\n",
      "Epoch 399 -  train loss:   403.2504,  time: 0:00:00\n",
      "------------------\n",
      "Exiting from training early\n",
      "##########################################################################################################\n",
      "Test Metirc At Best Valid Metric\n",
      "    Precision@10  : 0.0457    Precision@20  : 0.0422    Precision@50  : 0.0363    Precision@100 : 0.0303\n",
      "    Recall@10     : 0.0880    Recall@20     : 0.1544    Recall@50     : 0.2939    Recall@100    : 0.4431\n",
      "    nDCG@10       : 0.0651    nDCG@20       : 0.0927    nDCG@50       : 0.1409    nDCG@100      : 0.1859\n",
      "    MRR@10        : 0.1190    MRR@20        : 0.1315    MRR@50        : 0.1397    MRR@100       : 0.1416\n",
      "##########################################################################################################\n"
     ]
    }
   ],
   "source": [
    "dict_args = {}\n",
    "args = dotdict(dict_args)\n",
    "\n",
    "# Training hyper\n",
    "args.dataset_name = 'ml-1m_clean'\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "args.batch_size = 500\n",
    "args.lr = 1e-3\n",
    "args.weight_decay = 0.\n",
    "args.epochs = 500\n",
    "args.topK = [10, 20, 50, 100]\n",
    "args.beta = 0.2 # annealing의 최대값: MultVAE의 값 사용.\n",
    "args.patience = 200\n",
    "args.freq_metric = 20\n",
    "\n",
    "# vae hyper\n",
    "args.num_concepts = 7\n",
    "args.dim_item = 100\n",
    "\n",
    "\n",
    "dir_path = os.path.join(os.getcwd(), args.dataset_name)\n",
    "sp_train, sp_valid, sp_test, num_users, num_items =data_load(dir_path) \n",
    "# 논문 setting을 따라하면, 성능이 복원된다.\n",
    "# 귀찮아서 그냥 DiffuRec setting 그대로 사용.\n",
    "train_dataset = DataMacridVAE(torch.FloatTensor(sp_train.toarray()))\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, pin_memory=True, shuffle=True)\n",
    "test_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "# Build MacirdVAE   \n",
    "model = MacridVAE(args, num_items).to(args.device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "update_count_vae = 0\n",
    "best_metric, best_epoch = -100, 0\n",
    "best_test_result = None\n",
    "print(\"Start training\")\n",
    "for epoch in range(args.epochs):\n",
    "    if epoch - best_epoch >= args.patience: # early stopping\n",
    "        print('-'*18)\n",
    "        print('Exiting from training early')\n",
    "        break\n",
    "    start = time.time()\n",
    "    avg_loss, update_count_vae = train_one_epoch(args, model, optimizer, train_loader, update_count_vae)\n",
    "    print(f'Epoch {epoch+1:>3} -  train loss: {avg_loss: >10.4f},  time: {str(timedelta(seconds=int(time.time() - start)))}')\n",
    "\n",
    "    if (epoch+1) % args.freq_metric == 0:\n",
    "        val_results = evaluate(args, model, test_loader, sp_valid, sp_train)\n",
    "        test_results = evaluate(args, model, test_loader, sp_test, sp_train)\n",
    "    \n",
    "        val_recalls = val_results[1]\n",
    "        if val_recalls[1] > best_metric: # Metric: Recall@20\n",
    "            best_metric, best_epoch, best_test_result = val_recalls[1], epoch, test_results\n",
    "            print('  Update Best')\n",
    "\n",
    "\n",
    "        print('  Validation data')\n",
    "        print_metric_results(args.topK, val_results)\n",
    "        print('  Test data')\n",
    "        print_metric_results(args.topK, test_results)\n",
    "\n",
    "print('#'*106)\n",
    "print(\"Test Metirc At Best Valid Metric\")\n",
    "print_metric_results(args.topK, best_test_result)\n",
    "print('#'*106)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
